# TH-Suite ä¿®å¤éœ€æ±‚æ–‡æ¡£

> **æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
> **åˆ›å»ºæ—¶é—´**: 2025-09-11  
> **åŸºäºå®¡æŸ¥**: V6æ•°æ®åº“è®¾è®¡æ·±åº¦åˆ†æ  
> **ä¼˜å…ˆçº§åˆ†ç±»**: P0(é˜»æ–­) > P1(ä¸¥é‡) > P2(é‡è¦) > P3(ä¼˜åŒ–)

---

## ğŸ¯ é¡¹ç›®æ„¿æ™¯ç†è§£

åŸºäºå®¡æŸ¥åˆ†æï¼ŒTH-Suiteçš„æ ¸å¿ƒåŠŸèƒ½åº”è¯¥æ˜¯ï¼š

### ğŸ® Minecraftæœ¬åœ°åŒ–å·¥ä½œæµ
```
æ‰«æMOD/ç»„åˆåŒ… â†’ æå–ç¿»è¯‘é”®å€¼ â†’ ä¸Šä¼ åˆ°æœåŠ¡å™¨ â†’ 
ç¿»è¯‘å¤„ç† â†’ ä¸‹è½½è¡¥ä¸ â†’ å®‰å…¨å›å†™ â†’ éªŒæ”¶ä¸å›æ»š
```

### ğŸ”‘ å…³é”®æŠ€æœ¯ç‰¹æ€§
- **å†…å®¹å¯»å€**: åŸºäºBLAKE3çš„å»é‡å­˜å‚¨
- **ç»Ÿä¸€æ ‡è¯†**: UIDAæ¶æ„æ”¯æŒè·¨ç‰ˆæœ¬ä¸€è‡´æ€§
- **æ™ºèƒ½åŒæ­¥**: Bloomè¿‡æ»¤å™¨ä¼˜åŒ–çš„å¢é‡ä¼ è¾“
- **3-wayåˆå¹¶**: æ”¯æŒå†²çªæ£€æµ‹å’Œè§£å†³
- **å®‰å…¨å›å†™**: Overlayä¼˜å…ˆï¼Œæ”¯æŒJARä¿®æ”¹å’Œå›æ»š

---

## ğŸš¨ P0çº§ä¿®å¤éœ€æ±‚ï¼ˆé˜»æ–­é—®é¢˜ï¼‰

### 1. æ•°æ®ä¸€è‡´æ€§ç³»ç»Ÿé‡æ„ âš¡

**é—®é¢˜**: UIDAå“ˆå¸Œä¸CIDæ˜ å°„æœºåˆ¶å­˜åœ¨æ ¹æœ¬æ€§ç¼ºé™·

**ç°çŠ¶é—®é¢˜**:
```python
# å½“å‰é”™è¯¯å®ç° - sync_service.py:72-75
if cid.startswith("blake3:"):
    uida_hash = cid[7:]  # å±é™©ï¼šç›´æ¥æˆªå–
    entries = await self.translation_repo.find_by_uida_hash(uida_hash)
```

**ä¿®å¤æ–¹æ¡ˆ**:
```python
class UidaCidMappingService:
    """UIDA-CIDæ˜ å°„æœåŠ¡"""
    
    def __init__(self, uida_service, content_addressing):
        self.uida_service = uida_service
        self.content_addressing = content_addressing
        self.mapping_cache = LRUCache(maxsize=10000)
    
    async def generate_consistent_mapping(
        self, 
        translation_entry: TranslationEntry,
        mod_id: str,
        locale: str
    ) -> Tuple[ContentId, UidaComponents]:
        """ç”Ÿæˆä¸€è‡´çš„UIDA-CIDæ˜ å°„"""
        
        # 1. ç”Ÿæˆæ ‡å‡†åŒ–çš„æ¡ç›®æ•°æ®
        normalized_data = self._normalize_entry_data(translation_entry)
        
        # 2. è®¡ç®—å†…å®¹CID
        cid = self.content_addressing.compute_cid(normalized_data)
        
        # 3. ç”ŸæˆUIDAæ ‡è¯†ç¬¦
        uida = self.uida_service.generate_translation_entry_uida(
            mod_id=mod_id,
            translation_key=translation_entry.key,
            locale=locale
        )
        
        # 4. éªŒè¯æ˜ å°„ä¸€è‡´æ€§
        self._validate_mapping_consistency(cid, uida, normalized_data)
        
        # 5. ç¼“å­˜æ˜ å°„å…³ç³»
        self._cache_mapping(cid, uida)
        
        return cid, uida
    
    def _normalize_entry_data(self, entry: TranslationEntry) -> Dict[str, Any]:
        """æ ‡å‡†åŒ–æ¡ç›®æ•°æ®ä»¥ç¡®ä¿ä¸€è‡´çš„CIDè®¡ç®—"""
        return {
            "key": entry.key,
            "src_text": entry.src_text,
            "dst_text": entry.dst_text or "",
            "status": entry.status,
            # æ’é™¤æ—¶é—´æˆ³ç­‰æ˜“å˜å­—æ®µ
            "metadata": {
                "language_file_uid": entry.language_file_uid,
                "qa_flags": entry.qa_flags or {}
            }
        }
    
    async def verify_mapping_integrity(
        self, 
        cid: ContentId, 
        uida_hash: str,
        content_data: bytes
    ) -> bool:
        """éªŒè¯CID-UIDAæ˜ å°„çš„å®Œæ•´æ€§"""
        
        # 1. éªŒè¯CID
        expected_cid = self.content_addressing.compute_cid(content_data)
        if expected_cid != cid:
            logger.error("CIDéªŒè¯å¤±è´¥", 
                        expected=str(expected_cid), 
                        actual=str(cid))
            return False
        
        # 2. æŸ¥æ‰¾æ˜ å°„å…³ç³»
        cached_mapping = self._get_cached_mapping(cid)
        if cached_mapping and cached_mapping.uida_hash != uida_hash:
            logger.error("UIDAæ˜ å°„ä¸ä¸€è‡´", 
                        cached=cached_mapping.uida_hash,
                        provided=uida_hash)
            return False
        
        return True
```

**æ•°æ®åº“Schemaä¿®æ”¹**:
```sql
-- æ–°å¢æ˜ å°„å…³ç³»è¡¨
CREATE TABLE cid_uida_mappings (
    id INTEGER PRIMARY KEY,
    cid_algorithm TEXT NOT NULL,
    cid_hash TEXT NOT NULL,
    uida_hash TEXT NOT NULL,
    content_size INTEGER NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_verified TIMESTAMP,
    verification_count INTEGER DEFAULT 0,
    
    UNIQUE(cid_algorithm, cid_hash, uida_hash),
    INDEX idx_cid_lookup (cid_algorithm, cid_hash),
    INDEX idx_uida_lookup (uida_hash)
);
```

### 2. ä¼šè¯æŒä¹…åŒ–ç³»ç»Ÿ ğŸ’¾

**é—®é¢˜**: åŒæ­¥ä¼šè¯å’Œåˆ†ç‰‡æ•°æ®å®Œå…¨å­˜å‚¨åœ¨å†…å­˜ä¸­

**ä¿®å¤æ–¹æ¡ˆ**:
```python
class PersistentSyncSessionManager:
    """æŒä¹…åŒ–åŒæ­¥ä¼šè¯ç®¡ç†å™¨"""
    
    def __init__(self, db_manager, storage_backend="sqlite"):
        self.db_manager = db_manager
        self.storage_backend = storage_backend
        self.session_cache = LRUCache(maxsize=100)
    
    async def create_session(
        self, 
        client_id: str, 
        session_id: str,
        ttl_hours: int = 1
    ) -> PersistentSyncSession:
        """åˆ›å»ºæŒä¹…åŒ–ä¼šè¯"""
        
        session = PersistentSyncSession(
            session_id=session_id,
            client_id=client_id,
            status="active",
            created_at=datetime.now().isoformat(),
            expires_at=(datetime.now() + timedelta(hours=ttl_hours)).isoformat(),
            metadata={}
        )
        
        # å­˜å‚¨åˆ°æ•°æ®åº“
        await self._store_session(session)
        
        # åˆ›å»ºåˆ†ç‰‡å­˜å‚¨ç›®å½•
        await self._create_chunk_storage(session_id)
        
        return session
    
    async def store_chunk(
        self, 
        session_id: str, 
        cid: str, 
        chunk_index: int,
        chunk_data: bytes
    ) -> bool:
        """æŒä¹…åŒ–å­˜å‚¨åˆ†ç‰‡æ•°æ®"""
        
        # éªŒè¯ä¼šè¯
        session = await self.get_session(session_id)
        if not session or session.status != "active":
            return False
        
        # è®¡ç®—åˆ†ç‰‡å­˜å‚¨è·¯å¾„
        chunk_path = self._get_chunk_path(session_id, cid, chunk_index)
        
        try:
            # åŸå­å†™å…¥
            temp_path = f"{chunk_path}.tmp"
            async with aiofiles.open(temp_path, 'wb') as f:
                await f.write(chunk_data)
            
            # åŸå­ç§»åŠ¨
            os.rename(temp_path, chunk_path)
            
            # æ›´æ–°ä¼šè¯ç»Ÿè®¡
            await self._update_session_progress(session_id, cid, chunk_index)
            
            return True
            
        except Exception as e:
            logger.error("åˆ†ç‰‡å­˜å‚¨å¤±è´¥", 
                        session_id=session_id,
                        cid=cid, 
                        chunk_index=chunk_index,
                        error=str(e))
            return False
    
    async def reconstruct_content(
        self, 
        session_id: str, 
        cid: str, 
        total_chunks: int
    ) -> Optional[bytes]:
        """é‡å»ºå®Œæ•´å†…å®¹å¹¶éªŒè¯"""
        
        chunks = []
        
        for i in range(total_chunks):
            chunk_path = self._get_chunk_path(session_id, cid, i)
            
            if not os.path.exists(chunk_path):
                logger.error("åˆ†ç‰‡ç¼ºå¤±", 
                           session_id=session_id,
                           cid=cid, 
                           chunk_index=i)
                return None
            
            try:
                async with aiofiles.open(chunk_path, 'rb') as f:
                    chunk_data = await f.read()
                chunks.append(chunk_data)
            except Exception as e:
                logger.error("åˆ†ç‰‡è¯»å–å¤±è´¥", 
                           session_id=session_id,
                           cid=cid,
                           chunk_index=i, 
                           error=str(e))
                return None
        
        # é‡å»ºå®Œæ•´å†…å®¹
        full_content = b''.join(chunks)
        
        # CIDå®Œæ•´æ€§éªŒè¯
        expected_cid = compute_cid(full_content, HashAlgorithm.BLAKE3)
        if str(expected_cid) != cid:
            logger.error("å†…å®¹å®Œæ•´æ€§éªŒè¯å¤±è´¥",
                       session_id=session_id,
                       expected_cid=str(expected_cid),
                       actual_cid=cid)
            return None
        
        return full_content
```

**æ•°æ®åº“Schema**:
```sql
-- æŒä¹…åŒ–ä¼šè¯è¡¨
CREATE TABLE sync_sessions (
    session_id TEXT PRIMARY KEY,
    client_id TEXT NOT NULL,
    status TEXT NOT NULL CHECK (status IN ('active', 'completed', 'expired', 'error')),
    created_at TIMESTAMP NOT NULL,
    expires_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    total_cids INTEGER DEFAULT 0,
    completed_cids INTEGER DEFAULT 0,
    chunk_size_bytes INTEGER NOT NULL,
    metadata JSON,
    
    INDEX idx_client_sessions (client_id, created_at),
    INDEX idx_active_sessions (status, expires_at)
);

-- åˆ†ç‰‡è¿›åº¦è¿½è¸ªè¡¨
CREATE TABLE session_chunk_progress (
    id INTEGER PRIMARY KEY,
    session_id TEXT NOT NULL,
    cid TEXT NOT NULL,
    total_chunks INTEGER NOT NULL,
    received_chunks INTEGER DEFAULT 0,
    storage_path TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    FOREIGN KEY (session_id) REFERENCES sync_sessions(session_id),
    UNIQUE (session_id, cid)
);
```

### 3. ç«¯åˆ°ç«¯æ•°æ®éªŒè¯ç³»ç»Ÿ ğŸ”’

**é—®é¢˜**: åˆ†ç‰‡é‡ç»„åç¼ºä¹å®Œæ•´æ€§éªŒè¯

**ä¿®å¤æ–¹æ¡ˆ**:
```python
class ContentIntegrityValidator:
    """å†…å®¹å®Œæ•´æ€§éªŒè¯å™¨"""
    
    def __init__(self):
        self.validation_cache = LRUCache(maxsize=1000)
    
    async def validate_upload_pipeline(
        self,
        chunks: List[bytes],
        expected_cid: str,
        expected_uida_hash: str,
        metadata: Dict[str, Any]
    ) -> ValidationResult:
        """ç«¯åˆ°ç«¯ä¸Šä¼ æµæ°´çº¿éªŒè¯"""
        
        result = ValidationResult()
        
        try:
            # 1. åˆ†ç‰‡å®Œæ•´æ€§æ£€æŸ¥
            chunk_validation = await self._validate_chunks(chunks)
            result.add_check("chunk_integrity", chunk_validation)
            
            # 2. å†…å®¹é‡å»ºéªŒè¯
            full_content = b''.join(chunks)
            content_validation = await self._validate_content_cid(
                full_content, expected_cid
            )
            result.add_check("content_cid", content_validation)
            
            # 3. UIDAä¸€è‡´æ€§éªŒè¯
            uida_validation = await self._validate_uida_consistency(
                full_content, expected_uida_hash, metadata
            )
            result.add_check("uida_consistency", uida_validation)
            
            # 4. Entry-Deltaæ ¼å¼éªŒè¯
            delta_validation = await self._validate_entry_delta_format(
                full_content
            )
            result.add_check("delta_format", delta_validation)
            
            # 5. ä¸šåŠ¡è§„åˆ™éªŒè¯
            business_validation = await self._validate_business_rules(
                full_content, metadata
            )
            result.add_check("business_rules", business_validation)
            
        except Exception as e:
            result.add_error(f"éªŒè¯è¿‡ç¨‹å¼‚å¸¸: {str(e)}")
        
        return result
    
    async def _validate_chunks(self, chunks: List[bytes]) -> CheckResult:
        """éªŒè¯åˆ†ç‰‡å®Œæ•´æ€§"""
        
        if not chunks:
            return CheckResult(False, "åˆ†ç‰‡åˆ—è¡¨ä¸ºç©º")
        
        # æ£€æŸ¥æ¯ä¸ªåˆ†ç‰‡çš„BLAKE3å“ˆå¸Œ
        for i, chunk in enumerate(chunks):
            if not chunk:
                return CheckResult(False, f"åˆ†ç‰‡{i}ä¸ºç©º")
            
            # è®¡ç®—åˆ†ç‰‡å“ˆå¸Œå¹¶éªŒè¯
            chunk_hash = blake3.blake3(chunk).hexdigest()
            # è¿™é‡Œéœ€è¦ä¸ä¸Šä¼ æ—¶æä¾›çš„åˆ†ç‰‡å“ˆå¸Œè¿›è¡Œæ¯”å¯¹
            
        return CheckResult(True, f"æ‰€æœ‰{len(chunks)}ä¸ªåˆ†ç‰‡éªŒè¯é€šè¿‡")
    
    async def _validate_entry_delta_format(self, content: bytes) -> CheckResult:
        """éªŒè¯Entry-Deltaæ ¼å¼"""
        
        try:
            from api.v6.sync.entry_delta import get_entry_delta_processor
            processor = get_entry_delta_processor()
            
            deltas = processor.parse_delta_payload(content)
            
            if not deltas:
                return CheckResult(False, "Entry-Deltaåˆ—è¡¨ä¸ºç©º")
            
            # éªŒè¯æ¯ä¸ªdeltaçš„å¿…éœ€å­—æ®µ
            for i, delta in enumerate(deltas):
                if not delta.entry_uid:
                    return CheckResult(False, f"Delta{i}ç¼ºå°‘entry_uid")
                if not delta.uida_hash:
                    return CheckResult(False, f"Delta{i}ç¼ºå°‘uida_hash")
                if not delta.key:
                    return CheckResult(False, f"Delta{i}ç¼ºå°‘translation key")
            
            return CheckResult(True, f"Entry-Deltaæ ¼å¼éªŒè¯é€šè¿‡ï¼ŒåŒ…å«{len(deltas)}ä¸ªæ¡ç›®")
            
        except Exception as e:
            return CheckResult(False, f"Entry-Deltaè§£æå¤±è´¥: {str(e)}")

@dataclass
class ValidationResult:
    """éªŒè¯ç»“æœ"""
    checks: Dict[str, CheckResult] = field(default_factory=dict)
    errors: List[str] = field(default_factory=list)
    warnings: List[str] = field(default_factory=list)
    
    def add_check(self, name: str, result: CheckResult):
        self.checks[name] = result
        if not result.passed:
            self.errors.append(f"{name}: {result.message}")
    
    def add_error(self, error: str):
        self.errors.append(error)
    
    def is_valid(self) -> bool:
        return len(self.errors) == 0 and all(
            check.passed for check in self.checks.values()
        )
```

---

## ğŸŸ¡ P1çº§ä¿®å¤éœ€æ±‚ï¼ˆä¸¥é‡é—®é¢˜ï¼‰

### 4. æ™ºèƒ½3-wayåˆå¹¶ç³»ç»Ÿ

**é—®é¢˜**: ç¼ºå°‘baseç‰ˆæœ¬ä¿¡æ¯ï¼Œå†²çªè§£å†³ä¸å®Œå–„

**ä¿®å¤æ–¹æ¡ˆ**:
```python
class Enhanced3WayMerger:
    """å¢å¼ºçš„3-wayåˆå¹¶å™¨"""
    
    async def perform_intelligent_merge(
        self,
        merge_context: Enhanced3WayContext
    ) -> Enhanced3WayResult:
        """æ‰§è¡Œæ™ºèƒ½3-wayåˆå¹¶"""
        
        # 1. æ„å»ºå®Œæ•´çš„ç‰ˆæœ¬å†å²
        version_history = await self._build_version_history(merge_context)
        
        # 2. è¯†åˆ«çœŸæ­£çš„baseç‰ˆæœ¬
        true_base = await self._identify_true_base(version_history)
        
        # 3. æ‰§è¡Œè¯­ä¹‰çº§åˆ«çš„å·®å¼‚åˆ†æ
        semantic_diff = await self._analyze_semantic_differences(
            true_base, merge_context.local_entry, merge_context.remote_entry
        )
        
        # 4. åº”ç”¨æ™ºèƒ½åˆå¹¶ç­–ç•¥
        merge_result = await self._apply_intelligent_strategies(
            semantic_diff, merge_context.merge_options
        )
        
        return merge_result
    
    async def _analyze_semantic_differences(
        self,
        base: TranslationEntry,
        local: TranslationEntry,
        remote: TranslationEntry
    ) -> SemanticDiff:
        """åˆ†æè¯­ä¹‰çº§å·®å¼‚"""
        
        diff = SemanticDiff()
        
        # æ–‡æœ¬è¯­ä¹‰å˜åŒ–åˆ†æ
        if base and local:
            local_changes = self._analyze_text_changes(base.dst_text, local.dst_text)
            diff.local_changes = local_changes
        
        if base and remote:
            remote_changes = self._analyze_text_changes(base.dst_text, remote.dst_text)
            diff.remote_changes = remote_changes
        
        # å†²çªç±»å‹åˆ†ç±»
        diff.conflict_type = self._classify_conflict(diff.local_changes, diff.remote_changes)
        
        # ç”Ÿæˆåˆå¹¶å»ºè®®
        diff.merge_suggestions = self._generate_merge_suggestions(diff)
        
        return diff
    
    def _classify_conflict(
        self, 
        local_changes: TextChanges, 
        remote_changes: TextChanges
    ) -> ConflictType:
        """åˆ†ç±»å†²çªç±»å‹"""
        
        if local_changes.is_formatting_only and remote_changes.is_formatting_only:
            return ConflictType.FORMATTING_CONFLICT
        elif local_changes.is_terminology_change or remote_changes.is_terminology_change:
            return ConflictType.TERMINOLOGY_CONFLICT
        elif local_changes.has_placeholder_changes or remote_changes.has_placeholder_changes:
            return ConflictType.PLACEHOLDER_CONFLICT
        else:
            return ConflictType.CONTENT_CONFLICT
```

### 5. Bloomè¿‡æ»¤å™¨ä¼˜åŒ–ç³»ç»Ÿ

**ä¿®å¤æ–¹æ¡ˆ**:
```python
class AdaptiveBloomFilter:
    """è‡ªé€‚åº”Bloomè¿‡æ»¤å™¨"""
    
    def __init__(self, target_fpr: float = 0.001):
        self.target_fpr = target_fpr
        self.optimization_enabled = True
    
    async def optimize_for_dataset(
        self, 
        expected_items: int,
        available_memory: int
    ) -> BloomConfig:
        """æ ¹æ®æ•°æ®é›†ä¼˜åŒ–Bloomè¿‡æ»¤å™¨é…ç½®"""
        
        # è®¡ç®—æœ€ä¼˜å‚æ•°
        optimal_bits = self._calculate_optimal_bits(expected_items, self.target_fpr)
        optimal_hashes = self._calculate_optimal_hashes(optimal_bits, expected_items)
        
        # å†…å­˜çº¦æŸæ£€æŸ¥
        required_memory = optimal_bits // 8
        if required_memory > available_memory:
            # å†…å­˜å—é™æ—¶é™çº§å¤„ç†
            optimal_bits = available_memory * 8
            optimal_hashes = self._calculate_optimal_hashes(optimal_bits, expected_items)
            actual_fpr = self._calculate_actual_fpr(optimal_bits, optimal_hashes, expected_items)
            logger.warning("å†…å­˜é™åˆ¶å¯¼è‡´FPRä¸Šå‡", 
                         target_fpr=self.target_fpr,
                         actual_fpr=actual_fpr)
        
        return BloomConfig(
            bits=optimal_bits,
            hashes=optimal_hashes,
            expected_items=expected_items,
            target_fpr=self.target_fpr
        )
    
    async def handle_false_positives(
        self,
        client_bloom: BloomFilter,
        server_cids: List[str],
        client_actual_cids: Set[str]
    ) -> FalsePositiveReport:
        """å¤„ç†å‡é˜³æ€§æƒ…å†µ"""
        
        false_positives = []
        
        for cid in server_cids:
            if client_bloom.might_contain(cid):
                # Bloomè¯´å®¢æˆ·ç«¯æœ‰ï¼ŒéªŒè¯å®¢æˆ·ç«¯æ˜¯å¦çœŸçš„æœ‰
                if cid not in client_actual_cids:
                    false_positives.append(cid)
        
        # ç”Ÿæˆå‡é˜³æ€§æŠ¥å‘Š
        report = FalsePositiveReport(
            total_server_cids=len(server_cids),
            bloom_positive_count=len([cid for cid in server_cids if client_bloom.might_contain(cid)]),
            false_positive_count=len(false_positives),
            actual_fpr=len(false_positives) / len(server_cids) if server_cids else 0,
            false_positive_cids=false_positives[:10]  # åªè®°å½•å‰10ä¸ªç”¨äºè°ƒè¯•
        )
        
        return report
```

### 6. æ•°æ®åº“å¹¶å‘ä¼˜åŒ–

**ä¿®å¤æ–¹æ¡ˆ**:
```python
class OptimizedDatabaseManager:
    """ä¼˜åŒ–çš„æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str, connection_pool_size: int = 10):
        self.db_path = db_path
        self.connection_pool = None
        self.write_queue = asyncio.Queue(maxsize=1000)
        self.batch_processor = None
    
    async def initialize(self):
        """åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†å™¨"""
        
        # åˆ›å»ºè¿æ¥æ± 
        self.connection_pool = await self._create_connection_pool()
        
        # å¯åŠ¨æ‰¹é‡å†™å…¥å¤„ç†å™¨
        self.batch_processor = asyncio.create_task(self._process_write_batches())
        
        # ä¼˜åŒ–SQLiteé…ç½®
        await self._optimize_sqlite_settings()
    
    async def _optimize_sqlite_settings(self):
        """ä¼˜åŒ–SQLiteè®¾ç½®"""
        
        async with self.get_write_connection() as conn:
            # ä¼˜åŒ–è®¾ç½®
            await conn.execute("PRAGMA journal_mode=WAL")  # Write-Ahead Logging
            await conn.execute("PRAGMA synchronous=NORMAL")  # å¹³è¡¡æ€§èƒ½å’Œå®‰å…¨
            await conn.execute("PRAGMA cache_size=10000")   # å¢åŠ ç¼“å­˜
            await conn.execute("PRAGMA temp_store=MEMORY")   # ä¸´æ—¶è¡¨å­˜å‚¨åœ¨å†…å­˜
            await conn.execute("PRAGMA mmap_size=268435456") # 256MBå†…å­˜æ˜ å°„
            await conn.commit()
    
    async def batch_insert_entries(
        self, 
        entries: List[TranslationEntry]
    ) -> int:
        """æ‰¹é‡æ’å…¥ç¿»è¯‘æ¡ç›®"""
        
        if not entries:
            return 0
        
        # ä½¿ç”¨äº‹åŠ¡æ‰¹é‡æ’å…¥
        async with self.get_write_connection() as conn:
            try:
                await conn.execute("BEGIN TRANSACTION")
                
                insert_sql = """
                INSERT OR REPLACE INTO core_translation_entries 
                (uid, language_file_uid, key, src_text, dst_text, status, 
                 qa_flags, updated_at, uida_keys_b64, uida_hash, created_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                """
                
                batch_data = [
                    (
                        entry.uid, entry.language_file_uid, entry.key,
                        entry.src_text, entry.dst_text, entry.status,
                        json.dumps(entry.qa_flags or {}), entry.updated_at,
                        entry.uida_keys_b64, entry.uida_hash, entry.created_at
                    )
                    for entry in entries
                ]
                
                await conn.executemany(insert_sql, batch_data)
                await conn.execute("COMMIT")
                
                return len(entries)
                
            except Exception as e:
                await conn.execute("ROLLBACK")
                logger.error("æ‰¹é‡æ’å…¥å¤±è´¥", error=str(e))
                raise
```

---

## ğŸŸ  P2çº§ä¿®å¤éœ€æ±‚ï¼ˆé‡è¦ä¼˜åŒ–ï¼‰

### 7. æ€§èƒ½ç›‘æ§ç³»ç»Ÿ

**å»ºè®®å®ç°**:
```python
class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§ç³»ç»Ÿ"""
    
    def __init__(self):
        self.metrics = defaultdict(list)
        self.alerts = []
    
    async def monitor_sync_performance(
        self,
        session_id: str,
        operation: str,
        duration: float,
        data_size: int
    ):
        """ç›‘æ§åŒæ­¥æ€§èƒ½"""
        
        metric = PerformanceMetric(
            session_id=session_id,
            operation=operation,
            duration=duration,
            data_size=data_size,
            timestamp=time.time(),
            throughput=data_size / duration if duration > 0 else 0
        )
        
        self.metrics[operation].append(metric)
        
        # æ€§èƒ½å‘Šè­¦æ£€æŸ¥
        await self._check_performance_alerts(operation, metric)
    
    async def generate_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "operations": {},
            "summary": {},
            "alerts": self.alerts[-10:]  # æœ€è¿‘10ä¸ªå‘Šè­¦
        }
        
        for operation, metrics in self.metrics.items():
            if metrics:
                durations = [m.duration for m in metrics]
                throughputs = [m.throughput for m in metrics if m.throughput > 0]
                
                report["operations"][operation] = {
                    "count": len(metrics),
                    "avg_duration": statistics.mean(durations),
                    "p95_duration": statistics.quantiles(durations, n=20)[18] if len(durations) > 1 else durations[0],
                    "avg_throughput": statistics.mean(throughputs) if throughputs else 0,
                    "total_data_size": sum(m.data_size for m in metrics)
                }
        
        return report
```

### 8. å®¹é”™ä¸æ¢å¤ç³»ç»Ÿ

**å»ºè®®å®ç°**:
```python
class FaultToleranceManager:
    """å®¹é”™ä¸æ¢å¤ç®¡ç†å™¨"""
    
    async def recover_interrupted_session(
        self,
        session_id: str
    ) -> RecoveryResult:
        """æ¢å¤ä¸­æ–­çš„åŒæ­¥ä¼šè¯"""
        
        try:
            # 1. æŸ¥æ‰¾ä¼šè¯çŠ¶æ€
            session = await self._find_interrupted_session(session_id)
            if not session:
                return RecoveryResult(False, "ä¼šè¯ä¸å­˜åœ¨")
            
            # 2. æ£€æŸ¥å·²å­˜å‚¨çš„åˆ†ç‰‡
            stored_chunks = await self._scan_stored_chunks(session_id)
            
            # 3. éªŒè¯å·²å­˜å‚¨åˆ†ç‰‡çš„å®Œæ•´æ€§
            validated_chunks = await self._validate_stored_chunks(stored_chunks)
            
            # 4. é‡å»ºå¯æ¢å¤çš„å†…å®¹
            recoverable_cids = await self._identify_recoverable_cids(validated_chunks)
            
            # 5. æ›´æ–°ä¼šè¯æ¢å¤çŠ¶æ€
            recovery_session = await self._create_recovery_session(
                session, recoverable_cids
            )
            
            return RecoveryResult(
                True, 
                f"æ¢å¤{len(recoverable_cids)}ä¸ªCIDï¼Œéœ€è¦é‡ä¼ å…¶ä½™å†…å®¹",
                recovery_session
            )
            
        except Exception as e:
            logger.error("ä¼šè¯æ¢å¤å¤±è´¥", session_id=session_id, error=str(e))
            return RecoveryResult(False, f"æ¢å¤å¤±è´¥: {str(e)}")
```

---

## ğŸ”§ å®æ–½å»ºè®®

### ç«‹å³è¡ŒåŠ¨è®¡åˆ’ï¼ˆ1-2å‘¨ï¼‰

1. **åœç”¨å½“å‰V6ç”Ÿäº§ç¯å¢ƒ**
2. **å®æ–½P0çº§ä¿®å¤**ï¼š
   - UIDA-CIDæ˜ å°„ç³»ç»Ÿé‡æ„
   - ä¼šè¯æŒä¹…åŒ–å®ç°
   - ç«¯åˆ°ç«¯éªŒè¯ç³»ç»Ÿ

### çŸ­æœŸè®¡åˆ’ï¼ˆ2-4å‘¨ï¼‰

1. **P1çº§é—®é¢˜ä¿®å¤**ï¼š
   - æ™ºèƒ½3-wayåˆå¹¶
   - Bloomè¿‡æ»¤å™¨ä¼˜åŒ–
   - æ•°æ®åº“å¹¶å‘ä¼˜åŒ–

### ä¸­æœŸè®¡åˆ’ï¼ˆ1-2æœˆï¼‰

1. **P2çº§ä¼˜åŒ–å®æ–½**ï¼š
   - æ€§èƒ½ç›‘æ§ç³»ç»Ÿ
   - å®¹é”™æ¢å¤æœºåˆ¶
   - å…¨é¢æµ‹è¯•è¦†ç›–

### è´¨é‡ä¿è¯

1. **æµ‹è¯•ç­–ç•¥**ï¼š
   - å•å…ƒæµ‹è¯•è¦†ç›–ç‡ â‰¥ 90%
   - é›†æˆæµ‹è¯•è¦†ç›–å…³é”®æ•°æ®æµ
   - å‹åŠ›æµ‹è¯•éªŒè¯å¹¶å‘æ€§èƒ½
   - ç¾éš¾æ¢å¤æµ‹è¯•

2. **ç›‘æ§æŒ‡æ ‡**ï¼š
   - æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
   - åŒæ­¥æˆåŠŸç‡
   - å¹³å‡å“åº”æ—¶é—´
   - é”™è¯¯ç‡å’Œæ¢å¤æ—¶é—´

**ç»“è®º**: TH-Suiteå…·æœ‰å¾ˆå¼ºçš„æŠ€æœ¯æ½œåŠ›ï¼Œä½†å½“å‰V6å®ç°å­˜åœ¨ä¸¥é‡çš„æ•°æ®å®‰å…¨é—®é¢˜ã€‚æŒ‰ç…§æ­¤ä¿®å¤è®¡åˆ’å®æ–½åï¼Œå°†æˆä¸ºä¸€ä¸ªå¯é çš„Minecraftæœ¬åœ°åŒ–å·¥å…·å¹³å°ã€‚