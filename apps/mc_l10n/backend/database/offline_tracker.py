#!/usr/bin/env python
"""
ç¦»çº¿å˜æ›´è·Ÿè¸ªå™¨
è·Ÿè¸ªæ‰€æœ‰ç¦»çº¿çŠ¶æ€ä¸‹çš„æ•°æ®å˜æ›´ï¼Œä»¥ä¾¿åœ¨çº¿æ—¶åŒæ­¥
"""

import sqlite3
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional
from enum import Enum
import logging
from contextlib import contextmanager

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ChangeOperation(Enum):
    """å˜æ›´æ“ä½œç±»å‹"""
    CREATE = "create"
    UPDATE = "update"
    DELETE = "delete"
    MERGE = "merge"


class EntityType(Enum):
    """å®ä½“ç±»å‹"""
    PROJECT = "project"
    MOD = "mod"
    TRANSLATION = "translation"
    TERMINOLOGY = "terminology"
    SETTING = "setting"


class OfflineChangeTracker:
    """ç¦»çº¿å˜æ›´è·Ÿè¸ªå™¨"""
    
    def __init__(self, db_path: str = "mc_l10n_local.db"):
        self.db_path = Path(db_path)
        self.conn = sqlite3.connect(str(self.db_path), check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self.conn.execute("PRAGMA foreign_keys = ON")
        
        # å¯ç”¨è§¦å‘å™¨è‡ªåŠ¨è·Ÿè¸ª
        self.setup_triggers()
        
    @contextmanager
    def transaction(self):
        """äº‹åŠ¡ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        try:
            yield self.conn
            self.conn.commit()
        except Exception as e:
            self.conn.rollback()
            logger.error(f"äº‹åŠ¡å¤±è´¥: {e}")
            raise
            
    def setup_triggers(self):
        """è®¾ç½®è‡ªåŠ¨è·Ÿè¸ªè§¦å‘å™¨"""
        
        # ç¿»è¯‘æ¡ç›®å˜æ›´è§¦å‘å™¨
        self.conn.execute("""
            CREATE TRIGGER IF NOT EXISTS track_translation_update
            AFTER UPDATE ON translation_entry_cache
            WHEN NEW.translated_text != OLD.translated_text
            BEGIN
                INSERT INTO offline_changes (
                    entity_type, entity_id, operation, change_data
                ) VALUES (
                    'translation',
                    NEW.entry_id,
                    'update',
                    json_object(
                        'cache_id', NEW.cache_id,
                        'translation_key', NEW.translation_key,
                        'old_text', OLD.translated_text,
                        'new_text', NEW.translated_text,
                        'status', NEW.status
                    )
                );
            END
        """)
        
        # é¡¹ç›®åˆ›å»ºè§¦å‘å™¨
        self.conn.execute("""
            CREATE TRIGGER IF NOT EXISTS track_project_create
            AFTER INSERT ON local_projects
            WHEN NEW.project_id != 'default-project'
            BEGIN
                INSERT INTO offline_changes (
                    entity_type, entity_id, operation, change_data
                ) VALUES (
                    'project',
                    NEW.project_id,
                    'create',
                    json_object(
                        'project_name', NEW.project_name,
                        'target_language', NEW.target_language,
                        'source_language', NEW.source_language
                    )
                );
            END
        """)
        
        # é¡¹ç›®æ›´æ–°è§¦å‘å™¨
        self.conn.execute("""
            CREATE TRIGGER IF NOT EXISTS track_project_update
            AFTER UPDATE ON local_projects
            WHEN NEW.project_id != 'default-project'
            BEGIN
                INSERT INTO offline_changes (
                    entity_type, entity_id, operation, change_data
                ) VALUES (
                    'project',
                    NEW.project_id,
                    'update',
                    json_object(
                        'project_name', NEW.project_name,
                        'scan_paths', NEW.scan_paths,
                        'auto_scan', NEW.auto_scan
                    )
                );
            END
        """)
        
        self.conn.commit()
        logger.info("ç¦»çº¿è·Ÿè¸ªè§¦å‘å™¨å·²è®¾ç½®")
        
    def track_change(self, entity_type: EntityType, entity_id: str, 
                    operation: ChangeOperation, change_data: Dict,
                    conflict_resolution: str = "client_wins"):
        """æ‰‹åŠ¨è·Ÿè¸ªå˜æ›´"""
        
        with self.transaction():
            self.conn.execute("""
                INSERT INTO offline_changes (
                    entity_type, entity_id, operation, 
                    change_data, conflict_resolution
                ) VALUES (?, ?, ?, ?, ?)
            """, (
                entity_type.value,
                entity_id,
                operation.value,
                json.dumps(change_data, ensure_ascii=False),
                conflict_resolution
            ))
            
        logger.info(f"è·Ÿè¸ªå˜æ›´: {entity_type.value}/{entity_id} - {operation.value}")
        
    def batch_track_changes(self, changes: List[Dict]):
        """æ‰¹é‡è·Ÿè¸ªå˜æ›´"""
        
        with self.transaction():
            for change in changes:
                self.conn.execute("""
                    INSERT INTO offline_changes (
                        entity_type, entity_id, operation, 
                        change_data, conflict_resolution
                    ) VALUES (?, ?, ?, ?, ?)
                """, (
                    change['entity_type'],
                    change['entity_id'],
                    change['operation'],
                    json.dumps(change.get('change_data', {}), ensure_ascii=False),
                    change.get('conflict_resolution', 'client_wins')
                ))
                
        logger.info(f"æ‰¹é‡è·Ÿè¸ªäº† {len(changes)} ä¸ªå˜æ›´")
        
    def get_pending_changes(self, entity_type: Optional[EntityType] = None,
                           limit: int = 100) -> List[Dict]:
        """è·å–å¾…åŒæ­¥çš„å˜æ›´"""
        
        query = """
            SELECT * FROM offline_changes 
            WHERE is_synced = 0
        """
        params = []
        
        if entity_type:
            query += " AND entity_type = ?"
            params.append(entity_type.value)
            
        query += " ORDER BY created_at ASC LIMIT ?"
        params.append(limit)
        
        cursor = self.conn.cursor()
        cursor.execute(query, params)
        
        changes = []
        for row in cursor.fetchall():
            changes.append({
                'change_id': row['change_id'],
                'entity_type': row['entity_type'],
                'entity_id': row['entity_id'],
                'operation': row['operation'],
                'change_data': json.loads(row['change_data']) if row['change_data'] else {},
                'conflict_resolution': row['conflict_resolution'],
                'created_at': row['created_at']
            })
            
        return changes
        
    def mark_changes_synced(self, change_ids: List[str]):
        """æ ‡è®°å˜æ›´ä¸ºå·²åŒæ­¥"""
        
        if not change_ids:
            return
            
        placeholders = ','.join(['?'] * len(change_ids))
        
        with self.transaction():
            self.conn.execute(f"""
                UPDATE offline_changes 
                SET is_synced = 1, synced_at = CURRENT_TIMESTAMP
                WHERE change_id IN ({placeholders})
            """, change_ids)
            
        logger.info(f"æ ‡è®° {len(change_ids)} ä¸ªå˜æ›´ä¸ºå·²åŒæ­¥")
        
    def get_change_summary(self) -> Dict:
        """è·å–å˜æ›´æ‘˜è¦"""
        
        cursor = self.conn.cursor()
        
        # æŒ‰å®ä½“ç±»å‹ç»Ÿè®¡
        cursor.execute("""
            SELECT 
                entity_type,
                COUNT(*) as total,
                SUM(CASE WHEN is_synced = 0 THEN 1 ELSE 0 END) as pending,
                SUM(CASE WHEN is_synced = 1 THEN 1 ELSE 0 END) as synced
            FROM offline_changes
            GROUP BY entity_type
        """)
        
        by_entity = {}
        for row in cursor.fetchall():
            by_entity[row['entity_type']] = {
                'total': row['total'],
                'pending': row['pending'],
                'synced': row['synced']
            }
            
        # æŒ‰æ“ä½œç±»å‹ç»Ÿè®¡
        cursor.execute("""
            SELECT 
                operation,
                COUNT(*) as count
            FROM offline_changes
            WHERE is_synced = 0
            GROUP BY operation
        """)
        
        by_operation = {}
        for row in cursor.fetchall():
            by_operation[row['operation']] = row['count']
            
        # æœ€è¿‘å˜æ›´
        cursor.execute("""
            SELECT * FROM offline_changes
            ORDER BY created_at DESC
            LIMIT 10
        """)
        
        recent = []
        for row in cursor.fetchall():
            recent.append({
                'entity_type': row['entity_type'],
                'entity_id': row['entity_id'],
                'operation': row['operation'],
                'created_at': row['created_at'],
                'is_synced': row['is_synced']
            })
            
        return {
            'by_entity': by_entity,
            'by_operation': by_operation,
            'recent_changes': recent,
            'total_pending': sum(e['pending'] for e in by_entity.values())
        }
        
    def resolve_conflicts(self, local_changes: List[Dict], server_changes: List[Dict]) -> List[Dict]:
        """è§£å†³å†²çª"""
        
        resolved = []
        conflicts = []
        
        # åˆ›å»ºæœåŠ¡å™¨å˜æ›´ç´¢å¼•
        server_index = {}
        for change in server_changes:
            key = f"{change['entity_type']}:{change['entity_id']}"
            server_index[key] = change
            
        for local in local_changes:
            key = f"{local['entity_type']}:{local['entity_id']}"
            
            if key in server_index:
                server = server_index[key]
                
                # æ£€æµ‹å†²çª
                if self.is_conflict(local, server):
                    resolution = local['conflict_resolution']
                    
                    if resolution == 'client_wins':
                        resolved.append(local)
                    elif resolution == 'server_wins':
                        resolved.append(server)
                    elif resolution == 'newest_wins':
                        local_time = datetime.fromisoformat(local['created_at'])
                        server_time = datetime.fromisoformat(server['created_at'])
                        resolved.append(local if local_time > server_time else server)
                    else:  # manual
                        conflicts.append({
                            'local': local,
                            'server': server
                        })
                else:
                    # æ— å†²çªï¼Œåˆå¹¶å˜æ›´
                    merged = self.merge_changes(local, server)
                    resolved.append(merged)
            else:
                # ä»…æœ¬åœ°æœ‰å˜æ›´
                resolved.append(local)
                
        # æ·»åŠ ä»…æœåŠ¡å™¨æœ‰çš„å˜æ›´
        for server in server_changes:
            key = f"{server['entity_type']}:{server['entity_id']}"
            if key not in {f"{l['entity_type']}:{l['entity_id']}" for l in local_changes}:
                resolved.append(server)
                
        if conflicts:
            logger.warning(f"æ£€æµ‹åˆ° {len(conflicts)} ä¸ªå†²çªéœ€è¦æ‰‹åŠ¨è§£å†³")
            
        return resolved
        
    def is_conflict(self, local: Dict, server: Dict) -> bool:
        """æ£€æµ‹æ˜¯å¦å­˜åœ¨å†²çª"""
        
        # ç›¸åŒå®ä½“çš„ä¸åŒæ“ä½œ
        if local['operation'] != server['operation']:
            return True
            
        # UPDATEæ“ä½œæ—¶ï¼Œæ£€æŸ¥æ˜¯å¦ä¿®æ”¹äº†ç›¸åŒå­—æ®µ
        if local['operation'] == 'update':
            local_fields = set(local['change_data'].keys())
            server_fields = set(server['change_data'].keys())
            
            # æœ‰é‡å å­—æ®µä¸”å€¼ä¸åŒ
            common_fields = local_fields & server_fields
            for field in common_fields:
                if local['change_data'][field] != server['change_data'][field]:
                    return True
                    
        return False
        
    def merge_changes(self, local: Dict, server: Dict) -> Dict:
        """åˆå¹¶æ— å†²çªçš„å˜æ›´"""
        
        merged = local.copy()
        
        # åˆå¹¶change_data
        if 'change_data' in server:
            merged['change_data'].update(server['change_data'])
            
        return merged
        
    def cleanup_old_changes(self, days: int = 30):
        """æ¸…ç†æ—§çš„å·²åŒæ­¥å˜æ›´"""
        
        with self.transaction():
            cursor = self.conn.execute("""
                DELETE FROM offline_changes
                WHERE is_synced = 1 
                AND synced_at < datetime('now', '-' || ? || ' days')
            """, (days,))
            
        deleted = cursor.rowcount
        logger.info(f"æ¸…ç†äº† {deleted} ä¸ªæ—§å˜æ›´è®°å½•")
        
    def export_changes(self, file_path: str):
        """å¯¼å‡ºå˜æ›´è®°å½•"""
        
        changes = self.get_pending_changes(limit=10000)
        
        export_data = {
            'export_time': datetime.now().isoformat(),
            'total_changes': len(changes),
            'changes': changes
        }
        
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
            
        logger.info(f"å¯¼å‡ºäº† {len(changes)} ä¸ªå˜æ›´åˆ° {file_path}")
        
    def import_changes(self, file_path: str):
        """å¯¼å…¥å˜æ›´è®°å½•"""
        
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        changes = data.get('changes', [])
        
        with self.transaction():
            for change in changes:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                cursor = self.conn.cursor()
                cursor.execute("""
                    SELECT COUNT(*) as count FROM offline_changes
                    WHERE entity_type = ? AND entity_id = ? 
                    AND operation = ? AND created_at = ?
                """, (
                    change['entity_type'],
                    change['entity_id'],
                    change['operation'],
                    change['created_at']
                ))
                
                if cursor.fetchone()['count'] == 0:
                    self.conn.execute("""
                        INSERT INTO offline_changes (
                            entity_type, entity_id, operation,
                            change_data, conflict_resolution, created_at
                        ) VALUES (?, ?, ?, ?, ?, ?)
                    """, (
                        change['entity_type'],
                        change['entity_id'],
                        change['operation'],
                        json.dumps(change.get('change_data', {})),
                        change.get('conflict_resolution', 'client_wins'),
                        change['created_at']
                    ))
                    
        logger.info(f"å¯¼å…¥äº† {len(changes)} ä¸ªå˜æ›´è®°å½•")


def main():
    """æµ‹è¯•å‡½æ•°"""
    tracker = OfflineChangeTracker()
    
    # æµ‹è¯•è·Ÿè¸ªå˜æ›´
    tracker.track_change(
        EntityType.TRANSLATION,
        "test-entry-001",
        ChangeOperation.UPDATE,
        {
            'translation_key': 'item.minecraft.diamond',
            'old_text': 'Diamond',
            'new_text': 'é’»çŸ³'
        }
    )
    
    # æ‰¹é‡è·Ÿè¸ª
    changes = [
        {
            'entity_type': 'project',
            'entity_id': 'test-project',
            'operation': 'create',
            'change_data': {'name': 'æµ‹è¯•é¡¹ç›®'}
        },
        {
            'entity_type': 'mod',
            'entity_id': 'test-mod',
            'operation': 'update',
            'change_data': {'version': '1.0.1'}
        }
    ]
    tracker.batch_track_changes(changes)
    
    # è·å–æ‘˜è¦
    summary = tracker.get_change_summary()
    print("\nğŸ“Š å˜æ›´æ‘˜è¦:")
    print(f"  å¾…åŒæ­¥: {summary['total_pending']}")
    print(f"  æŒ‰ç±»å‹:")
    for entity_type, stats in summary['by_entity'].items():
        print(f"    - {entity_type}: {stats['pending']}/{stats['total']}")
        
    # è·å–å¾…åŒæ­¥å˜æ›´
    pending = tracker.get_pending_changes(limit=5)
    print(f"\nğŸ“ æœ€è¿‘å˜æ›´ ({len(pending)} æ¡):")
    for change in pending:
        print(f"  - [{change['operation']}] {change['entity_type']}/{change['entity_id']}")
        
    # å¯¼å‡ºå˜æ›´
    tracker.export_changes("offline_changes.json")
    print("\nâœ… å˜æ›´å·²å¯¼å‡ºåˆ° offline_changes.json")


if __name__ == "__main__":
    main()