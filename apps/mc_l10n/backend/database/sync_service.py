#!/usr/bin/env python
"""
æ•°æ®åŒæ­¥æœåŠ¡
å¤„ç†æœ¬åœ°å®¢æˆ·ç«¯ä¸Trans-HubæœåŠ¡å™¨ä¹‹é—´çš„æ•°æ®åŒæ­¥
"""

import sqlite3
import json
import asyncio
import aiohttp
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from enum import Enum
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SyncDirection(Enum):
    """åŒæ­¥æ–¹å‘"""
    UPLOAD = "upload"      # æœ¬åœ°åˆ°æœåŠ¡å™¨
    DOWNLOAD = "download"  # æœåŠ¡å™¨åˆ°æœ¬åœ°
    BIDIRECTIONAL = "bidirectional"  # åŒå‘åŒæ­¥


class ConflictResolution(Enum):
    """å†²çªè§£å†³ç­–ç•¥"""
    CLIENT_WINS = "client_wins"  # å®¢æˆ·ç«¯ä¼˜å…ˆ
    SERVER_WINS = "server_wins"  # æœåŠ¡å™¨ä¼˜å…ˆ
    NEWEST_WINS = "newest_wins"  # æœ€æ–°ä¼˜å…ˆ
    MANUAL = "manual"            # æ‰‹åŠ¨è§£å†³


class DataSyncService:
    """æ•°æ®åŒæ­¥æœåŠ¡"""
    
    def __init__(self, db_path: str = "mc_l10n_local.db", server_url: str = None):
        self.db_path = Path(db_path)
        self.conn = sqlite3.connect(str(self.db_path), check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self.conn.execute("PRAGMA foreign_keys = ON")
        
        # Trans-HubæœåŠ¡å™¨é…ç½®
        self.server_url = server_url or "http://localhost:8001"
        self.api_key = None
        self.session = None
        
    async def initialize(self):
        """åˆå§‹åŒ–å¼‚æ­¥ä¼šè¯"""
        self.session = aiohttp.ClientSession(
            headers={
                'Content-Type': 'application/json',
                'X-Client-Type': 'mc-l10n'
            }
        )
        
    async def close(self):
        """å…³é—­è¿æ¥"""
        if self.session:
            await self.session.close()
        self.conn.close()
        
    def get_sync_settings(self) -> Dict:
        """è·å–åŒæ­¥è®¾ç½®"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT setting_key, setting_value 
            FROM local_settings 
            WHERE setting_key IN ('auto_sync', 'sync_interval', 'conflict_resolution')
        """)
        
        settings = {}
        for row in cursor.fetchall():
            settings[row['setting_key']] = row['setting_value']
            
        return {
            'auto_sync': settings.get('auto_sync', 'false') == 'true',
            'sync_interval': int(settings.get('sync_interval', '300')),
            'conflict_resolution': settings.get('conflict_resolution', 'client_wins')
        }
        
    async def sync_projects(self, direction: SyncDirection = SyncDirection.BIDIRECTIONAL):
        """åŒæ­¥é¡¹ç›®æ•°æ®"""
        sync_log_id = self.create_sync_log('projects', direction.value)
        
        try:
            if direction in [SyncDirection.UPLOAD, SyncDirection.BIDIRECTIONAL]:
                await self.upload_local_projects()
                
            if direction in [SyncDirection.DOWNLOAD, SyncDirection.BIDIRECTIONAL]:
                await self.download_server_projects()
                
            self.complete_sync_log(sync_log_id, success=True)
            
        except Exception as e:
            logger.error(f"é¡¹ç›®åŒæ­¥å¤±è´¥: {e}")
            self.complete_sync_log(sync_log_id, success=False, error=str(e))
            raise
            
    async def upload_local_projects(self):
        """ä¸Šä¼ æœ¬åœ°é¡¹ç›®åˆ°æœåŠ¡å™¨"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT * FROM local_projects 
            WHERE project_id != 'default-project'
        """)
        
        projects = []
        for row in cursor.fetchall():
            projects.append({
                'project_id': row['project_id'],
                'project_name': row['project_name'],
                'target_language': row['target_language'],
                'source_language': row['source_language'],
                'scan_paths': json.loads(row['scan_paths']) if row['scan_paths'] else [],
                'created_at': row['created_at'],
                'updated_at': row['updated_at']
            })
            
        if projects:
            async with self.session.post(
                f"{self.server_url}/api/sync/projects",
                json={'projects': projects}
            ) as response:
                if response.status == 200:
                    result = await response.json()
                    logger.info(f"ä¸Šä¼ äº† {len(projects)} ä¸ªé¡¹ç›®")
                else:
                    logger.error(f"é¡¹ç›®ä¸Šä¼ å¤±è´¥: {response.status}")
                    
    async def download_server_projects(self):
        """ä»æœåŠ¡å™¨ä¸‹è½½é¡¹ç›®"""
        async with self.session.get(f"{self.server_url}/api/projects") as response:
            if response.status == 200:
                data = await response.json()
                projects = data.get('projects', [])
                
                for project in projects:
                    self.conn.execute("""
                        INSERT OR REPLACE INTO local_projects (
                            project_id, project_name, target_language, source_language,
                            scan_paths, created_at, updated_at
                        ) VALUES (?, ?, ?, ?, ?, ?, ?)
                    """, (
                        project['project_id'],
                        project['project_name'],
                        project['target_language'],
                        project['source_language'],
                        json.dumps(project.get('scan_paths', [])),
                        project.get('created_at'),
                        project.get('updated_at')
                    ))
                    
                self.conn.commit()
                logger.info(f"ä¸‹è½½äº† {len(projects)} ä¸ªé¡¹ç›®")
            else:
                logger.error(f"é¡¹ç›®ä¸‹è½½å¤±è´¥: {response.status}")
                
    async def sync_mod_discoveries(self):
        """åŒæ­¥MODå‘ç°æ•°æ®"""
        sync_log_id = self.create_sync_log('mod_discoveries', 'upload')
        
        try:
            cursor = self.conn.cursor()
            cursor.execute("""
                SELECT * FROM mod_discoveries 
                WHERE is_uploaded = 0
                LIMIT 100
            """)
            
            mods = []
            mod_ids = []
            
            for row in cursor.fetchall():
                mods.append({
                    'mod_id': row['mod_id'],
                    'mod_name': row['mod_name'],
                    'display_name': row['display_name'],
                    'version': row['version'],
                    'minecraft_version': row['minecraft_version'],
                    'mod_loader': row['mod_loader'],
                    'file_path': row['file_path'],
                    'file_hash': row['file_hash'],
                    'file_size': row['file_size'],
                    'language_count': row['language_count'],
                    'total_keys': row['total_keys'],
                    'metadata': json.loads(row['metadata']) if row['metadata'] else {},
                    'discovered_at': row['discovered_at']
                })
                mod_ids.append(row['mod_id'])
                
            if mods:
                async with self.session.post(
                    f"{self.server_url}/api/sync/mods",
                    json={'mods': mods}
                ) as response:
                    if response.status == 200:
                        # æ ‡è®°ä¸ºå·²ä¸Šä¼ 
                        placeholders = ','.join(['?'] * len(mod_ids))
                        self.conn.execute(f"""
                            UPDATE mod_discoveries 
                            SET is_uploaded = 1, uploaded_at = CURRENT_TIMESTAMP
                            WHERE mod_id IN ({placeholders})
                        """, mod_ids)
                        self.conn.commit()
                        
                        self.complete_sync_log(sync_log_id, success=True, entity_count=len(mods))
                        logger.info(f"ä¸Šä¼ äº† {len(mods)} ä¸ªMODå‘ç°")
                    else:
                        raise Exception(f"ä¸Šä¼ å¤±è´¥: {response.status}")
            else:
                self.complete_sync_log(sync_log_id, success=True, entity_count=0)
                
        except Exception as e:
            logger.error(f"MODåŒæ­¥å¤±è´¥: {e}")
            self.complete_sync_log(sync_log_id, success=False, error=str(e))
            raise
            
    async def sync_translations(self, project_id: str):
        """åŒæ­¥ç¿»è¯‘æ•°æ®"""
        sync_log_id = self.create_sync_log('translations', 'bidirectional')
        
        try:
            # è·å–æœ¬åœ°å˜æ›´
            local_changes = self.get_offline_changes('translation', project_id)
            
            # ä¸Šä¼ æœ¬åœ°å˜æ›´
            if local_changes:
                await self.upload_translation_changes(local_changes)
                
            # ä¸‹è½½æœåŠ¡å™¨æ›´æ–°
            await self.download_translation_updates(project_id)
            
            self.complete_sync_log(sync_log_id, success=True)
            
        except Exception as e:
            logger.error(f"ç¿»è¯‘åŒæ­¥å¤±è´¥: {e}")
            self.complete_sync_log(sync_log_id, success=False, error=str(e))
            raise
            
    def get_offline_changes(self, entity_type: str, entity_id: Optional[str] = None) -> List[Dict]:
        """è·å–ç¦»çº¿å˜æ›´"""
        query = """
            SELECT * FROM offline_changes 
            WHERE entity_type = ? AND is_synced = 0
        """
        params = [entity_type]
        
        if entity_id:
            query += " AND entity_id = ?"
            params.append(entity_id)
            
        cursor = self.conn.cursor()
        cursor.execute(query, params)
        
        changes = []
        for row in cursor.fetchall():
            changes.append({
                'change_id': row['change_id'],
                'entity_type': row['entity_type'],
                'entity_id': row['entity_id'],
                'operation': row['operation'],
                'change_data': json.loads(row['change_data']) if row['change_data'] else {},
                'created_at': row['created_at']
            })
            
        return changes
        
    async def upload_translation_changes(self, changes: List[Dict]):
        """ä¸Šä¼ ç¿»è¯‘å˜æ›´"""
        async with self.session.post(
            f"{self.server_url}/api/sync/translation-changes",
            json={'changes': changes}
        ) as response:
            if response.status == 200:
                # æ ‡è®°å˜æ›´ä¸ºå·²åŒæ­¥
                change_ids = [c['change_id'] for c in changes]
                placeholders = ','.join(['?'] * len(change_ids))
                
                self.conn.execute(f"""
                    UPDATE offline_changes 
                    SET is_synced = 1, synced_at = CURRENT_TIMESTAMP
                    WHERE change_id IN ({placeholders})
                """, change_ids)
                self.conn.commit()
                
                logger.info(f"ä¸Šä¼ äº† {len(changes)} ä¸ªç¿»è¯‘å˜æ›´")
            else:
                raise Exception(f"å˜æ›´ä¸Šä¼ å¤±è´¥: {response.status}")
                
    async def download_translation_updates(self, project_id: str):
        """ä¸‹è½½ç¿»è¯‘æ›´æ–°"""
        # è·å–æœ€ååŒæ­¥æ—¶é—´
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT MAX(completed_at) as last_sync 
            FROM sync_log 
            WHERE sync_type = 'translations' AND completed_at IS NOT NULL
        """)
        result = cursor.fetchone()
        last_sync = result['last_sync'] if result else None
        
        params = {'project_id': project_id}
        if last_sync:
            params['since'] = last_sync
            
        async with self.session.get(
            f"{self.server_url}/api/translations/updates",
            params=params
        ) as response:
            if response.status == 200:
                data = await response.json()
                translations = data.get('translations', [])
                
                for trans in translations:
                    # æ›´æ–°æœ¬åœ°ç¿»è¯‘ç¼“å­˜
                    self.update_translation_cache(trans)
                    
                logger.info(f"ä¸‹è½½äº† {len(translations)} ä¸ªç¿»è¯‘æ›´æ–°")
            else:
                logger.error(f"ç¿»è¯‘ä¸‹è½½å¤±è´¥: {response.status}")
                
    def update_translation_cache(self, translation: Dict):
        """æ›´æ–°ç¿»è¯‘ç¼“å­˜"""
        self.conn.execute("""
            UPDATE translation_entry_cache 
            SET translated_text = ?, status = ?, cached_at = CURRENT_TIMESTAMP
            WHERE translation_key = ? AND cache_id IN (
                SELECT cache_id FROM language_file_cache 
                WHERE mod_id = ? AND language_code = ?
            )
        """, (
            translation['translated_text'],
            translation['status'],
            translation['translation_key'],
            translation['mod_id'],
            translation['language_code']
        ))
        self.conn.commit()
        
    def track_offline_change(self, entity_type: str, entity_id: str, operation: str, change_data: Dict):
        """è·Ÿè¸ªç¦»çº¿å˜æ›´"""
        settings = self.get_sync_settings()
        conflict_resolution = settings['conflict_resolution']
        
        self.conn.execute("""
            INSERT INTO offline_changes (
                entity_type, entity_id, operation, change_data, conflict_resolution
            ) VALUES (?, ?, ?, ?, ?)
        """, (
            entity_type,
            entity_id,
            operation,
            json.dumps(change_data, ensure_ascii=False),
            conflict_resolution
        ))
        self.conn.commit()
        
    def resolve_conflicts(self, local_data: Dict, server_data: Dict, strategy: ConflictResolution) -> Dict:
        """è§£å†³å†²çª"""
        if strategy == ConflictResolution.CLIENT_WINS:
            return local_data
        elif strategy == ConflictResolution.SERVER_WINS:
            return server_data
        elif strategy == ConflictResolution.NEWEST_WINS:
            local_time = datetime.fromisoformat(local_data.get('updated_at', ''))
            server_time = datetime.fromisoformat(server_data.get('updated_at', ''))
            return local_data if local_time > server_time else server_data
        else:  # MANUAL
            # éœ€è¦ç”¨æˆ·æ‰‹åŠ¨è§£å†³ï¼Œè¿™é‡Œå…ˆè¿”å›æœ¬åœ°æ•°æ®
            logger.warning(f"éœ€è¦æ‰‹åŠ¨è§£å†³å†²çª: {local_data['id']}")
            return local_data
            
    def create_sync_log(self, sync_type: str, direction: str) -> int:
        """åˆ›å»ºåŒæ­¥æ—¥å¿—"""
        cursor = self.conn.execute("""
            INSERT INTO sync_log (sync_type, sync_direction)
            VALUES (?, ?)
        """, (sync_type, direction))
        
        self.conn.commit()
        return cursor.lastrowid
        
    def complete_sync_log(self, log_id: int, success: bool, entity_count: int = 0, error: Optional[str] = None):
        """å®ŒæˆåŒæ­¥æ—¥å¿—"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT started_at FROM sync_log WHERE log_id = ?
        """, (log_id,))
        
        result = cursor.fetchone()
        if result:
            started_at = datetime.fromisoformat(result['started_at'])
            duration = (datetime.now() - started_at).total_seconds()
            
            if success:
                self.conn.execute("""
                    UPDATE sync_log 
                    SET completed_at = CURRENT_TIMESTAMP,
                        entity_count = ?,
                        success_count = ?,
                        duration_seconds = ?
                    WHERE log_id = ?
                """, (entity_count, entity_count, duration, log_id))
            else:
                self.conn.execute("""
                    UPDATE sync_log 
                    SET completed_at = CURRENT_TIMESTAMP,
                        error_count = 1,
                        sync_data = ?,
                        duration_seconds = ?
                    WHERE log_id = ?
                """, (json.dumps({'error': error}), duration, log_id))
                
            self.conn.commit()
            
    async def auto_sync_loop(self):
        """è‡ªåŠ¨åŒæ­¥å¾ªç¯"""
        settings = self.get_sync_settings()
        
        if not settings['auto_sync']:
            logger.info("è‡ªåŠ¨åŒæ­¥å·²ç¦ç”¨")
            return
            
        interval = settings['sync_interval']
        logger.info(f"å¯åŠ¨è‡ªåŠ¨åŒæ­¥ï¼Œé—´éš”: {interval}ç§’")
        
        while True:
            try:
                # åŒæ­¥å„ç§æ•°æ®
                await self.sync_projects()
                await self.sync_mod_discoveries()
                
                # è·å–æ‰€æœ‰é¡¹ç›®å¹¶åŒæ­¥ç¿»è¯‘
                cursor = self.conn.cursor()
                cursor.execute("SELECT project_id FROM local_projects")
                for row in cursor.fetchall():
                    await self.sync_translations(row['project_id'])
                    
                logger.info("è‡ªåŠ¨åŒæ­¥å®Œæˆ")
                
            except Exception as e:
                logger.error(f"è‡ªåŠ¨åŒæ­¥å¤±è´¥: {e}")
                
            # ç­‰å¾…ä¸‹ä¸€æ¬¡åŒæ­¥
            await asyncio.sleep(interval)
            
    def get_sync_status(self) -> Dict:
        """è·å–åŒæ­¥çŠ¶æ€"""
        cursor = self.conn.cursor()
        
        # æœ€è¿‘åŒæ­¥è®°å½•
        cursor.execute("""
            SELECT * FROM sync_log 
            ORDER BY started_at DESC 
            LIMIT 10
        """)
        recent_syncs = []
        for row in cursor.fetchall():
            recent_syncs.append({
                'sync_type': row['sync_type'],
                'direction': row['sync_direction'],
                'started_at': row['started_at'],
                'completed_at': row['completed_at'],
                'entity_count': row['entity_count'],
                'success_count': row['success_count'],
                'error_count': row['error_count'],
                'duration': row['duration_seconds']
            })
            
        # å¾…åŒæ­¥ç»Ÿè®¡
        cursor.execute("""
            SELECT COUNT(*) as pending FROM mod_discoveries WHERE is_uploaded = 0
        """)
        pending_mods = cursor.fetchone()['pending']
        
        cursor.execute("""
            SELECT COUNT(*) as pending FROM offline_changes WHERE is_synced = 0
        """)
        pending_changes = cursor.fetchone()['pending']
        
        return {
            'recent_syncs': recent_syncs,
            'pending': {
                'mods': pending_mods,
                'changes': pending_changes
            },
            'settings': self.get_sync_settings()
        }


async def main():
    """æµ‹è¯•å‡½æ•°"""
    service = DataSyncService()
    await service.initialize()
    
    try:
        # è·å–åŒæ­¥çŠ¶æ€
        status = service.get_sync_status()
        print("ğŸ“Š åŒæ­¥çŠ¶æ€:")
        print(f"  å¾…ä¸Šä¼ MOD: {status['pending']['mods']}")
        print(f"  å¾…åŒæ­¥å˜æ›´: {status['pending']['changes']}")
        print(f"  è‡ªåŠ¨åŒæ­¥: {status['settings']['auto_sync']}")
        
        # æ‰§è¡Œä¸€æ¬¡åŒæ­¥
        print("\nå¼€å§‹åŒæ­¥...")
        await service.sync_projects()
        await service.sync_mod_discoveries()
        
        print("âœ… åŒæ­¥å®Œæˆ")
        
    finally:
        await service.close()


if __name__ == "__main__":
    asyncio.run(main())