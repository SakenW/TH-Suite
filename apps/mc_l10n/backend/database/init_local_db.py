#!/usr/bin/env python
"""
æœ¬åœ°æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬
ç”¨äºŽåˆ›å»ºå’Œåˆå§‹åŒ–MC L10nå®¢æˆ·ç«¯çš„æœ¬åœ°SQLiteæ•°æ®åº“
"""

import sqlite3
from pathlib import Path
from datetime import datetime
import json
import logging
from typing import Optional

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class LocalDatabaseInitializer:
    """æœ¬åœ°æ•°æ®åº“åˆå§‹åŒ–å™¨"""
    
    def __init__(self, db_path: str = "mc_l10n_local.db"):
        self.db_path = Path(db_path)
        self.conn: Optional[sqlite3.Connection] = None
        
    def connect(self):
        """è¿žæŽ¥åˆ°æ•°æ®åº“"""
        self.conn = sqlite3.connect(self.db_path)
        self.conn.execute("PRAGMA foreign_keys = ON")
        self.conn.execute("PRAGMA journal_mode = WAL")
        logger.info(f"è¿žæŽ¥åˆ°æ•°æ®åº“: {self.db_path}")
        
    def close(self):
        """å…³é—­æ•°æ®åº“è¿žæŽ¥"""
        if self.conn:
            self.conn.close()
            logger.info("æ•°æ®åº“è¿žæŽ¥å·²å…³é—­")
            
    def create_tables(self):
        """åˆ›å»ºæ‰€æœ‰æœ¬åœ°æ•°æ®åº“è¡¨"""
        
        # 1. æ‰«æç¼“å­˜è¡¨
        self.conn.execute("""
        CREATE TABLE IF NOT EXISTS scan_cache (
            cache_id TEXT PRIMARY KEY DEFAULT (hex(randomblob(16))),
            scan_path TEXT NOT NULL,
            file_hash TEXT,
            scan_result TEXT,
            metadata TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            expires_at TIMESTAMP,
            is_valid BOOLEAN DEFAULT TRUE,
            UNIQUE(scan_path, file_hash)
        )
        """)
        
        # 2. MODå‘çŽ°è¡¨
        self.conn.execute("""
        CREATE TABLE IF NOT EXISTS mod_discoveries (
            discovery_id TEXT PRIMARY KEY DEFAULT (hex(randomblob(16))),
            mod_id TEXT UNIQUE,
            mod_name TEXT NOT NULL,
            display_name TEXT,
            version TEXT,
            minecraft_version TEXT,
            mod_loader TEXT,
            file_path TEXT NOT NULL,
            file_hash TEXT,
            file_size INTEGER,
            language_count INTEGER DEFAULT 0,
            total_keys INTEGER DEFAULT 0,
            metadata TEXT,
            scan_result TEXT,
            is_uploaded BOOLEAN DEFAULT FALSE,
            discovered_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            uploaded_at TIMESTAMP
        )
        """)
        
        # 3. è¯­è¨€æ–‡ä»¶ç¼“å­˜è¡¨
        self.conn.execute("""
        CREATE TABLE IF NOT EXISTS language_file_cache (
            cache_id TEXT PRIMARY KEY DEFAULT (hex(randomblob(16))),
            mod_id TEXT NOT NULL,
            language_code TEXT NOT NULL,
            file_path TEXT,
            file_format TEXT DEFAULT 'json',
            content TEXT,
            content_hash TEXT,
            entry_count INTEGER DEFAULT 0,
            cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            expires_at TIMESTAMP,
            UNIQUE(mod_id, language_code),
            FOREIGN KEY (mod_id) REFERENCES mod_discoveries(mod_id) ON DELETE CASCADE
        )
        """)
        
        # 4. ç¿»è¯‘æ¡ç›®ç¼“å­˜è¡¨
        self.conn.execute("""
        CREATE TABLE IF NOT EXISTS translation_entry_cache (
            entry_id TEXT PRIMARY KEY DEFAULT (hex(randomblob(16))),
            cache_id TEXT NOT NULL,
            translation_key TEXT NOT NULL,
            original_text TEXT,
            translated_text TEXT,
            machine_translation TEXT,
            status TEXT DEFAULT 'pending',
            cached_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            UNIQUE(cache_id, translation_key),
            FOREIGN KEY (cache_id) REFERENCES language_file_cache(cache_id) ON DELETE CASCADE
        )
        """)
        
        # 5. å·¥ä½œé˜Ÿåˆ—è¡¨
        self.conn.execute("""
        CREATE TABLE IF NOT EXISTS work_queue (
            task_id TEXT PRIMARY KEY DEFAULT (hex(randomblob(16))),
            task_type TEXT NOT NULL,
            task_data TEXT,
            priority INTEGER DEFAULT 5,
            status TEXT DEFAULT 'pending',
            retry_count INTEGER DEFAULT 0,
            max_retries INTEGER DEFAULT 3,
            error_message TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            scheduled_at TIMESTAMP,
            started_at TIMESTAMP,
            completed_at TIMESTAMP
        )
        """)
        
        # 6. ç¦»çº¿å˜æ›´è·Ÿè¸ªè¡¨
        self.conn.execute("""
        CREATE TABLE IF NOT EXISTS offline_changes (
            change_id TEXT PRIMARY KEY DEFAULT (hex(randomblob(16))),
            entity_type TEXT NOT NULL,
            entity_id TEXT NOT NULL,
            operation TEXT NOT NULL,
            change_data TEXT,
            conflict_resolution TEXT DEFAULT 'client_wins',
            is_synced BOOLEAN DEFAULT FALSE,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            synced_at TIMESTAMP
        )
        """)
        
        # 7. æœ¬åœ°è®¾ç½®è¡¨
        self.conn.execute("""
        CREATE TABLE IF NOT EXISTS local_settings (
            setting_key TEXT PRIMARY KEY,
            setting_value TEXT,
            setting_type TEXT DEFAULT 'string',
            description TEXT,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        # 8. æœ¬åœ°é¡¹ç›®è¡¨
        self.conn.execute("""
        CREATE TABLE IF NOT EXISTS local_projects (
            project_id TEXT PRIMARY KEY,
            project_name TEXT NOT NULL,
            target_language TEXT DEFAULT 'zh_cn',
            source_language TEXT DEFAULT 'en_us',
            scan_paths TEXT,
            include_patterns TEXT,
            exclude_patterns TEXT,
            auto_scan BOOLEAN DEFAULT FALSE,
            scan_interval INTEGER DEFAULT 3600,
            last_scan_at TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        )
        """)
        
        # 9. æ–‡ä»¶ç›‘æŽ§è¡¨
        self.conn.execute("""
        CREATE TABLE IF NOT EXISTS file_watch (
            watch_id TEXT PRIMARY KEY DEFAULT (hex(randomblob(16))),
            project_id TEXT,
            watch_path TEXT NOT NULL,
            watch_type TEXT DEFAULT 'directory',
            include_patterns TEXT,
            exclude_patterns TEXT,
            is_active BOOLEAN DEFAULT TRUE,
            last_check_at TIMESTAMP,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (project_id) REFERENCES local_projects(project_id) ON DELETE CASCADE
        )
        """)
        
        # 10. åŒæ­¥æ—¥å¿—è¡¨
        self.conn.execute("""
        CREATE TABLE IF NOT EXISTS sync_log (
            log_id TEXT PRIMARY KEY DEFAULT (hex(randomblob(16))),
            sync_type TEXT NOT NULL,
            sync_direction TEXT,
            entity_count INTEGER DEFAULT 0,
            success_count INTEGER DEFAULT 0,
            error_count INTEGER DEFAULT 0,
            conflict_count INTEGER DEFAULT 0,
            sync_data TEXT,
            started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            completed_at TIMESTAMP,
            duration_seconds REAL
        )
        """)
        
        logger.info("æ‰€æœ‰è¡¨åˆ›å»ºå®Œæˆ")
        
    def create_indexes(self):
        """åˆ›å»ºç´¢å¼•ä»¥ä¼˜åŒ–æŸ¥è¯¢æ€§èƒ½"""
        
        indexes = [
            # æ‰«æç¼“å­˜ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_scan_cache_path ON scan_cache(scan_path)",
            "CREATE INDEX IF NOT EXISTS idx_scan_cache_hash ON scan_cache(file_hash)",
            "CREATE INDEX IF NOT EXISTS idx_scan_cache_expires ON scan_cache(expires_at)",
            
            # MODå‘çŽ°ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_discoveries_name ON mod_discoveries(mod_name)",
            "CREATE INDEX IF NOT EXISTS idx_discoveries_uploaded ON mod_discoveries(is_uploaded)",
            "CREATE INDEX IF NOT EXISTS idx_discoveries_path ON mod_discoveries(file_path)",
            
            # è¯­è¨€æ–‡ä»¶ç¼“å­˜ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_lang_cache_mod ON language_file_cache(mod_id)",
            "CREATE INDEX IF NOT EXISTS idx_lang_cache_lang ON language_file_cache(language_code)",
            
            # ç¿»è¯‘æ¡ç›®ç¼“å­˜ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_entry_cache_id ON translation_entry_cache(cache_id)",
            "CREATE INDEX IF NOT EXISTS idx_entry_cache_key ON translation_entry_cache(translation_key)",
            "CREATE INDEX IF NOT EXISTS idx_entry_cache_status ON translation_entry_cache(status)",
            
            # å·¥ä½œé˜Ÿåˆ—ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_queue_type ON work_queue(task_type)",
            "CREATE INDEX IF NOT EXISTS idx_queue_status ON work_queue(status)",
            "CREATE INDEX IF NOT EXISTS idx_queue_priority ON work_queue(priority DESC)",
            "CREATE INDEX IF NOT EXISTS idx_queue_scheduled ON work_queue(scheduled_at)",
            
            # ç¦»çº¿å˜æ›´ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_changes_entity ON offline_changes(entity_type, entity_id)",
            "CREATE INDEX IF NOT EXISTS idx_changes_synced ON offline_changes(is_synced)",
            
            # æ–‡ä»¶ç›‘æŽ§ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_watch_project ON file_watch(project_id)",
            "CREATE INDEX IF NOT EXISTS idx_watch_active ON file_watch(is_active)",
            
            # åŒæ­¥æ—¥å¿—ç´¢å¼•
            "CREATE INDEX IF NOT EXISTS idx_sync_type ON sync_log(sync_type)",
            "CREATE INDEX IF NOT EXISTS idx_sync_started ON sync_log(started_at DESC)"
        ]
        
        for index_sql in indexes:
            self.conn.execute(index_sql)
            
        logger.info(f"åˆ›å»ºäº† {len(indexes)} ä¸ªç´¢å¼•")
        
    def create_triggers(self):
        """åˆ›å»ºè§¦å‘å™¨"""
        
        # æ›´æ–°æ—¶é—´æˆ³è§¦å‘å™¨
        triggers = [
            """
            CREATE TRIGGER IF NOT EXISTS update_local_settings_timestamp
            AFTER UPDATE ON local_settings
            BEGIN
                UPDATE local_settings SET updated_at = CURRENT_TIMESTAMP
                WHERE setting_key = NEW.setting_key;
            END
            """,
            """
            CREATE TRIGGER IF NOT EXISTS update_local_projects_timestamp
            AFTER UPDATE ON local_projects
            BEGIN
                UPDATE local_projects SET updated_at = CURRENT_TIMESTAMP
                WHERE project_id = NEW.project_id;
            END
            """
        ]
        
        for trigger_sql in triggers:
            self.conn.execute(trigger_sql)
            
        logger.info("è§¦å‘å™¨åˆ›å»ºå®Œæˆ")
        
    def insert_default_settings(self):
        """æ’å…¥é»˜è®¤è®¾ç½®"""
        
        default_settings = [
            ('cache_ttl', '86400', 'integer', 'ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰'),
            ('auto_sync', 'false', 'boolean', 'è‡ªåŠ¨åŒæ­¥å¼€å…³'),
            ('sync_interval', '300', 'integer', 'åŒæ­¥é—´éš”ï¼ˆç§’ï¼‰'),
            ('max_cache_size', '1073741824', 'integer', 'æœ€å¤§ç¼“å­˜å¤§å°ï¼ˆå­—èŠ‚ï¼‰'),
            ('scan_threads', '4', 'integer', 'æ‰«æçº¿ç¨‹æ•°'),
            ('api_timeout', '30', 'integer', 'APIè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰'),
            ('language_priority', 'zh_cn,en_us', 'string', 'è¯­è¨€ä¼˜å…ˆçº§'),
            ('enable_file_watch', 'true', 'boolean', 'å¯ç”¨æ–‡ä»¶ç›‘æŽ§'),
            ('conflict_resolution', 'client_wins', 'string', 'å†²çªè§£å†³ç­–ç•¥'),
            ('log_level', 'INFO', 'string', 'æ—¥å¿—çº§åˆ«')
        ]
        
        for key, value, type_, desc in default_settings:
            self.conn.execute("""
                INSERT OR IGNORE INTO local_settings (setting_key, setting_value, setting_type, description)
                VALUES (?, ?, ?, ?)
            """, (key, value, type_, desc))
            
        logger.info(f"æ’å…¥äº† {len(default_settings)} ä¸ªé»˜è®¤è®¾ç½®")
        
    def create_default_project(self):
        """åˆ›å»ºé»˜è®¤é¡¹ç›®"""
        
        self.conn.execute("""
            INSERT OR IGNORE INTO local_projects (
                project_id, project_name, target_language, source_language,
                scan_paths, auto_scan
            ) VALUES (
                'default-project',
                'é»˜è®¤é¡¹ç›®',
                'zh_cn',
                'en_us',
                '[]',
                0
            )
        """)
        
        logger.info("åˆ›å»ºé»˜è®¤é¡¹ç›®å®Œæˆ")
        
    def create_views(self):
        """åˆ›å»ºè§†å›¾"""
        
        views = [
            """
            CREATE VIEW IF NOT EXISTS v_cache_statistics AS
            SELECT 
                COUNT(*) as total_cache_entries,
                SUM(CASE WHEN is_valid = 1 AND (expires_at IS NULL OR expires_at > CURRENT_TIMESTAMP) THEN 1 ELSE 0 END) as valid_entries,
                SUM(CASE WHEN expires_at <= CURRENT_TIMESTAMP THEN 1 ELSE 0 END) as expired_entries,
                COUNT(DISTINCT scan_path) as unique_paths
            FROM scan_cache
            """,
            """
            CREATE VIEW IF NOT EXISTS v_discovery_summary AS
            SELECT 
                COUNT(*) as total_mods,
                SUM(CASE WHEN is_uploaded = 1 THEN 1 ELSE 0 END) as uploaded_mods,
                SUM(language_count) as total_languages,
                SUM(total_keys) as total_translation_keys,
                MIN(discovered_at) as first_discovery,
                MAX(discovered_at) as latest_discovery
            FROM mod_discoveries
            """,
            """
            CREATE VIEW IF NOT EXISTS v_queue_status AS
            SELECT 
                task_type,
                COUNT(*) as total_tasks,
                SUM(CASE WHEN status = 'pending' THEN 1 ELSE 0 END) as pending,
                SUM(CASE WHEN status = 'processing' THEN 1 ELSE 0 END) as processing,
                SUM(CASE WHEN status = 'completed' THEN 1 ELSE 0 END) as completed,
                SUM(CASE WHEN status = 'failed' THEN 1 ELSE 0 END) as failed,
                AVG(CASE WHEN completed_at IS NOT NULL THEN 
                    (julianday(completed_at) - julianday(started_at)) * 86400 
                ELSE NULL END) as avg_duration_seconds
            FROM work_queue
            GROUP BY task_type
            """,
            """
            CREATE VIEW IF NOT EXISTS v_sync_history AS
            SELECT 
                sync_type,
                COUNT(*) as total_syncs,
                AVG(duration_seconds) as avg_duration,
                SUM(entity_count) as total_entities,
                SUM(success_count) as total_success,
                SUM(error_count) as total_errors,
                SUM(conflict_count) as total_conflicts,
                MAX(completed_at) as last_sync
            FROM sync_log
            WHERE completed_at IS NOT NULL
            GROUP BY sync_type
            """
        ]
        
        for view_sql in views:
            self.conn.execute(view_sql)
            
        logger.info(f"åˆ›å»ºäº† {len(views)} ä¸ªè§†å›¾")
        
    def initialize(self, reset: bool = False):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        
        try:
            if reset and self.db_path.exists():
                self.db_path.unlink()
                logger.info(f"å·²åˆ é™¤æ—§æ•°æ®åº“: {self.db_path}")
                
            self.connect()
            
            # åˆ›å»ºè¡¨ç»“æž„
            self.create_tables()
            
            # åˆ›å»ºç´¢å¼•
            self.create_indexes()
            
            # åˆ›å»ºè§¦å‘å™¨
            self.create_triggers()
            
            # åˆ›å»ºè§†å›¾
            self.create_views()
            
            # æ’å…¥é»˜è®¤æ•°æ®
            self.insert_default_settings()
            self.create_default_project()
            
            # æäº¤äº‹åŠ¡
            self.conn.commit()
            
            logger.info("âœ… æœ¬åœ°æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")
            
            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            self.show_statistics()
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {e}")
            if self.conn:
                self.conn.rollback()
            raise
        finally:
            self.close()
            
    def show_statistics(self):
        """æ˜¾ç¤ºæ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        
        cursor = self.conn.cursor()
        
        # èŽ·å–è¡¨ä¿¡æ¯
        cursor.execute("""
            SELECT name FROM sqlite_master 
            WHERE type='table' AND name NOT LIKE 'sqlite_%'
            ORDER BY name
        """)
        tables = cursor.fetchall()
        
        logger.info("\nðŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
        logger.info(f"  è¡¨æ•°é‡: {len(tables)}")
        
        for table_name, in tables:
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            count = cursor.fetchone()[0]
            logger.info(f"  - {table_name}: {count} è¡Œ")
            
        # èŽ·å–ç´¢å¼•æ•°é‡
        cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='index'")
        index_count = cursor.fetchone()[0]
        logger.info(f"  ç´¢å¼•æ•°é‡: {index_count}")
        
        # èŽ·å–è§†å›¾æ•°é‡
        cursor.execute("SELECT COUNT(*) FROM sqlite_master WHERE type='view'")
        view_count = cursor.fetchone()[0]
        logger.info(f"  è§†å›¾æ•°é‡: {view_count}")
        
        # æ•°æ®åº“æ–‡ä»¶å¤§å°
        if self.db_path.exists():
            size_mb = self.db_path.stat().st_size / 1024 / 1024
            logger.info(f"  æ•°æ®åº“å¤§å°: {size_mb:.2f} MB")


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="æœ¬åœ°æ•°æ®åº“åˆå§‹åŒ–å·¥å…·")
    parser.add_argument(
        "--db",
        default="mc_l10n_local.db",
        help="æ•°æ®åº“æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--reset",
        action="store_true",
        help="é‡ç½®æ•°æ®åº“ï¼ˆåˆ é™¤æ—§æ•°æ®ï¼‰"
    )
    
    args = parser.parse_args()
    
    initializer = LocalDatabaseInitializer(args.db)
    initializer.initialize(reset=args.reset)


if __name__ == "__main__":
    main()