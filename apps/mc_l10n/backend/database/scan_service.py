#!/usr/bin/env python
"""
æ‰«ææœåŠ¡ä¸æ•°æ®åº“é›†æˆ
å¤„ç†MODæ‰«æã€ç¼“å­˜ç®¡ç†å’Œæ•°æ®æŒä¹…åŒ–
"""

import sqlite3
import hashlib
import json
import zipfile
from pathlib import Path
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import logging
import toml
from concurrent.futures import ThreadPoolExecutor, as_completed

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ScanDatabaseService:
    """æ‰«ææ•°æ®åº“æœåŠ¡"""
    
    def __init__(self, db_path: str = "mc_l10n_local.db"):
        self.db_path = Path(db_path)
        self.conn = sqlite3.connect(str(self.db_path), check_same_thread=False)
        self.conn.row_factory = sqlite3.Row
        self.conn.execute("PRAGMA foreign_keys = ON")
        
    def get_cache_ttl(self) -> int:
        """è·å–ç¼“å­˜TTLè®¾ç½®"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT setting_value FROM local_settings 
            WHERE setting_key = 'cache_ttl'
        """)
        result = cursor.fetchone()
        return int(result['setting_value']) if result else 86400
        
    def calculate_file_hash(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
        
    def check_scan_cache(self, scan_path: str, file_hash: Optional[str] = None) -> Optional[Dict]:
        """æ£€æŸ¥æ‰«æç¼“å­˜"""
        cursor = self.conn.cursor()
        
        query = """
            SELECT * FROM scan_cache 
            WHERE scan_path = ? 
            AND is_valid = 1
            AND (expires_at IS NULL OR expires_at > CURRENT_TIMESTAMP)
        """
        params = [scan_path]
        
        if file_hash:
            query += " AND file_hash = ?"
            params.append(file_hash)
            
        cursor.execute(query, params)
        result = cursor.fetchone()
        
        if result:
            return {
                'cache_id': result['cache_id'],
                'scan_result': json.loads(result['scan_result']) if result['scan_result'] else None,
                'metadata': json.loads(result['metadata']) if result['metadata'] else None,
                'created_at': result['created_at']
            }
        return None
        
    def save_scan_cache(self, scan_path: str, file_hash: str, scan_result: Dict, metadata: Optional[Dict] = None):
        """ä¿å­˜æ‰«æç¼“å­˜"""
        ttl = self.get_cache_ttl()
        expires_at = datetime.now() + timedelta(seconds=ttl)
        
        self.conn.execute("""
            INSERT OR REPLACE INTO scan_cache (scan_path, file_hash, scan_result, metadata, expires_at)
            VALUES (?, ?, ?, ?, ?)
        """, (
            scan_path,
            file_hash,
            json.dumps(scan_result, ensure_ascii=False),
            json.dumps(metadata, ensure_ascii=False) if metadata else None,
            expires_at
        ))
        self.conn.commit()
        
    def discover_mod(self, jar_path: Path) -> Optional[Dict]:
        """å‘ç°å¹¶åˆ†æMOD"""
        try:
            # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
            file_hash = self.calculate_file_hash(jar_path)
            
            # æ£€æŸ¥ç¼“å­˜
            cached = self.check_scan_cache(str(jar_path), file_hash)
            if cached:
                logger.info(f"ä½¿ç”¨ç¼“å­˜: {jar_path.name}")
                return cached['scan_result']
                
            # æå–MODä¿¡æ¯
            mod_info = self.extract_mod_info(jar_path)
            if not mod_info:
                return None
                
            # ä¿å­˜åˆ°å‘ç°è¡¨
            self.save_mod_discovery(mod_info, jar_path, file_hash)
            
            # ä¿å­˜ç¼“å­˜
            self.save_scan_cache(str(jar_path), file_hash, mod_info)
            
            return mod_info
            
        except Exception as e:
            logger.error(f"æ‰«æMODå¤±è´¥ {jar_path}: {e}")
            return None
            
    def extract_mod_info(self, jar_path: Path) -> Optional[Dict]:
        """ä»JARæ–‡ä»¶æå–MODä¿¡æ¯"""
        try:
            with zipfile.ZipFile(jar_path, 'r') as jar:
                # æ£€æŸ¥Forge MOD
                if 'META-INF/mods.toml' in jar.namelist():
                    return self.parse_forge_mod(jar, jar_path)
                # æ£€æŸ¥Fabric MOD
                elif 'fabric.mod.json' in jar.namelist():
                    return self.parse_fabric_mod(jar, jar_path)
                # æ£€æŸ¥Quilt MOD
                elif 'quilt.mod.json' in jar.namelist():
                    return self.parse_quilt_mod(jar, jar_path)
                    
        except Exception as e:
            logger.error(f"æå–MODä¿¡æ¯å¤±è´¥: {e}")
            
        return None
        
    def parse_forge_mod(self, jar: zipfile.ZipFile, jar_path: Path) -> Dict:
        """è§£æForge MOD"""
        try:
            toml_content = jar.read('META-INF/mods.toml').decode('utf-8')
            # æ¸…ç†TOMLå†…å®¹
            clean_content = self.clean_toml_content(toml_content)
            data = toml.loads(clean_content)
            
            mod = data.get('mods', [{}])[0]
            
            # ç»Ÿè®¡è¯­è¨€æ–‡ä»¶
            lang_stats = self.count_language_files(jar)
            
            return {
                'mod_id': mod.get('modId', jar_path.stem),
                'mod_name': mod.get('modId', jar_path.stem),
                'display_name': mod.get('displayName', ''),
                'version': mod.get('version', 'unknown'),
                'minecraft_version': self.extract_mc_version(data),
                'mod_loader': 'forge',
                'language_count': lang_stats['count'],
                'total_keys': lang_stats['total_keys'],
                'metadata': data
            }
        except Exception as e:
            logger.error(f"è§£æForge MODå¤±è´¥: {e}")
            return None
            
    def parse_fabric_mod(self, jar: zipfile.ZipFile, jar_path: Path) -> Dict:
        """è§£æFabric MOD"""
        try:
            json_content = jar.read('fabric.mod.json').decode('utf-8')
            data = json.loads(json_content)
            
            lang_stats = self.count_language_files(jar)
            
            return {
                'mod_id': data.get('id', jar_path.stem),
                'mod_name': data.get('id', jar_path.stem),
                'display_name': data.get('name', ''),
                'version': data.get('version', 'unknown'),
                'minecraft_version': data.get('depends', {}).get('minecraft', '*'),
                'mod_loader': 'fabric',
                'language_count': lang_stats['count'],
                'total_keys': lang_stats['total_keys'],
                'metadata': data
            }
        except Exception as e:
            logger.error(f"è§£æFabric MODå¤±è´¥: {e}")
            return None
            
    def parse_quilt_mod(self, jar: zipfile.ZipFile, jar_path: Path) -> Dict:
        """è§£æQuilt MOD"""
        try:
            json_content = jar.read('quilt.mod.json').decode('utf-8')
            data = json.loads(json_content)
            
            quilt_loader = data.get('quilt_loader', {})
            lang_stats = self.count_language_files(jar)
            
            return {
                'mod_id': quilt_loader.get('id', jar_path.stem),
                'mod_name': quilt_loader.get('id', jar_path.stem),
                'display_name': quilt_loader.get('metadata', {}).get('name', ''),
                'version': quilt_loader.get('version', 'unknown'),
                'minecraft_version': quilt_loader.get('depends', [{}])[0].get('id', '*') if quilt_loader.get('depends') else '*',
                'mod_loader': 'quilt',
                'language_count': lang_stats['count'],
                'total_keys': lang_stats['total_keys'],
                'metadata': data
            }
        except Exception as e:
            logger.error(f"è§£æQuilt MODå¤±è´¥: {e}")
            return None
            
    def clean_toml_content(self, content: str) -> str:
        """æ¸…ç†TOMLå†…å®¹"""
        lines = []
        for line in content.split('\n'):
            # ç§»é™¤æ³¨é‡Š
            if '#' in line:
                line = line[:line.index('#')]
            # æ›¿æ¢å˜é‡
            line = line.replace('${file.jarVersion}', '"unknown"')
            lines.append(line)
        return '\n'.join(lines)
        
    def extract_mc_version(self, toml_data: Dict) -> str:
        """æå–Minecraftç‰ˆæœ¬"""
        dependencies = toml_data.get('dependencies', {})
        if isinstance(dependencies, dict):
            for mod_id, deps in dependencies.items():
                if isinstance(deps, list):
                    for dep in deps:
                        if isinstance(dep, dict) and dep.get('modId') == 'minecraft':
                            return dep.get('versionRange', '*')
        return '*'
        
    def count_language_files(self, jar: zipfile.ZipFile) -> Dict:
        """ç»Ÿè®¡è¯­è¨€æ–‡ä»¶"""
        lang_files = []
        total_keys = 0
        
        for file_name in jar.namelist():
            if '/lang/' in file_name and (file_name.endswith('.json') or file_name.endswith('.lang')):
                lang_files.append(file_name)
                try:
                    content = jar.read(file_name).decode('utf-8')
                    if file_name.endswith('.json'):
                        data = json.loads(content)
                        total_keys += len(data)
                    else:  # .lang file
                        # ç®€å•ç»Ÿè®¡è¡Œæ•°ï¼ˆä¸åŒ…æ‹¬ç©ºè¡Œå’Œæ³¨é‡Šï¼‰
                        lines = content.split('\n')
                        total_keys += sum(1 for line in lines if line.strip() and not line.strip().startswith('#'))
                except:
                    pass
                    
        return {
            'count': len(lang_files),
            'total_keys': total_keys,
            'files': lang_files
        }
        
    def save_mod_discovery(self, mod_info: Dict, jar_path: Path, file_hash: str):
        """ä¿å­˜MODå‘ç°è®°å½•"""
        self.conn.execute("""
            INSERT OR REPLACE INTO mod_discoveries (
                mod_id, mod_name, display_name, version, minecraft_version,
                mod_loader, file_path, file_hash, file_size,
                language_count, total_keys, metadata, scan_result
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            mod_info['mod_id'],
            mod_info['mod_name'],
            mod_info.get('display_name', ''),
            mod_info.get('version', 'unknown'),
            mod_info.get('minecraft_version', '*'),
            mod_info.get('mod_loader', 'unknown'),
            str(jar_path),
            file_hash,
            jar_path.stat().st_size,
            mod_info.get('language_count', 0),
            mod_info.get('total_keys', 0),
            json.dumps(mod_info.get('metadata', {}), ensure_ascii=False),
            json.dumps(mod_info, ensure_ascii=False)
        ))
        self.conn.commit()
        
    def extract_language_files(self, mod_id: str) -> List[Dict]:
        """æå–MODçš„è¯­è¨€æ–‡ä»¶"""
        cursor = self.conn.cursor()
        cursor.execute("""
            SELECT file_path, file_hash FROM mod_discoveries
            WHERE mod_id = ?
        """, (mod_id,))
        
        result = cursor.fetchone()
        if not result:
            return []
            
        jar_path = Path(result['file_path'])
        if not jar_path.exists():
            return []
            
        language_files = []
        
        try:
            with zipfile.ZipFile(jar_path, 'r') as jar:
                for file_name in jar.namelist():
                    if '/lang/' in file_name and (file_name.endswith('.json') or file_name.endswith('.lang')):
                        # æå–è¯­è¨€ä»£ç 
                        lang_code = Path(file_name).stem
                        content = jar.read(file_name).decode('utf-8')
                        
                        # ä¿å­˜è¯­è¨€æ–‡ä»¶ç¼“å­˜
                        self.save_language_cache(mod_id, lang_code, file_name, content)
                        
                        language_files.append({
                            'mod_id': mod_id,
                            'language_code': lang_code,
                            'file_path': file_name,
                            'content': content
                        })
                        
        except Exception as e:
            logger.error(f"æå–è¯­è¨€æ–‡ä»¶å¤±è´¥: {e}")
            
        return language_files
        
    def save_language_cache(self, mod_id: str, lang_code: str, file_path: str, content: str):
        """ä¿å­˜è¯­è¨€æ–‡ä»¶ç¼“å­˜"""
        content_hash = hashlib.md5(content.encode()).hexdigest()
        
        # ç»Ÿè®¡æ¡ç›®æ•°
        entry_count = 0
        if file_path.endswith('.json'):
            try:
                data = json.loads(content)
                entry_count = len(data)
            except:
                pass
        else:  # .lang file
            lines = content.split('\n')
            entry_count = sum(1 for line in lines if line.strip() and not line.strip().startswith('#'))
            
        ttl = self.get_cache_ttl()
        expires_at = datetime.now() + timedelta(seconds=ttl)
        
        cursor = self.conn.execute("""
            INSERT OR REPLACE INTO language_file_cache (
                mod_id, language_code, file_path, file_format, 
                content, content_hash, entry_count, expires_at
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            mod_id,
            lang_code,
            file_path,
            'json' if file_path.endswith('.json') else 'properties',
            content,
            content_hash,
            entry_count,
            expires_at
        ))
        
        cache_id = cursor.lastrowid
        
        # è§£æå¹¶ä¿å­˜ç¿»è¯‘æ¡ç›®
        if file_path.endswith('.json'):
            self.save_translation_entries_json(cache_id, content)
        else:
            self.save_translation_entries_lang(cache_id, content)
            
        self.conn.commit()
        
    def save_translation_entries_json(self, cache_id: int, content: str):
        """ä¿å­˜JSONæ ¼å¼çš„ç¿»è¯‘æ¡ç›®"""
        try:
            data = json.loads(content)
            for key, value in data.items():
                self.conn.execute("""
                    INSERT OR REPLACE INTO translation_entry_cache (
                        cache_id, translation_key, original_text, status
                    ) VALUES (?, ?, ?, ?)
                """, (cache_id, key, value, 'pending'))
        except Exception as e:
            logger.error(f"ä¿å­˜JSONç¿»è¯‘æ¡ç›®å¤±è´¥: {e}")
            
    def save_translation_entries_lang(self, cache_id: int, content: str):
        """ä¿å­˜.langæ ¼å¼çš„ç¿»è¯‘æ¡ç›®"""
        for line in content.split('\n'):
            line = line.strip()
            if line and not line.startswith('#') and '=' in line:
                key, value = line.split('=', 1)
                self.conn.execute("""
                    INSERT OR REPLACE INTO translation_entry_cache (
                        cache_id, translation_key, original_text, status
                    ) VALUES (?, ?, ?, ?)
                """, (cache_id, key.strip(), value.strip(), 'pending'))
                
    def scan_directory(self, directory: Path, progress_callback=None) -> Dict:
        """æ‰«æç›®å½•"""
        jar_files = list(directory.rglob("*.jar"))
        total = len(jar_files)
        
        results = {
            'total_files': total,
            'successful': 0,
            'failed': 0,
            'cached': 0,
            'mods': []
        }
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            futures = {executor.submit(self.discover_mod, jar_path): jar_path 
                      for jar_path in jar_files}
            
            for i, future in enumerate(as_completed(futures), 1):
                jar_path = futures[future]
                try:
                    mod_info = future.result()
                    if mod_info:
                        results['successful'] += 1
                        results['mods'].append(mod_info)
                    else:
                        results['failed'] += 1
                        
                    if progress_callback:
                        progress_callback(i, total, jar_path.name)
                        
                except Exception as e:
                    logger.error(f"å¤„ç†å¤±è´¥ {jar_path}: {e}")
                    results['failed'] += 1
                    
        return results
        
    def add_to_work_queue(self, task_type: str, task_data: Dict, priority: int = 5):
        """æ·»åŠ ä»»åŠ¡åˆ°å·¥ä½œé˜Ÿåˆ—"""
        self.conn.execute("""
            INSERT INTO work_queue (task_type, task_data, priority)
            VALUES (?, ?, ?)
        """, (
            task_type,
            json.dumps(task_data, ensure_ascii=False),
            priority
        ))
        self.conn.commit()
        
    def get_pending_tasks(self, task_type: Optional[str] = None, limit: int = 10) -> List[Dict]:
        """è·å–å¾…å¤„ç†ä»»åŠ¡"""
        query = """
            SELECT * FROM work_queue 
            WHERE status = 'pending'
        """
        params = []
        
        if task_type:
            query += " AND task_type = ?"
            params.append(task_type)
            
        query += " ORDER BY priority DESC, created_at ASC LIMIT ?"
        params.append(limit)
        
        cursor = self.conn.cursor()
        cursor.execute(query, params)
        
        tasks = []
        for row in cursor.fetchall():
            tasks.append({
                'task_id': row['task_id'],
                'task_type': row['task_type'],
                'task_data': json.loads(row['task_data']) if row['task_data'] else {},
                'priority': row['priority'],
                'retry_count': row['retry_count']
            })
            
        return tasks
        
    def update_task_status(self, task_id: str, status: str, error_message: Optional[str] = None):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        if status == 'processing':
            self.conn.execute("""
                UPDATE work_queue 
                SET status = ?, started_at = CURRENT_TIMESTAMP
                WHERE task_id = ?
            """, (status, task_id))
        elif status == 'completed':
            self.conn.execute("""
                UPDATE work_queue 
                SET status = ?, completed_at = CURRENT_TIMESTAMP
                WHERE task_id = ?
            """, (status, task_id))
        elif status == 'failed':
            self.conn.execute("""
                UPDATE work_queue 
                SET status = ?, error_message = ?, retry_count = retry_count + 1
                WHERE task_id = ?
            """, (status, error_message, task_id))
            
        self.conn.commit()
        
    def get_statistics(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        cursor = self.conn.cursor()
        
        # MODç»Ÿè®¡
        cursor.execute("SELECT COUNT(*) as total FROM mod_discoveries")
        mod_count = cursor.fetchone()['total']
        
        cursor.execute("SELECT COUNT(*) as uploaded FROM mod_discoveries WHERE is_uploaded = 1")
        uploaded_count = cursor.fetchone()['uploaded']
        
        # è¯­è¨€æ–‡ä»¶ç»Ÿè®¡
        cursor.execute("SELECT COUNT(*) as total FROM language_file_cache")
        lang_count = cursor.fetchone()['total']
        
        # ç¿»è¯‘æ¡ç›®ç»Ÿè®¡
        cursor.execute("SELECT COUNT(*) as total FROM translation_entry_cache")
        entry_count = cursor.fetchone()['total']
        
        cursor.execute("""
            SELECT status, COUNT(*) as count 
            FROM translation_entry_cache 
            GROUP BY status
        """)
        status_counts = {row['status']: row['count'] for row in cursor.fetchall()}
        
        # å·¥ä½œé˜Ÿåˆ—ç»Ÿè®¡
        cursor.execute("""
            SELECT status, COUNT(*) as count 
            FROM work_queue 
            GROUP BY status
        """)
        queue_counts = {row['status']: row['count'] for row in cursor.fetchall()}
        
        return {
            'mods': {
                'total': mod_count,
                'uploaded': uploaded_count,
                'pending': mod_count - uploaded_count
            },
            'language_files': lang_count,
            'translation_entries': {
                'total': entry_count,
                'by_status': status_counts
            },
            'work_queue': queue_counts
        }
        
    def cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        self.conn.execute("""
            UPDATE scan_cache 
            SET is_valid = 0 
            WHERE expires_at <= CURRENT_TIMESTAMP
        """)
        
        self.conn.execute("""
            DELETE FROM language_file_cache 
            WHERE expires_at <= CURRENT_TIMESTAMP
        """)
        
        self.conn.commit()
        
        cursor = self.conn.cursor()
        cursor.execute("SELECT changes() as deleted")
        deleted_count = cursor.fetchone()['deleted']
        
        logger.info(f"æ¸…ç†äº† {deleted_count} æ¡è¿‡æœŸç¼“å­˜")
        
    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        self.conn.close()


def main():
    """æµ‹è¯•å‡½æ•°"""
    import sys
    
    # åˆå§‹åŒ–æœåŠ¡
    service = ScanDatabaseService()
    
    if len(sys.argv) > 1:
        scan_path = Path(sys.argv[1])
        if scan_path.exists():
            if scan_path.is_file() and scan_path.suffix == '.jar':
                # æ‰«æå•ä¸ªJARæ–‡ä»¶
                result = service.discover_mod(scan_path)
                if result:
                    print(f"âœ… æˆåŠŸæ‰«æ: {result['mod_name']}")
                    print(f"  - MOD ID: {result['mod_id']}")
                    print(f"  - ç‰ˆæœ¬: {result['version']}")
                    print(f"  - è¯­è¨€æ–‡ä»¶: {result['language_count']}")
                    print(f"  - ç¿»è¯‘é”®: {result['total_keys']}")
            elif scan_path.is_dir():
                # æ‰«æç›®å½•
                def progress(current, total, name):
                    print(f"[{current}/{total}] æ‰«æ: {name}")
                    
                results = service.scan_directory(scan_path, progress)
                print(f"\næ‰«æå®Œæˆ:")
                print(f"  - æ€»æ–‡ä»¶: {results['total_files']}")
                print(f"  - æˆåŠŸ: {results['successful']}")
                print(f"  - å¤±è´¥: {results['failed']}")
                
    # æ˜¾ç¤ºç»Ÿè®¡
    stats = service.get_statistics()
    print(f"\nğŸ“Š æ•°æ®åº“ç»Ÿè®¡:")
    print(f"  - MODæ€»æ•°: {stats['mods']['total']}")
    print(f"  - å·²ä¸Šä¼ : {stats['mods']['uploaded']}")
    print(f"  - è¯­è¨€æ–‡ä»¶: {stats['language_files']}")
    print(f"  - ç¿»è¯‘æ¡ç›®: {stats['translation_entries']['total']}")
    
    service.close()


if __name__ == "__main__":
    main()