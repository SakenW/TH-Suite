#!/usr/bin/env python
"""
æµ‹è¯•SLOæ€§èƒ½ç›‘æ§ç³»ç»Ÿ
éªŒè¯SLOç›‘æ§ã€è¯„ä¼°ã€å‘Šè­¦ç­‰åŠŸèƒ½
"""

import sys
import os
import time
import asyncio
import random
from typing import List, Dict, Any

sys.path.append('.')

import structlog

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.dev.ConsoleRenderer()
    ]
)

logger = structlog.get_logger(__name__)

async def test_slo_data_collection():
    """æµ‹è¯•SLOæ•°æ®æ”¶é›†"""
    print("=== æµ‹è¯•SLOæ•°æ®æ”¶é›† ===")
    
    try:
        from services.slo_monitoring import SLODataCollector
        
        collector = SLODataCollector(max_data_points=1000)
        print("âœ“ SLOæ•°æ®æ”¶é›†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # è®°å½•ä¸€äº›æµ‹è¯•æŒ‡æ ‡
        test_metrics = [
            ("api_latency", 45.5, {"endpoint": "/translations", "method": "GET"}),
            ("api_latency", 67.8, {"endpoint": "/translations", "method": "POST"}),
            ("api_latency", 123.4, {"endpoint": "/packs", "method": "GET"}),
            ("error_rate", 0.5, {"service": "translation_api"}),
            ("error_rate", 1.2, {"service": "sync_service"}),
            ("availability", 99.95, {"service": "translation_api"}),
            ("throughput", 1500.0, {"service": "translation_api"}),
        ]
        
        start_time = time.time()
        
        for slo_name, value, labels in test_metrics:
            await collector.record_metric(
                slo_name=slo_name,
                value=value,
                labels=labels,
                metadata={"test": "true"}
            )
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿæ—¶é—´é—´éš”
        
        print(f"âœ“ è®°å½•äº† {len(test_metrics)} ä¸ªæŒ‡æ ‡æ•°æ®")
        
        # æŸ¥è¯¢æ•°æ®
        api_latency_metrics = await collector.get_metrics("api_latency")
        if len(api_latency_metrics) == 3:
            print("âœ“ APIå»¶è¿Ÿæ•°æ®æŸ¥è¯¢æˆåŠŸ")
        else:
            print(f"âœ— APIå»¶è¿Ÿæ•°æ®æŸ¥è¯¢å¤±è´¥: æœŸæœ›3ä¸ªï¼Œå®é™…{len(api_latency_metrics)}ä¸ª")
            return False
        
        # æ—¶é—´èŒƒå›´æŸ¥è¯¢
        recent_metrics = await collector.get_metrics(
            "api_latency",
            start_time=start_time,
            limit=2
        )
        
        if len(recent_metrics) == 2:
            print("âœ“ æ—¶é—´èŒƒå›´å’Œé™åˆ¶æŸ¥è¯¢æˆåŠŸ")
        else:
            print(f"âœ— æ—¶é—´èŒƒå›´æŸ¥è¯¢å¤±è´¥: æœŸæœ›2ä¸ªï¼Œå®é™…{len(recent_metrics)}ä¸ª")
            return False
        
        # æµ‹è¯•æ•°æ®æ¸…ç†
        await collector.cleanup_old_data(retention_seconds=0)  # æ¸…ç†æ‰€æœ‰æ•°æ®
        
        empty_metrics = await collector.get_metrics("api_latency")
        if len(empty_metrics) == 0:
            print("âœ“ æ•°æ®æ¸…ç†åŠŸèƒ½æ­£å¸¸")
        else:
            print(f"âœ— æ•°æ®æ¸…ç†å¤±è´¥: æœŸæœ›0ä¸ªï¼Œå®é™…{len(empty_metrics)}ä¸ª")
            return False
        
        return True
        
    except Exception as e:
        print(f"âœ— SLOæ•°æ®æ”¶é›†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_slo_evaluation():
    """æµ‹è¯•SLOè¯„ä¼°"""
    print("\n=== æµ‹è¯•SLOè¯„ä¼° ===")
    
    try:
        from services.slo_monitoring import (
            SLODataCollector, SLOEvaluator, SLOTarget, SLOType, SLOStatus
        )
        
        # åˆ›å»ºæ”¶é›†å™¨å’Œè¯„ä¼°å™¨
        collector = SLODataCollector()
        evaluator = SLOEvaluator(collector)
        
        print("âœ“ SLOè¯„ä¼°å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºAPIå»¶è¿ŸSLOç›®æ ‡
        latency_target = SLOTarget(
            name="test_api_latency",
            slo_type=SLOType.LATENCY,
            target_value=100.0,  # ç›®æ ‡å€¼100ms
            threshold_warning=80.0,  # è­¦å‘Šé˜ˆå€¼80ms
            threshold_critical=50.0,  # ä¸¥é‡é˜ˆå€¼50ms
            measurement_window_seconds=60,  # 1åˆ†é’Ÿçª—å£
            evaluation_interval_seconds=30,  # 30ç§’è¯„ä¼°
            aggregation_method="p95"
        )
        
        # åˆ›å»ºå¯ç”¨æ€§SLOç›®æ ‡
        availability_target = SLOTarget(
            name="test_availability",
            slo_type=SLOType.AVAILABILITY,
            target_value=99.9,  # ç›®æ ‡99.9%
            threshold_warning=99.5,  # è­¦å‘Š99.5%
            threshold_critical=99.0,  # ä¸¥é‡99.0%
            measurement_window_seconds=300,  # 5åˆ†é’Ÿçª—å£
            evaluation_interval_seconds=60,
            aggregation_method="average"
        )
        
        # æ¨¡æ‹Ÿæ”¶é›†å»¶è¿Ÿæ•°æ®ï¼ˆå¥åº·çŠ¶æ€ï¼‰
        healthy_latencies = [25, 32, 28, 41, 33, 35, 29, 47, 31, 38]  # éƒ½å°äº50msï¼Œåº”è¯¥æ˜¯HEALTHY
        for latency in healthy_latencies:
            await collector.record_metric("test_api_latency", latency)
            await asyncio.sleep(0.05)
        
        # è¯„ä¼°å»¶è¿ŸSLOï¼ˆåº”è¯¥æ˜¯å¥åº·ï¼‰
        latency_evaluation = await evaluator.evaluate_slo(latency_target)
        
        if latency_evaluation.status == SLOStatus.HEALTHY:
            print("âœ“ å»¶è¿ŸSLOå¥åº·çŠ¶æ€è¯„ä¼°æ­£ç¡®")
        else:
            print(f"âœ— å»¶è¿ŸSLOå¥åº·çŠ¶æ€è¯„ä¼°å¤±è´¥: {latency_evaluation.status}")
            return False
        
        print(f"  P95å»¶è¿Ÿ: {latency_evaluation.current_value:.1f}ms")
        print(f"  æ ·æœ¬æ•°: {latency_evaluation.sample_count}")
        
        # æ¨¡æ‹Ÿæ”¶é›†å»¶è¿Ÿæ•°æ®ï¼ˆè­¦å‘ŠçŠ¶æ€ï¼‰  
        warning_latencies = [55, 62, 58, 75, 68, 71, 64, 77, 66, 69]  # åœ¨50-80msä¹‹é—´ï¼Œåº”è¯¥æ˜¯WARNING
        for latency in warning_latencies:
            await collector.record_metric("test_api_latency", latency)
            await asyncio.sleep(0.05)
        
        # é‡æ–°è¯„ä¼°å»¶è¿ŸSLOï¼ˆåº”è¯¥æ˜¯è­¦å‘Šï¼‰
        latency_evaluation_warning = await evaluator.evaluate_slo(latency_target)
        
        if latency_evaluation_warning.status == SLOStatus.WARNING:
            print("âœ“ å»¶è¿ŸSLOè­¦å‘ŠçŠ¶æ€è¯„ä¼°æ­£ç¡®")
        else:
            print(f"âœ— å»¶è¿ŸSLOè­¦å‘ŠçŠ¶æ€è¯„ä¼°å¤±è´¥: {latency_evaluation_warning.status}")
            return False
        
        # æ¨¡æ‹Ÿå¯ç”¨æ€§æ•°æ®
        availability_data = [99.95, 99.92, 99.98, 99.94, 99.96]  # é«˜å¯ç”¨æ€§
        for availability in availability_data:
            await collector.record_metric("test_availability", availability)
            await asyncio.sleep(0.1)
        
        # è¯„ä¼°å¯ç”¨æ€§SLO
        availability_evaluation = await evaluator.evaluate_slo(availability_target)
        
        if availability_evaluation.status == SLOStatus.HEALTHY:
            print("âœ“ å¯ç”¨æ€§SLOå¥åº·çŠ¶æ€è¯„ä¼°æ­£ç¡®")
        else:
            print(f"âœ— å¯ç”¨æ€§SLOè¯„ä¼°å¤±è´¥: {availability_evaluation.status}")
            return False
        
        print(f"  å¹³å‡å¯ç”¨æ€§: {availability_evaluation.current_value:.2f}%")
        print(f"  é”™è¯¯é¢„ç®—å‰©ä½™: {availability_evaluation.error_budget_remaining:.1%}")
        
        return True
        
    except Exception as e:
        print(f"âœ— SLOè¯„ä¼°æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_slo_alerting():
    """æµ‹è¯•SLOå‘Šè­¦"""
    print("\n=== æµ‹è¯•SLOå‘Šè­¦ ===")
    
    try:
        from services.slo_monitoring import (
            SLOAlertManager, SLOEvaluation, SLOTarget, SLOType, SLOStatus, AlertSeverity
        )
        
        alert_manager = SLOAlertManager()
        print("âœ“ SLOå‘Šè­¦ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºå‘Šè­¦å¤„ç†å™¨
        received_alerts = []
        
        def alert_handler(alert):
            received_alerts.append(alert)
            print(f"  ğŸ“¢ å‘Šè­¦è§¦å‘: {alert.message}")
        
        alert_manager.add_alert_handler(alert_handler)
        
        # åˆ›å»ºæµ‹è¯•SLOç›®æ ‡
        target = SLOTarget(
            name="test_error_rate",
            slo_type=SLOType.ERROR_RATE,
            target_value=1.0,  # ç›®æ ‡é”™è¯¯ç‡1%
            threshold_warning=0.5,  # è­¦å‘Š0.5%
            threshold_critical=0.1,  # ä¸¥é‡0.1%
            measurement_window_seconds=300,
            evaluation_interval_seconds=60,
            aggregation_method="average"
        )
        
        # æ¨¡æ‹Ÿå¥åº·è¯„ä¼°ï¼ˆæ— å‘Šè­¦ï¼‰
        healthy_evaluation = SLOEvaluation(
            slo_name="test_error_rate",
            timestamp=time.time(),
            status=SLOStatus.HEALTHY,
            current_value=0.05,  # 0.05%é”™è¯¯ç‡
            target_value=1.0,
            error_budget_remaining=0.95,
            sample_count=100,
            evaluation_window_seconds=300
        )
        
        await alert_manager.process_evaluation(healthy_evaluation, target)
        
        if len(received_alerts) == 0:
            print("âœ“ å¥åº·çŠ¶æ€æ— å‘Šè­¦")
        else:
            print(f"âœ— å¥åº·çŠ¶æ€ä¸åº”è¯¥æœ‰å‘Šè­¦ï¼Œå®é™…æ”¶åˆ°{len(received_alerts)}ä¸ª")
            return False
        
        # æ¨¡æ‹Ÿè­¦å‘Šè¯„ä¼°
        warning_evaluation = SLOEvaluation(
            slo_name="test_error_rate",
            timestamp=time.time(),
            status=SLOStatus.WARNING,
            current_value=0.7,  # 0.7%é”™è¯¯ç‡
            target_value=1.0,
            error_budget_remaining=0.3,
            sample_count=100,
            evaluation_window_seconds=300
        )
        
        await alert_manager.process_evaluation(warning_evaluation, target)
        
        if len(received_alerts) == 1 and received_alerts[0].severity == AlertSeverity.WARNING:
            print("âœ“ è­¦å‘ŠçŠ¶æ€å‘Šè­¦è§¦å‘æ­£ç¡®")
        else:
            print(f"âœ— è­¦å‘ŠçŠ¶æ€å‘Šè­¦è§¦å‘å¤±è´¥")
            return False
        
        # æ¨¡æ‹Ÿä¸¥é‡è¯„ä¼°
        critical_evaluation = SLOEvaluation(
            slo_name="test_error_rate",
            timestamp=time.time(),
            status=SLOStatus.CRITICAL,
            current_value=0.8,  # 0.8%é”™è¯¯ç‡
            target_value=1.0,
            error_budget_remaining=0.2,
            sample_count=100,
            evaluation_window_seconds=300
        )
        
        await alert_manager.process_evaluation(critical_evaluation, target)
        
        if len(received_alerts) == 2 and received_alerts[1].severity == AlertSeverity.ERROR:
            print("âœ“ ä¸¥é‡çŠ¶æ€å‘Šè­¦è§¦å‘æ­£ç¡®")
        else:
            print(f"âœ— ä¸¥é‡çŠ¶æ€å‘Šè­¦è§¦å‘å¤±è´¥")
            return False
        
        # æ¨¡æ‹Ÿè¿åè¯„ä¼°
        breach_evaluation = SLOEvaluation(
            slo_name="test_error_rate",
            timestamp=time.time(),
            status=SLOStatus.BREACH,
            current_value=1.5,  # 1.5%é”™è¯¯ç‡ï¼Œè¶…è¿‡ç›®æ ‡
            target_value=1.0,
            error_budget_remaining=0.0,
            sample_count=100,
            evaluation_window_seconds=300
        )
        
        await alert_manager.process_evaluation(breach_evaluation, target)
        
        if len(received_alerts) == 3 and received_alerts[2].severity == AlertSeverity.CRITICAL:
            print("âœ“ è¿åçŠ¶æ€å‘Šè­¦è§¦å‘æ­£ç¡®")
        else:
            print(f"âœ— è¿åçŠ¶æ€å‘Šè­¦è§¦å‘å¤±è´¥")
            return False
        
        # æ£€æŸ¥æ´»è·ƒå‘Šè­¦
        active_alerts = alert_manager.get_active_alerts()
        if len(active_alerts) > 0:
            print(f"âœ“ æ´»è·ƒå‘Šè­¦: {len(active_alerts)} ä¸ª")
        else:
            print("âœ— åº”è¯¥æœ‰æ´»è·ƒå‘Šè­¦")
            return False
        
        # æ¨¡æ‹Ÿæ¢å¤ï¼ˆå›åˆ°å¥åº·çŠ¶æ€ï¼‰
        recovered_evaluation = SLOEvaluation(
            slo_name="test_error_rate",
            timestamp=time.time(),
            status=SLOStatus.HEALTHY,
            current_value=0.05,  # 0.05%é”™è¯¯ç‡
            target_value=1.0,
            error_budget_remaining=0.95,
            sample_count=100,
            evaluation_window_seconds=300
        )
        
        await alert_manager.process_evaluation(recovered_evaluation, target)
        
        # æ£€æŸ¥å‘Šè­¦æ˜¯å¦å·²è§£é™¤
        final_active_alerts = alert_manager.get_active_alerts()
        if len(final_active_alerts) == 0:
            print("âœ“ å‘Šè­¦å·²è‡ªåŠ¨è§£é™¤")
        else:
            print(f"âœ— å‘Šè­¦æœªæ­£ç¡®è§£é™¤ï¼Œä»æœ‰{len(final_active_alerts)}ä¸ªæ´»è·ƒå‘Šè­¦")
            return False
        
        # æ£€æŸ¥å‘Šè­¦å†å²
        alert_history = alert_manager.get_alert_history()
        if len(alert_history) == 3:
            print("âœ“ å‘Šè­¦å†å²è®°å½•æ­£ç¡®")
        else:
            print(f"âœ— å‘Šè­¦å†å²è®°å½•å¼‚å¸¸: æœŸæœ›3ä¸ªï¼Œå®é™…{len(alert_history)}ä¸ª")
            return False
        
        return True
        
    except Exception as e:
        print(f"âœ— SLOå‘Šè­¦æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_slo_monitoring_service():
    """æµ‹è¯•SLOç›‘æ§æœåŠ¡é›†æˆ"""
    print("\n=== æµ‹è¯•SLOç›‘æ§æœåŠ¡ ===")
    
    try:
        from services.slo_monitoring import (
            SLOMonitoringService, 
            create_api_latency_slo,
            create_availability_slo,
            create_error_rate_slo
        )
        
        # åˆ›å»ºç›‘æ§æœåŠ¡
        service = SLOMonitoringService()
        print("âœ“ SLOç›‘æ§æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        
        # æ·»åŠ SLOç›®æ ‡
        latency_slo = create_api_latency_slo("test_api_latency", 100.0, 80.0, 50.0)
        availability_slo = create_availability_slo("test_availability", 99.9, 99.5, 99.0)
        error_rate_slo = create_error_rate_slo("test_error_rate", 1.0, 0.5, 0.1)
        
        service.add_slo_target(latency_slo)
        service.add_slo_target(availability_slo)
        service.add_slo_target(error_rate_slo)
        
        print("âœ“ SLOç›®æ ‡å·²æ·»åŠ ")
        
        # å¯åŠ¨ç›‘æ§æœåŠ¡
        await service.start()
        print("âœ“ SLOç›‘æ§æœåŠ¡å·²å¯åŠ¨")
        
        # æ¨¡æ‹ŸæŒ‡æ ‡æ•°æ®è®°å½•
        await service.record_metric("test_api_latency", 45.5)
        await service.record_metric("test_api_latency", 67.8)
        await service.record_metric("test_api_latency", 52.3)
        
        await service.record_metric("test_availability", 99.95)
        await service.record_metric("test_availability", 99.92)
        
        await service.record_metric("test_error_rate", 0.3)
        await service.record_metric("test_error_rate", 0.7)  # è§¦å‘è­¦å‘Š
        
        print("âœ“ æŒ‡æ ‡æ•°æ®å·²è®°å½•")
        
        # ç­‰å¾…è¯„ä¼°å‘¨æœŸ
        await asyncio.sleep(2)
        
        # è·å–è¯„ä¼°ç»“æœ
        evaluations = await service.get_all_evaluations()
        
        if len(evaluations) == 3:
            print("âœ“ æ‰€æœ‰SLOè¯„ä¼°å®Œæˆ")
            
            for evaluation in evaluations:
                print(f"  {evaluation.slo_name}: {evaluation.status.value} "
                      f"(å½“å‰å€¼: {evaluation.current_value:.2f}, "
                      f"ç›®æ ‡å€¼: {evaluation.target_value:.2f})")
        else:
            print(f"âœ— SLOè¯„ä¼°æ•°é‡ä¸æ­£ç¡®: æœŸæœ›3ä¸ªï¼Œå®é™…{len(evaluations)}ä¸ª")
            return False
        
        # è·å–ä»ªè¡¨æ¿æ•°æ®
        dashboard_data = service.get_dashboard_data()
        
        if dashboard_data["summary"]["total_slos"] == 3:
            print("âœ“ ä»ªè¡¨æ¿æ•°æ®æ­£ç¡®")
            print(f"  æ€»SLOæ•°: {dashboard_data['summary']['total_slos']}")
            print(f"  å¯ç”¨SLOæ•°: {dashboard_data['summary']['enabled_slos']}")
            print(f"  æ´»è·ƒå‘Šè­¦æ•°: {dashboard_data['summary']['active_alerts']}")
        else:
            print("âœ— ä»ªè¡¨æ¿æ•°æ®å¼‚å¸¸")
            return False
        
        # åœæ­¢ç›‘æ§æœåŠ¡
        await service.stop()
        print("âœ“ SLOç›‘æ§æœåŠ¡å·²åœæ­¢")
        
        return True
        
    except Exception as e:
        print(f"âœ— SLOç›‘æ§æœåŠ¡æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_slo_performance():
    """æµ‹è¯•SLOæ€§èƒ½"""
    print("\n=== æµ‹è¯•SLOæ€§èƒ½ ===")
    
    try:
        from services.slo_monitoring import SLOMonitoringService, create_api_latency_slo
        
        service = SLOMonitoringService()
        
        # åˆ›å»ºSLOç›®æ ‡
        latency_slo = create_api_latency_slo("perf_test_latency")
        service.add_slo_target(latency_slo)
        
        # å¤§é‡æ•°æ®å†™å…¥æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        
        metric_count = 10000
        for i in range(metric_count):
            latency = random.uniform(20, 200)  # 20-200mséšæœºå»¶è¿Ÿ
            await service.record_metric("perf_test_latency", latency)
            
            if i % 1000 == 0:
                print(f"  å·²è®°å½• {i} ä¸ªæŒ‡æ ‡...")
        
        write_time = time.time() - start_time
        
        print(f"âœ“ å†™å…¥æ€§èƒ½æµ‹è¯•å®Œæˆ:")
        print(f"  æŒ‡æ ‡æ•°é‡: {metric_count}")
        print(f"  æ€»è€—æ—¶: {write_time:.2f} ç§’")
        print(f"  å†™å…¥é€Ÿç‡: {metric_count/write_time:.0f} æŒ‡æ ‡/ç§’")
        
        # è¯„ä¼°æ€§èƒ½æµ‹è¯•
        start_time = time.time()
        
        evaluation_count = 100
        for i in range(evaluation_count):
            evaluation = await service.get_slo_evaluation("perf_test_latency")
            if evaluation and evaluation.sample_count > 0:
                continue
        
        eval_time = time.time() - start_time
        
        print(f"âœ“ è¯„ä¼°æ€§èƒ½æµ‹è¯•å®Œæˆ:")
        print(f"  è¯„ä¼°æ¬¡æ•°: {evaluation_count}")
        print(f"  æ€»è€—æ—¶: {eval_time:.2f} ç§’")
        print(f"  è¯„ä¼°é€Ÿç‡: {evaluation_count/eval_time:.0f} è¯„ä¼°/ç§’")
        
        # æœ€ç»ˆè¯„ä¼°ç»“æœ
        final_evaluation = await service.get_slo_evaluation("perf_test_latency")
        if final_evaluation:
            print(f"âœ“ æœ€ç»ˆè¯„ä¼°ç»“æœ:")
            print(f"  æ ·æœ¬æ•°é‡: {final_evaluation.sample_count}")
            print(f"  P95å»¶è¿Ÿ: {final_evaluation.current_value:.1f}ms")
            print(f"  çŠ¶æ€: {final_evaluation.status.value}")
            print(f"  é”™è¯¯é¢„ç®—: {final_evaluation.error_budget_remaining:.1%}")
        
        return True
        
    except Exception as e:
        print(f"âœ— SLOæ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """è¿è¡Œæ‰€æœ‰SLOç›‘æ§æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹SLOæ€§èƒ½ç›‘æ§æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        test_slo_data_collection,
        test_slo_evaluation,
        test_slo_alerting,
        test_slo_monitoring_service,
        test_slo_performance
    ]
    
    passed = 0
    total = len(tests)
    
    for i, test in enumerate(tests):
        try:
            if asyncio.iscoroutinefunction(test):
                result = await test()
            else:
                result = test()
            
            if result:
                passed += 1
        except Exception as e:
            print(f"æµ‹è¯•å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 60)
    print(f"ğŸ SLOæ€§èƒ½ç›‘æ§æµ‹è¯•å®Œæˆ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰SLOç›‘æ§æµ‹è¯•é€šè¿‡!")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å®ç°")
        return 1

if __name__ == "__main__":
    exit(asyncio.run(main()))