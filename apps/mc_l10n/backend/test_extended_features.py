#!/usr/bin/env python
"""
æµ‹è¯•æ‰©å±•åŠŸèƒ½é›†æˆ
éªŒè¯ç£ç›˜ç¼“å­˜ã€åŒæ­¥ç›‘æ§ã€è¦†ç›–é“¾å¤„ç†ç­‰æ–°åŠŸèƒ½
"""

import sys
import os
import time
import json
import asyncio
sys.path.append('.')

import structlog

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.dev.ConsoleRenderer()
    ]
)

logger = structlog.get_logger(__name__)

async def test_disk_cache():
    """æµ‹è¯•ç£ç›˜JSONç¼“å­˜"""
    print("=== æµ‹è¯•ç£ç›˜JSONç¼“å­˜ ===")
    
    try:
        from services.disk_cache import DiskJSONCache
        
        # åˆ›å»ºä¸´æ—¶ç¼“å­˜ç›®å½•
        cache_dir = "./test_cache"
        cache = DiskJSONCache(
            cache_dir=cache_dir,
            max_size_mb=10,
            default_ttl=3600
        )
        
        await cache.initialize()
        print("âœ“ ç£ç›˜ç¼“å­˜åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•æ•°æ®å­˜å‚¨
        test_data = {
            "translations": [
                {"key": "item.test.disk_cache", "src": "Disk Cache Test", "dst": "ç£ç›˜ç¼“å­˜æµ‹è¯•"},
                {"key": "block.test.performance", "src": "Performance Block", "dst": "æ€§èƒ½æ–¹å—"},
            ],
            "metadata": {
                "locale": "zh_cn",
                "source": "disk_cache_test"
            }
        }
        
        success = await cache.put("test_translations", test_data, ttl=60)
        if success:
            print("âœ“ æ•°æ®å­˜å‚¨æˆåŠŸ")
        else:
            print("âœ— æ•°æ®å­˜å‚¨å¤±è´¥")
            return False
        
        # æµ‹è¯•æ•°æ®è¯»å–
        retrieved_data = await cache.get("test_translations")
        if retrieved_data == test_data:
            print("âœ“ æ•°æ®è¯»å–æˆåŠŸï¼Œå†…å®¹åŒ¹é…")
        else:
            print("âœ— æ•°æ®è¯»å–å¤±è´¥æˆ–å†…å®¹ä¸åŒ¹é…")
            return False
        
        # æµ‹è¯•ç¼“å­˜ç»Ÿè®¡
        stats = cache.get_stats()
        print(f"âœ“ ç¼“å­˜ç»Ÿè®¡: {stats['total_entries']} æ¡ç›®, {stats['total_size_mb']} MB")
        
        # æµ‹è¯•æ¡ç›®ä¿¡æ¯
        entries = cache.get_entries_info()
        print(f"âœ“ æ¡ç›®ä¿¡æ¯: {len(entries)} ä¸ªæ¡ç›®")
        
        # æ¸…ç†æµ‹è¯•ç¼“å­˜
        await cache.clear()
        await cache.shutdown()
        
        # æ¸…ç†ç›®å½•
        import shutil
        if os.path.exists(cache_dir):
            shutil.rmtree(cache_dir)
        
        return True
        
    except Exception as e:
        print(f"âœ— ç£ç›˜ç¼“å­˜æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_sync_metrics():
    """æµ‹è¯•åŒæ­¥åè®®æ€§èƒ½ç›‘æ§"""
    print("\n=== æµ‹è¯•åŒæ­¥åè®®æ€§èƒ½ç›‘æ§ ===")
    
    try:
        from services.sync_metrics import SyncMetricsCollector
        
        collector = SyncMetricsCollector()
        print("âœ“ æ€§èƒ½ç›‘æ§åˆå§‹åŒ–")
        
        # æµ‹è¯•Bloomè¿‡æ»¤å™¨ç›‘æ§
        collector.record_bloom_handshake(
            elements_checked=1000,
            filter_size_bytes=8192,
            missing_count=50,
            total_count=1000
        )
        print("âœ“ Bloomæ¡æ‰‹æ€§èƒ½è®°å½•")
        
        # æµ‹è¯•å‹ç¼©ç›‘æ§
        collector.record_compression(
            algorithm="zstd",
            input_bytes=10240,
            output_bytes=3072,
            compression_time_ms=15.5
        )
        
        collector.record_compression(
            algorithm="gzip",
            input_bytes=8192,
            output_bytes=2048,
            compression_time_ms=25.2
        )
        print("âœ“ å‹ç¼©æ€§èƒ½è®°å½•")
        
        # æµ‹è¯•åŒæ­¥ä¼šè¯ç›‘æ§
        session = collector.start_sync_session("test_session_001", "client_001")
        
        collector.record_handshake_time("test_session_001", 50.0)
        collector.record_chunk_upload("test_session_001", 1024, 20.0)
        collector.record_chunk_upload("test_session_001", 2048, 35.0)
        collector.record_merge_operation("test_session_001", 100.0, 2)
        
        collector.complete_sync_session("test_session_001", "completed")
        print("âœ“ åŒæ­¥ä¼šè¯ç›‘æ§")
        
        # æµ‹è¯•ç»¼åˆç»Ÿè®¡
        stats = collector.get_comprehensive_stats()
        print(f"âœ“ ç»¼åˆç»Ÿè®¡: Bloomæ¡æ‰‹ {stats['bloom_filter']['total_handshakes']} æ¬¡")
        print(f"  å‹ç¼©æ“ä½œ {stats['compression']['total_operations']} æ¬¡")
        print(f"  å·²å®Œæˆä¼šè¯ {stats['sync_sessions']['completed_sessions']} ä¸ª")
        
        # æµ‹è¯•æ€§èƒ½è¶‹åŠ¿
        trends = collector.get_performance_trends(hours=1)
        print(f"âœ“ æ€§èƒ½è¶‹åŠ¿: ååé‡æ ·æœ¬ {trends['throughput']['samples']} ä¸ª")
        
        return True
        
    except Exception as e:
        print(f"âœ— åŒæ­¥ç›‘æ§æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_override_chain():
    """æµ‹è¯•è¦†ç›–é“¾å¤„ç†"""
    print("\n=== æµ‹è¯•è¦†ç›–é“¾å¤„ç† ===")
    
    try:
        from services.override_chain import (
            OverrideChainProcessor, 
            OverridePriority,
            OverrideSource,
            TranslationOverride
        )
        
        processor = OverrideChainProcessor()
        print("âœ“ è¦†ç›–é“¾å¤„ç†å™¨åˆå§‹åŒ–")
        
        # åˆ›å»ºæµ‹è¯•è¦†ç›–æº
        mod_source = processor.create_override_source(
            source_type="mod",
            source_id="test_mod",
            source_name="æµ‹è¯•MOD",
            version="1.0.0"
        )
        
        resource_pack_source = processor.create_override_source(
            source_type="resource_pack", 
            source_id="test_pack",
            source_name="æµ‹è¯•èµ„æºåŒ…",
            version="2.0.0"
        )
        
        print("âœ“ è¦†ç›–æºåˆ›å»ºæˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•ç¿»è¯‘è¦†ç›–
        mod_translation = TranslationOverride(
            key="item.test.override",
            locale="zh_cn",
            src_text="Override Test Item",
            dst_text="MODè¦†ç›–æµ‹è¯•ç‰©å“",
            status="reviewed",
            source=mod_source,
            priority=OverridePriority.MOD,
            locked_fields={"key", "src_text"},
            metadata={"source": "mod"},
            created_at="2025-09-10T01:00:00Z",
            updated_at="2025-09-10T01:00:00Z"
        )
        
        pack_translation = TranslationOverride(
            key="item.test.override",
            locale="zh_cn", 
            src_text="Override Test Item",
            dst_text="èµ„æºåŒ…è¦†ç›–æµ‹è¯•ç‰©å“",
            status="approved",
            source=resource_pack_source,
            priority=OverridePriority.RESOURCE_PACK,
            locked_fields={"key", "src_text", "dst_text"},
            metadata={"source": "resource_pack"},
            created_at="2025-09-10T02:00:00Z",
            updated_at="2025-09-10T02:00:00Z"
        )
        
        # å¤„ç†è¦†ç›–é“¾
        result = processor.process_override_chain(
            [mod_translation, pack_translation],
            "item.test.override",
            "zh_cn"
        )
        
        if result:
            print(f"âœ“ è¦†ç›–é“¾å¤„ç†æˆåŠŸ: æœ€ç»ˆä¼˜å…ˆçº§ {result.priority.name}")
            print(f"  æœ€ç»ˆç¿»è¯‘: {result.dst_text}")
            print(f"  é”å®šå­—æ®µ: {result.locked_fields}")
        else:
            print("âœ— è¦†ç›–é“¾å¤„ç†å¤±è´¥")
            return False
        
        # æµ‹è¯•æ‰¹é‡å¤„ç†
        batch_result = processor.batch_process_overrides(
            [mod_translation, pack_translation],
            "zh_cn"
        )
        
        print(f"âœ“ æ‰¹é‡å¤„ç†: {len(batch_result)} ä¸ªkeyå¤„ç†å®Œæˆ")
        
        # æµ‹è¯•æ‰‹åŠ¨è¦†ç›–
        manual_override = processor.create_manual_override(
            key="item.test.manual",
            locale="zh_cn",
            dst_text="æ‰‹åŠ¨è¦†ç›–ç¿»è¯‘",
            user_id="test_user",
            reason="æµ‹è¯•æ‰‹åŠ¨è¦†ç›–åŠŸèƒ½"
        )
        
        print(f"âœ“ æ‰‹åŠ¨è¦†ç›–åˆ›å»º: {manual_override.priority.name}")
        
        # æµ‹è¯•éªŒè¯
        all_translations = [mod_translation, pack_translation, manual_override]
        issues = processor.validate_override_chain(all_translations)
        print(f"âœ“ è¦†ç›–é“¾éªŒè¯: {len(issues)} ä¸ªé—®é¢˜")
        
        # æµ‹è¯•ç»Ÿè®¡
        stats = processor.get_override_statistics(all_translations)
        print(f"âœ“ è¦†ç›–é“¾ç»Ÿè®¡: æ€»è®¡ {stats['total_translations']} ä¸ªç¿»è¯‘")
        print(f"  æŒ‰ä¼˜å…ˆçº§: {stats['by_priority']}")
        
        return True
        
    except Exception as e:
        print(f"âœ— è¦†ç›–é“¾æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def test_integrated_workflow():
    """æµ‹è¯•é›†æˆå·¥ä½œæµ"""
    print("\n=== æµ‹è¯•é›†æˆå·¥ä½œæµ ===")
    
    try:
        # æ¨¡æ‹Ÿä¸€ä¸ªå®Œæ•´çš„å·¥ä½œæµï¼šç¼“å­˜ + ç›‘æ§ + è¦†ç›–
        from services.sync_metrics import get_sync_metrics
        from services.override_chain import get_override_processor
        
        metrics = get_sync_metrics()
        processor = get_override_processor()
        
        # æ¨¡æ‹ŸåŒæ­¥ä¼šè¯å¼€å§‹
        session = metrics.start_sync_session("workflow_test", "integrated_client")
        
        # æ¨¡æ‹ŸBloomæ¡æ‰‹
        start_time = time.time()
        await asyncio.sleep(0.05)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
        handshake_time = (time.time() - start_time) * 1000
        
        metrics.record_handshake_time(session.session_id, handshake_time)
        metrics.record_bloom_handshake(500, 4096, 25, 500)
        
        # æ¨¡æ‹Ÿæ•°æ®å‹ç¼©å’Œä¼ è¾“
        metrics.record_compression("zstd", 20480, 4096, 12.5)
        metrics.record_chunk_upload(session.session_id, 4096, 30.0)
        
        # æ¨¡æ‹Ÿè¦†ç›–é“¾å¤„ç†
        # (è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”ç”¨ä¸­ä¼šæœ‰å¤æ‚çš„ç¿»è¯‘æ•°æ®)
        
        # å®Œæˆä¼šè¯
        metrics.complete_sync_session(session.session_id, "completed")
        
        print("âœ“ é›†æˆå·¥ä½œæµå®Œæˆ")
        
        # è·å–æœ€ç»ˆç»Ÿè®¡
        final_stats = metrics.get_comprehensive_stats()
        print(f"  æœ€ç»ˆä¼šè¯æ•°: {final_stats['sync_sessions']['completed_sessions']}")
        print(f"  å‹ç¼©æ¯”: {final_stats['compression']['average_compression_ratio']:.3f}")
        
        return True
        
    except Exception as e:
        print(f"âœ— é›†æˆå·¥ä½œæµæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """è¿è¡Œæ‰€æœ‰æ‰©å±•åŠŸèƒ½æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹æ‰©å±•åŠŸèƒ½é›†æˆæµ‹è¯•")
    print("=" * 60)
    
    tests = [
        test_disk_cache,
        test_sync_metrics,
        test_override_chain,
        test_integrated_workflow
    ]
    
    passed = 0
    total = len(tests)
    
    for i, test in enumerate(tests):
        try:
            if asyncio.iscoroutinefunction(test):
                result = await test()
            else:
                result = test()
            
            if result:
                passed += 1
        except Exception as e:
            print(f"æµ‹è¯•å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 60)
    print(f"ğŸ æ‰©å±•åŠŸèƒ½æµ‹è¯•å®Œæˆ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ‰©å±•åŠŸèƒ½æµ‹è¯•é€šè¿‡!")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å®ç°")
        return 1

if __name__ == "__main__":
    exit(asyncio.run(main()))