#!/usr/bin/env python
"""
æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨
éªŒè¯å¹¶å‘ä¸Šä¼ ã€å†…å­˜ä¼˜åŒ–ã€æµå¼å¤„ç†ç­‰åŠŸèƒ½
"""

import sys
import asyncio
import tempfile
import time
import threading
from pathlib import Path
from typing import List, Dict
import structlog

sys.path.append('.')

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="ISO"),
        structlog.dev.ConsoleRenderer()
    ],
    wrapper_class=structlog.stdlib.BoundLogger,
    logger_factory=structlog.stdlib.LoggerFactory(),
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger(__name__)

from services.performance_optimizer import (
    PerformanceOptimizer, ConcurrentUploadManager, StreamingUploader,
    MemoryMonitor, PerformanceMetrics, UploadTask, TaskPriority
)


async def test_concurrent_upload_manager():
    """æµ‹è¯•å¹¶å‘ä¸Šä¼ ç®¡ç†å™¨"""
    print("=== æµ‹è¯•å¹¶å‘ä¸Šä¼ ç®¡ç†å™¨ ===")
    
    try:
        # åˆ›å»ºæµ‹è¯•ä¸Šä¼ ç®¡ç†å™¨
        manager = ConcurrentUploadManager(
            max_concurrent=4,
            chunk_size=1024,  # å°å—ç”¨äºæµ‹è¯•
            memory_limit_mb=50,
            enable_compression=False  # ç®€åŒ–æµ‹è¯•
        )
        
        # æ¨¡æ‹Ÿä¸Šä¼ å‡½æ•°
        upload_results = []
        upload_times = []
        
        async def mock_upload(cid: str, chunk_data: bytes, chunk_index: int, total_chunks: int) -> Dict:
            start_time = time.time()
            await asyncio.sleep(0.1)  # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            end_time = time.time()
            
            result = {
                "cid": cid,
                "chunk_index": chunk_index,
                "success": True,
                "upload_time": end_time - start_time
            }
            upload_results.append(result)
            upload_times.append(end_time - start_time)
            return result
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_data = b"x" * 5000  # 5KBæµ‹è¯•æ•°æ®
        
        # æµ‹è¯•å¹¶å‘ä¸Šä¼ 
        tasks = []
        for i in range(8):  # 8ä¸ªå¹¶å‘ä»»åŠ¡
            task = UploadTask(
                cid=f"test_cid_{i}",
                data=test_data,
                priority=TaskPriority.NORMAL,
                total_chunks=5,
                metadata={"test_id": i}
            )
            tasks.append(task)
        
        print(f"åˆ›å»º {len(tasks)} ä¸ªä¸Šä¼ ä»»åŠ¡")
        
        # æ‰§è¡Œå¹¶å‘ä¸Šä¼ 
        start_time = time.time()
        results = await manager.batch_upload(tasks, mock_upload)
        end_time = time.time()
        
        print(f"âœ“ å¹¶å‘ä¸Šä¼ å®Œæˆ")
        print(f"  æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
        print(f"  æˆåŠŸä»»åŠ¡: {len([r for r in results if r['success']])}/{len(results)}")
        print(f"  å¹³å‡å—ä¸Šä¼ æ—¶é—´: {sum(upload_times)/len(upload_times):.3f}ç§’")
        print(f"  æœ€å¤§å¹¶å‘æ§åˆ¶: {manager.max_concurrent}")
        
        # éªŒè¯å†…å­˜ç›‘æ§
        if hasattr(manager, 'memory_monitor'):
            memory_stats = manager.memory_monitor.get_memory_stats()
            print(f"  å³°å€¼å†…å­˜ä½¿ç”¨: {memory_stats.get('peak_memory_mb', 0):.1f}MB")
        
        return True
        
    except Exception as e:
        print(f"âœ— å¹¶å‘ä¸Šä¼ ç®¡ç†å™¨æµ‹è¯•å¤±è´¥: {e}")
        logger.error("å¹¶å‘ä¸Šä¼ æµ‹è¯•å¼‚å¸¸", error=str(e))
        return False


async def test_streaming_uploader():
    """æµ‹è¯•æµå¼ä¸Šä¼ å™¨"""
    print("\n=== æµ‹è¯•æµå¼ä¸Šä¼ å™¨ ===")
    
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        temp_dir = Path(tempfile.mkdtemp())
        test_file = temp_dir / "large_test_file.bin"
        
        # ç”Ÿæˆæµ‹è¯•æ–‡ä»¶ (1MB)
        test_data = b"A" * (1024 * 1024)
        test_file.write_bytes(test_data)
        
        print(f"åˆ›å»ºæµ‹è¯•æ–‡ä»¶: {test_file} ({len(test_data)} bytes)")
        
        # åˆ›å»ºæµå¼ä¸Šä¼ å™¨
        uploader = StreamingUploader(
            chunk_size=256 * 1024,  # 256KBå—
            max_retries=3,
            enable_compression=False
        )
        
        # æ¨¡æ‹Ÿä¸Šä¼ ç»“æœ
        uploaded_chunks = []
        
        def upload_callback(chunk_index: int, total_chunks: int, bytes_uploaded: int):
            progress = (chunk_index + 1) / total_chunks * 100
            uploaded_chunks.append(chunk_index)
            print(f"  ä¸Šä¼ è¿›åº¦: {progress:.1f}% (å— {chunk_index+1}/{total_chunks})")
        
        # æ‰§è¡Œæµå¼ä¸Šä¼ 
        start_time = time.time()
        chunk_cids = await uploader.upload_file_stream(
            test_file, 
            "test_file_cid",
            callback=upload_callback
        )
        end_time = time.time()
        
        print(f"âœ“ æµå¼ä¸Šä¼ å®Œæˆ")
        print(f"  æ€»è€—æ—¶: {end_time - start_time:.2f}ç§’")
        print(f"  ç”Ÿæˆåˆ†ç‰‡: {len(chunk_cids)}")
        print(f"  ä¸Šä¼ é€Ÿåº¦: {len(test_data) / (end_time - start_time) / 1024 / 1024:.2f} MB/s")
        print(f"  å†…å­˜ä½¿ç”¨: æµå¼å¤„ç†ï¼Œæ— éœ€å…¨éƒ¨åŠ è½½åˆ°å†…å­˜")
        
        # éªŒè¯æ‰€æœ‰å—éƒ½è¢«ä¸Šä¼ 
        expected_chunks = (len(test_data) + uploader.chunk_size - 1) // uploader.chunk_size
        if len(uploaded_chunks) == expected_chunks:
            print(f"  åˆ†ç‰‡å®Œæ•´æ€§: âœ“ æ‰€æœ‰ {expected_chunks} ä¸ªåˆ†ç‰‡å·²ä¸Šä¼ ")
        else:
            print(f"  åˆ†ç‰‡å®Œæ•´æ€§: âœ— é¢„æœŸ {expected_chunks}, å®é™… {len(uploaded_chunks)}")
        
        # æ¸…ç†
        import shutil
        shutil.rmtree(temp_dir)
        
        return True
        
    except Exception as e:
        print(f"âœ— æµå¼ä¸Šä¼ å™¨æµ‹è¯•å¤±è´¥: {e}")
        logger.error("æµå¼ä¸Šä¼ æµ‹è¯•å¼‚å¸¸", error=str(e))
        return False


async def test_memory_monitor():
    """æµ‹è¯•å†…å­˜ç›‘æ§å™¨"""
    print("\n=== æµ‹è¯•å†…å­˜ç›‘æ§å™¨ ===")
    
    try:
        monitor = MemoryMonitor(
            warning_threshold_mb=100,
            critical_threshold_mb=200,
            gc_interval_seconds=5
        )
        
        # å¯åŠ¨ç›‘æ§
        monitor.start_monitoring()
        print("âœ“ å†…å­˜ç›‘æ§å·²å¯åŠ¨")
        
        # è·å–åˆå§‹å†…å­˜çŠ¶æ€
        initial_stats = monitor.get_memory_stats()
        print(f"  åˆå§‹å†…å­˜ä½¿ç”¨: {initial_stats['current_memory_mb']:.1f}MB")
        print(f"  å¯ç”¨å†…å­˜: {initial_stats['available_memory_mb']:.1f}MB")
        
        # æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨
        large_data = []
        for i in range(10):
            # æ¯æ¬¡åˆ†é…10MB
            data = bytearray(10 * 1024 * 1024)
            large_data.append(data)
            await asyncio.sleep(0.1)
        
        # æ£€æŸ¥å†…å­˜å¢é•¿
        peak_stats = monitor.get_memory_stats()
        print(f"  å³°å€¼å†…å­˜ä½¿ç”¨: {peak_stats['peak_memory_mb']:.1f}MB")
        print(f"  å†…å­˜å¢é•¿: {peak_stats['current_memory_mb'] - initial_stats['current_memory_mb']:.1f}MB")
        
        # è§¦å‘åƒåœ¾å›æ”¶
        gc_count_before = monitor.gc_count
        monitor.force_gc()
        await asyncio.sleep(0.1)
        
        if monitor.gc_count > gc_count_before:
            print(f"  åƒåœ¾å›æ”¶: âœ“ å·²æ‰§è¡Œ ({monitor.gc_count - gc_count_before} æ¬¡)")
        
        # æ¸…ç†å†…å­˜
        del large_data
        monitor.force_gc()
        
        final_stats = monitor.get_memory_stats()
        print(f"  æ¸…ç†åå†…å­˜: {final_stats['current_memory_mb']:.1f}MB")
        
        # åœæ­¢ç›‘æ§
        monitor.stop_monitoring()
        print("âœ“ å†…å­˜ç›‘æ§å·²åœæ­¢")
        
        return True
        
    except Exception as e:
        print(f"âœ— å†…å­˜ç›‘æ§å™¨æµ‹è¯•å¤±è´¥: {e}")
        logger.error("å†…å­˜ç›‘æ§æµ‹è¯•å¼‚å¸¸", error=str(e))
        return False


async def test_performance_metrics():
    """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡æ”¶é›†"""
    print("\n=== æµ‹è¯•æ€§èƒ½æŒ‡æ ‡æ”¶é›† ===")
    
    try:
        # åˆ›å»ºæ€§èƒ½æŒ‡æ ‡
        metrics = PerformanceMetrics()
        print("âœ“ æ€§èƒ½æŒ‡æ ‡æ”¶é›†å™¨åˆå§‹åŒ–")
        
        # æ¨¡æ‹Ÿä¸Šä¼ æ“ä½œ
        await asyncio.sleep(0.1)
        metrics.completed_tasks = 45
        metrics.failed_tasks = 5
        metrics.uploaded_bytes = 1024 * 1024 * 100  # 100MB
        metrics.end_time = time.time() + 30.5  # è®¾ç½®ç»“æŸæ—¶é—´
        
        # æµ‹è¯•æ€§èƒ½è®¡ç®—
        success_rate = metrics.get_success_rate()
        throughput = metrics.get_throughput_mbps()
        avg_speed = metrics.get_average_upload_speed_mbps()
        
        print(f"  ä¸Šä¼ æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"  å¹³å‡ååé‡: {throughput:.2f} MB/s")
        print(f"  å¹³å‡ä¸Šä¼ é€Ÿåº¦: {avg_speed:.2f} MB/s")
        
        # æµ‹è¯•æŒ‡æ ‡æ±‡æ€»
        summary = metrics.get_summary()
        print(f"  æŒ‡æ ‡æ±‡æ€»: {len(summary)} é¡¹")
        for key, value in list(summary.items())[:5]:  # æ˜¾ç¤ºå‰5é¡¹
            print(f"    {key}: {value}")
        
        # éªŒè¯è®¡ç®—æ­£ç¡®æ€§
        expected_success_rate = 45 / 50 * 100
        if abs(success_rate - expected_success_rate) < 0.1:
            print("  âœ“ æˆåŠŸç‡è®¡ç®—æ­£ç¡®")
        else:
            print(f"  âœ— æˆåŠŸç‡è®¡ç®—é”™è¯¯: æœŸæœ› {expected_success_rate}, å®é™… {success_rate}")
        
        return True
        
    except Exception as e:
        print(f"âœ— æ€§èƒ½æŒ‡æ ‡æµ‹è¯•å¤±è´¥: {e}")
        logger.error("æ€§èƒ½æŒ‡æ ‡æµ‹è¯•å¼‚å¸¸", error=str(e))
        return False


async def test_performance_optimizer_integration():
    """æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨é›†æˆ"""
    print("\n=== æµ‹è¯•æ€§èƒ½ä¼˜åŒ–å™¨é›†æˆ ===")
    
    try:
        # åˆ›å»ºæ€§èƒ½ä¼˜åŒ–å™¨
        optimizer = PerformanceOptimizer(
            max_concurrent_uploads=4,
            chunk_size_kb=256,
            memory_limit_mb=100,
            enable_compression=False
        )
        
        print("âœ“ æ€§èƒ½ä¼˜åŒ–å™¨åˆå§‹åŒ–")
        
        # å¯åŠ¨ä¼˜åŒ–å™¨
        await optimizer.start()
        print("âœ“ æ€§èƒ½ä¼˜åŒ–å™¨å·²å¯åŠ¨")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_files = []
        temp_dir = Path(tempfile.mkdtemp())
        
        for i in range(5):
            file_path = temp_dir / f"test_file_{i}.bin"
            # åˆ›å»ºä¸åŒå¤§å°çš„æ–‡ä»¶
            file_data = b"TEST_DATA_" * (1000 * (i + 1))  # é€’å¢å¤§å°
            file_path.write_bytes(file_data)
            test_files.append(file_path)
        
        print(f"åˆ›å»º {len(test_files)} ä¸ªæµ‹è¯•æ–‡ä»¶")
        
        # æ¨¡æ‹Ÿæ‰¹é‡ä¸Šä¼ 
        results = []
        
        async def process_file(file_path: Path):
            # æ¨¡æ‹Ÿæ–‡ä»¶å¤„ç†
            await asyncio.sleep(0.1)
            return {
                "file": file_path.name,
                "size": file_path.stat().st_size,
                "status": "success"
            }
        
        # å¹¶å‘å¤„ç†æ–‡ä»¶
        start_time = time.time()
        tasks = [process_file(f) for f in test_files]
        results = await asyncio.gather(*tasks)
        end_time = time.time()
        
        print(f"âœ“ æ‰¹é‡å¤„ç†å®Œæˆ")
        print(f"  å¤„ç†æ—¶é—´: {end_time - start_time:.2f}ç§’")
        print(f"  æˆåŠŸæ–‡ä»¶: {len([r for r in results if r['status'] == 'success'])}/{len(results)}")
        
        # è·å–æ€§èƒ½æŒ‡æ ‡
        if hasattr(optimizer, 'metrics'):
            summary = optimizer.metrics.get_summary()
            print(f"  æ€§èƒ½æŒ‡æ ‡: {len(summary)} é¡¹ç›‘æ§æ•°æ®")
        
        # åœæ­¢ä¼˜åŒ–å™¨
        await optimizer.stop()
        print("âœ“ æ€§èƒ½ä¼˜åŒ–å™¨å·²åœæ­¢")
        
        # æ¸…ç†
        import shutil
        shutil.rmtree(temp_dir)
        
        return True
        
    except Exception as e:
        print(f"âœ— æ€§èƒ½ä¼˜åŒ–å™¨é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        logger.error("æ€§èƒ½ä¼˜åŒ–å™¨é›†æˆæµ‹è¯•å¼‚å¸¸", error=str(e))
        return False


async def test_priority_queue():
    """æµ‹è¯•ä¼˜å…ˆçº§é˜Ÿåˆ—åŠŸèƒ½"""
    print("\n=== æµ‹è¯•ä¼˜å…ˆçº§é˜Ÿåˆ— ===")
    
    try:
        manager = ConcurrentUploadManager(max_concurrent=2)
        
        # åˆ›å»ºä¸åŒä¼˜å…ˆçº§çš„ä»»åŠ¡
        tasks = [
            UploadTask("low_1", b"data1", TaskPriority.LOW, 1, {"order": 1}),
            UploadTask("high_1", b"data2", TaskPriority.HIGH, 1, {"order": 2}),
            UploadTask("normal_1", b"data3", TaskPriority.NORMAL, 1, {"order": 3}),
            UploadTask("high_2", b"data4", TaskPriority.HIGH, 1, {"order": 4}),
            UploadTask("low_2", b"data5", TaskPriority.LOW, 1, {"order": 5}),
        ]
        
        # è®°å½•æ‰§è¡Œé¡ºåº
        execution_order = []
        
        async def priority_upload(cid: str, chunk_data: bytes, chunk_index: int, total_chunks: int) -> Dict:
            execution_order.append(cid)
            await asyncio.sleep(0.1)
            return {"cid": cid, "success": True}
        
        # æ‰§è¡Œä¼˜å…ˆçº§ä¸Šä¼ 
        results = await manager.batch_upload(tasks, priority_upload)
        
        print(f"âœ“ ä¼˜å…ˆçº§é˜Ÿåˆ—æµ‹è¯•å®Œæˆ")
        print(f"  æ‰§è¡Œé¡ºåº: {execution_order}")
        print(f"  æˆåŠŸä»»åŠ¡: {len([r for r in results if r['success']])}")
        
        # éªŒè¯é«˜ä¼˜å…ˆçº§ä»»åŠ¡ä¼˜å…ˆæ‰§è¡Œ
        high_priority_positions = [i for i, cid in enumerate(execution_order) if cid.startswith('high_')]
        if high_priority_positions and max(high_priority_positions) < len(execution_order) - 1:
            print("  âœ“ é«˜ä¼˜å…ˆçº§ä»»åŠ¡ä¼˜å…ˆæ‰§è¡Œ")
        else:
            print("  âš ï¸ ä¼˜å…ˆçº§é¡ºåºå¯èƒ½å—å¹¶å‘å½±å“")
        
        return True
        
    except Exception as e:
        print(f"âœ— ä¼˜å…ˆçº§é˜Ÿåˆ—æµ‹è¯•å¤±è´¥: {e}")
        return False


async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æ€§èƒ½ä¼˜åŒ–å™¨æµ‹è¯•"""
    print("ğŸš€ å¼€å§‹æ€§èƒ½ä¼˜åŒ–å™¨æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        test_concurrent_upload_manager,
        test_streaming_uploader,
        test_memory_monitor,
        test_performance_metrics,
        test_priority_queue,
        test_performance_optimizer_integration,
    ]
    
    passed = 0
    total = len(tests)
    
    for test in tests:
        try:
            result = await test()
            if result:
                passed += 1
        except Exception as e:
            logger.error(f"æµ‹è¯•æ‰§è¡Œå¼‚å¸¸: {test.__name__}", error=str(e))
    
    print("=" * 60)
    print(f"ğŸ æ€§èƒ½ä¼˜åŒ–å™¨æµ‹è¯•å®Œæˆ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ€§èƒ½ä¼˜åŒ–å™¨æµ‹è¯•é€šè¿‡!")
        return True
    else:
        print(f"âš ï¸  æœ‰ {total - passed} ä¸ªæµ‹è¯•å¤±è´¥")
        return False


if __name__ == "__main__":
    asyncio.run(run_all_tests())