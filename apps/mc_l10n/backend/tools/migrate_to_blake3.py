#!/usr/bin/env python
"""
è¿ç§»å·¥å…·ï¼šå°†ç°æœ‰ç³»ç»Ÿä»MD5/SHA256å“ˆå¸Œå‡çº§åˆ°BLAKE3
è‡ªåŠ¨æ›¿æ¢ç£ç›˜ç¼“å­˜å’Œå…¶ä»–å“ˆå¸Œä½¿ç”¨
"""

import sys
import os
import re
import json
import time
from pathlib import Path
from typing import List, Dict, Any, Optional

sys.path.append('.')

import structlog
from services.content_addressing import (
    ContentAddressingSystem, 
    HashAlgorithm, 
    compute_cid,
    benchmark_hash_algorithms
)
from services.blake3_disk_cache import Blake3DiskCache

logger = structlog.get_logger(__name__)

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.dev.ConsoleRenderer()
    ]
)


class Blake3MigrationTool:
    """BLAKE3è¿ç§»å·¥å…·"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.migration_log = []
        
        # éœ€è¦æ›´æ–°çš„æ–‡ä»¶æ¨¡å¼
        self.file_patterns = [
            "**/*.py",  # Pythonæ–‡ä»¶
            "**/*.sql", # SQLæ–‡ä»¶ï¼ˆå¦‚æœæœ‰å“ˆå¸Œç›¸å…³ï¼‰
        ]
        
        # éœ€è¦æ›¿æ¢çš„å“ˆå¸Œæ¨¡å¼
        self.hash_patterns = {
            # hashlib.md5 -> blake3
            r'hashlib\.md5\(\)': 'blake3.blake3()',
            r'hashlib\.md5\(([^)]+)\)': r'blake3.blake3(\1)',
            
            # hashlib.sha256 -> blake3 (åœ¨å†…å®¹å¯»å€åœºæ™¯)
            r'hashlib\.sha256\(\)\.hexdigest\(\)': 'blake3.blake3().hexdigest()',
            r'hashlib\.sha256\(([^)]+)\)\.hexdigest\(\)': r'blake3.blake3(\1).hexdigest()',
            
            # å¯¼å…¥è¯­å¥
            r'import hashlib': 'import blake3\nimport hashlib  # ä¿ç•™ç”¨äºå…¼å®¹æ€§',
        }
        
        logger.info("BLAKE3è¿ç§»å·¥å…·åˆå§‹åŒ–", project_root=str(self.project_root))
    
    def analyze_current_usage(self) -> Dict[str, Any]:
        """åˆ†æå½“å‰çš„å“ˆå¸Œä½¿ç”¨æƒ…å†µ"""
        logger.info("å¼€å§‹åˆ†æå½“å‰å“ˆå¸Œä½¿ç”¨æƒ…å†µ")
        
        analysis = {
            "files_with_hash": [],
            "hash_usage_count": {
                "md5": 0,
                "sha256": 0,
                "other": 0
            },
            "potential_replacements": []
        }
        
        for pattern in self.file_patterns:
            for file_path in self.project_root.glob(pattern):
                if self._should_skip_file(file_path):
                    continue
                
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    # æ£€æŸ¥å“ˆå¸Œä½¿ç”¨
                    hash_matches = self._find_hash_usage(content)
                    if hash_matches:
                        analysis["files_with_hash"].append({
                            "file": str(file_path),
                            "matches": hash_matches
                        })
                        
                        # ç»Ÿè®¡ä½¿ç”¨æ¬¡æ•°
                        for match_type, matches in hash_matches.items():
                            analysis["hash_usage_count"][match_type] += len(matches)
                    
                except Exception as e:
                    logger.warning("æ–‡ä»¶åˆ†æå¤±è´¥", file_path=str(file_path), error=str(e))
        
        logger.info("å“ˆå¸Œä½¿ç”¨æƒ…å†µåˆ†æå®Œæˆ", 
                   files_found=len(analysis["files_with_hash"]),
                   total_md5=analysis["hash_usage_count"]["md5"],
                   total_sha256=analysis["hash_usage_count"]["sha256"])
        
        return analysis
    
    def _find_hash_usage(self, content: str) -> Dict[str, List[str]]:
        """æŸ¥æ‰¾å†…å®¹ä¸­çš„å“ˆå¸Œä½¿ç”¨"""
        matches = {
            "md5": [],
            "sha256": [],
            "other": []
        }
        
        # MD5æ¨¡å¼
        md5_patterns = [
            r'hashlib\.md5\([^)]*\)',
            r'\.md5\(\)',
            r'md5\s*=',
            r'MD5\s*=',
        ]
        
        # SHA256æ¨¡å¼
        sha256_patterns = [
            r'hashlib\.sha256\([^)]*\)',
            r'\.sha256\(\)',
            r'sha256\s*=',
            r'SHA256\s*=',
        ]
        
        for pattern in md5_patterns:
            found = re.findall(pattern, content, re.IGNORECASE)
            matches["md5"].extend(found)
        
        for pattern in sha256_patterns:
            found = re.findall(pattern, content, re.IGNORECASE)
            matches["sha256"].extend(found)
        
        return {k: v for k, v in matches.items() if v}
    
    def _should_skip_file(self, file_path: Path) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥è·³è¿‡æ–‡ä»¶"""
        skip_patterns = [
            "__pycache__",
            ".git",
            ".pytest_cache",
            "node_modules",
            "venv",
            ".venv",
            "migrations",  # æ•°æ®åº“è¿ç§»æ–‡ä»¶é€šå¸¸ä¸éœ€è¦ä¿®æ”¹
            "test_blake3_content_addressing.py",  # æˆ‘ä»¬åˆšåˆ›å»ºçš„æµ‹è¯•æ–‡ä»¶
            "migrate_to_blake3.py",  # æœ¬æ–‡ä»¶
        ]
        
        path_str = str(file_path)
        return any(pattern in path_str for pattern in skip_patterns)
    
    def perform_migration(self, dry_run: bool = True) -> Dict[str, Any]:
        """æ‰§è¡ŒBLAKE3è¿ç§»"""
        logger.info("å¼€å§‹æ‰§è¡ŒBLAKE3è¿ç§»", dry_run=dry_run)
        
        migration_results = {
            "modified_files": [],
            "total_replacements": 0,
            "errors": []
        }
        
        for pattern in self.file_patterns:
            for file_path in self.project_root.glob(pattern):
                if self._should_skip_file(file_path):
                    continue
                
                try:
                    result = self._migrate_file(file_path, dry_run)
                    if result["replacements"] > 0:
                        migration_results["modified_files"].append(result)
                        migration_results["total_replacements"] += result["replacements"]
                    
                except Exception as e:
                    error_info = {
                        "file": str(file_path),
                        "error": str(e)
                    }
                    migration_results["errors"].append(error_info)
                    logger.error("æ–‡ä»¶è¿ç§»å¤±è´¥", **error_info)
        
        logger.info("BLAKE3è¿ç§»å®Œæˆ",
                   modified_files=len(migration_results["modified_files"]),
                   total_replacements=migration_results["total_replacements"],
                   errors=len(migration_results["errors"]),
                   dry_run=dry_run)
        
        return migration_results
    
    def _migrate_file(self, file_path: Path, dry_run: bool = True) -> Dict[str, Any]:
        """è¿ç§»å•ä¸ªæ–‡ä»¶"""
        with open(file_path, 'r', encoding='utf-8') as f:
            original_content = f.read()
        
        modified_content = original_content
        replacements = 0
        
        # åº”ç”¨æ‰€æœ‰æ›¿æ¢æ¨¡å¼
        for old_pattern, new_pattern in self.hash_patterns.items():
            new_content, count = re.subn(old_pattern, new_pattern, modified_content)
            if count > 0:
                modified_content = new_content
                replacements += count
                logger.debug("åº”ç”¨æ›¿æ¢æ¨¡å¼",
                           file=str(file_path),
                           pattern=old_pattern,
                           replacements=count)
        
        # å¦‚æœæœ‰ä¿®æ”¹ä¸”ä¸æ˜¯å¹²è¿è¡Œï¼Œå†™å…¥æ–‡ä»¶
        if replacements > 0 and not dry_run:
            # å¤‡ä»½åŸæ–‡ä»¶
            backup_path = file_path.with_suffix(file_path.suffix + '.backup')
            if not backup_path.exists():
                with open(backup_path, 'w', encoding='utf-8') as f:
                    f.write(original_content)
            
            # å†™å…¥ä¿®æ”¹åçš„å†…å®¹
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(modified_content)
            
            logger.info("æ–‡ä»¶å·²è¿ç§»", 
                       file=str(file_path),
                       replacements=replacements,
                       backup=str(backup_path))
        
        return {
            "file": str(file_path),
            "replacements": replacements,
            "backup": str(backup_path) if replacements > 0 and not dry_run else None
        }
    
    def benchmark_performance(self) -> Dict[str, float]:
        """åŸºå‡†æµ‹è¯•ï¼šæ¯”è¾ƒè¿ç§»å‰åçš„æ€§èƒ½"""
        logger.info("å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•")
        
        # æµ‹è¯•ä¸åŒå¤§å°çš„æ•°æ®
        test_cases = [
            (b"small content", "å°æ•°æ®"),
            (b"x" * 1024, "1KBæ•°æ®"),
            (b"x" * 10240, "10KBæ•°æ®"),
            (b"x" * 102400, "100KBæ•°æ®"),
        ]
        
        results = {}
        
        for data, description in test_cases:
            logger.info(f"æµ‹è¯• {description}")
            benchmark_result = benchmark_hash_algorithms(data, iterations=1000)
            results[description] = benchmark_result
            
            # æ˜¾ç¤ºç»“æœ
            blake3_time = benchmark_result.get("blake3", 0)
            md5_time = benchmark_result.get("md5", 0)
            sha256_time = benchmark_result.get("sha256", 0)
            
            if md5_time > 0:
                md5_improvement = md5_time / blake3_time
                logger.info(f"{description} - BLAKE3æ¯”MD5å¿« {md5_improvement:.2f}å€")
            
            if sha256_time > 0:
                sha256_improvement = sha256_time / blake3_time
                logger.info(f"{description} - BLAKE3æ¯”SHA256å¿« {sha256_improvement:.2f}å€")
        
        return results
    
    def create_migration_report(self, analysis: Dict[str, Any], migration_results: Dict[str, Any]) -> str:
        """åˆ›å»ºè¿ç§»æŠ¥å‘Š"""
        report = [
            "# BLAKE3è¿ç§»æŠ¥å‘Š",
            f"ç”Ÿæˆæ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}",
            "",
            "## è¿ç§»å‰åˆ†æ",
            f"- å‘ç°åŒ…å«å“ˆå¸Œçš„æ–‡ä»¶: {len(analysis['files_with_hash'])} ä¸ª",
            f"- MD5ä½¿ç”¨æ¬¡æ•°: {analysis['hash_usage_count']['md5']}",
            f"- SHA256ä½¿ç”¨æ¬¡æ•°: {analysis['hash_usage_count']['sha256']}",
            "",
            "## è¿ç§»ç»“æœ",
            f"- ä¿®æ”¹çš„æ–‡ä»¶: {len(migration_results['modified_files'])} ä¸ª",
            f"- æ€»æ›¿æ¢æ¬¡æ•°: {migration_results['total_replacements']}",
            f"- å‘ç”Ÿé”™è¯¯: {len(migration_results['errors'])} ä¸ª",
            "",
        ]
        
        if migration_results['modified_files']:
            report.append("## ä¿®æ”¹çš„æ–‡ä»¶")
            for file_info in migration_results['modified_files']:
                report.append(f"- {file_info['file']}: {file_info['replacements']} æ¬¡æ›¿æ¢")
            report.append("")
        
        if migration_results['errors']:
            report.append("## é”™è¯¯ä¿¡æ¯")
            for error_info in migration_results['errors']:
                report.append(f"- {error_info['file']}: {error_info['error']}")
            report.append("")
        
        report.extend([
            "## BLAKE3ä¼˜åŠ¿",
            "1. **æ€§èƒ½**: åœ¨å¤§å¤šæ•°åœºæ™¯ä¸‹æ¯”MD5å’ŒSHA256æ›´å¿«",
            "2. **å®‰å…¨æ€§**: å¯†ç å­¦å®‰å…¨ï¼ŒæŠ—ç¢°æ’æ”»å‡»",
            "3. **å¹¶è¡ŒåŒ–**: æ”¯æŒSIMDå’Œå¤šçº¿ç¨‹ä¼˜åŒ–",
            "4. **ä¸€è‡´æ€§**: æ— é•¿åº¦æ‰©å±•æ”»å‡»",
            "",
            "## æ³¨æ„äº‹é¡¹",
            "1. ç¡®ä¿æ‰€æœ‰ä¾èµ–çš„ç³»ç»Ÿéƒ½æ”¯æŒBLAKE3",
            "2. å¦‚æœéœ€è¦ä¸å¤–éƒ¨ç³»ç»Ÿå…¼å®¹ï¼Œå¯èƒ½éœ€è¦ä¿ç•™åŸæœ‰å“ˆå¸Œç®—æ³•",
            "3. æ•°æ®åº“ä¸­å­˜å‚¨çš„å“ˆå¸Œå€¼éœ€è¦é‡æ–°è®¡ç®—",
            "4. ç¼“å­˜æ–‡ä»¶å¯èƒ½éœ€è¦æ¸…ç†é‡å»º",
        ])
        
        return "\\n".join(report)
    
    def verify_migration(self) -> bool:
        """éªŒè¯è¿ç§»ç»“æœ"""
        logger.info("å¼€å§‹éªŒè¯è¿ç§»ç»“æœ")
        
        try:
            # æµ‹è¯•BLAKE3åŠŸèƒ½
            cas = ContentAddressingSystem(HashAlgorithm.BLAKE3)
            test_content = "è¿ç§»éªŒè¯æµ‹è¯•å†…å®¹"
            cid = cas.compute_cid(test_content)
            
            if cid.algorithm == HashAlgorithm.BLAKE3:
                logger.info("âœ“ BLAKE3å†…å®¹å¯»å€ç³»ç»Ÿå·¥ä½œæ­£å¸¸")
            else:
                logger.error("âœ— BLAKE3å†…å®¹å¯»å€ç³»ç»Ÿå¼‚å¸¸")
                return False
            
            # æµ‹è¯•BLAKE3ç¼“å­˜
            import tempfile
            with tempfile.TemporaryDirectory() as temp_dir:
                cache = Blake3DiskCache(cache_dir=temp_dir, max_size_mb=1)
                
                # è¿™é‡Œéœ€è¦ç­‰å¾…åˆå§‹åŒ–å®Œæˆ
                import asyncio
                async def test_cache():
                    await cache.initialize()
                    success = await cache.put("test_key", {"test": "data"})
                    if success:
                        data = await cache.get("test_key")
                        if data and data.get("test") == "data":
                            logger.info("âœ“ BLAKE3ç£ç›˜ç¼“å­˜å·¥ä½œæ­£å¸¸")
                            await cache.shutdown()
                            return True
                    await cache.shutdown()
                    return False
                
                if asyncio.run(test_cache()):
                    logger.info("è¿ç§»éªŒè¯å®Œæˆ: æ‰€æœ‰BLAKE3åŠŸèƒ½æ­£å¸¸")
                    return True
                else:
                    logger.error("è¿ç§»éªŒè¯å¤±è´¥: BLAKE3ç£ç›˜ç¼“å­˜å¼‚å¸¸")
                    return False
        
        except Exception as e:
            logger.error("è¿ç§»éªŒè¯å¼‚å¸¸", error=str(e))
            return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ BLAKE3è¿ç§»å·¥å…·")
    print("=" * 50)
    
    # åˆ›å»ºè¿ç§»å·¥å…·
    migration_tool = Blake3MigrationTool()
    
    # æ­¥éª¤1: åˆ†æå½“å‰ä½¿ç”¨æƒ…å†µ
    print("ğŸ“Š åˆ†æå½“å‰å“ˆå¸Œä½¿ç”¨æƒ…å†µ...")
    analysis = migration_tool.analyze_current_usage()
    
    # æ­¥éª¤2: æ€§èƒ½åŸºå‡†æµ‹è¯•
    print("âš¡ æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
    benchmark_results = migration_tool.benchmark_performance()
    
    # æ­¥éª¤3: å¹²è¿è¡Œè¿ç§»ï¼ˆé¢„è§ˆï¼‰
    print("ğŸ” æ‰§è¡Œè¿ç§»é¢„è§ˆï¼ˆå¹²è¿è¡Œï¼‰...")
    dry_run_results = migration_tool.perform_migration(dry_run=True)
    
    # æ­¥éª¤4: ç”ŸæˆæŠ¥å‘Š
    report = migration_tool.create_migration_report(analysis, dry_run_results)
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = Path("blake3_migration_report.md")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ“„ è¿ç§»æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
    print("\\n" + "=" * 50)
    print("è¿ç§»é¢„è§ˆå®Œæˆ!")
    print(f"å‘ç° {len(analysis['files_with_hash'])} ä¸ªåŒ…å«å“ˆå¸Œçš„æ–‡ä»¶")
    print(f"é¢„è®¡ä¿®æ”¹ {len(dry_run_results['modified_files'])} ä¸ªæ–‡ä»¶")
    print(f"æ€»å…± {dry_run_results['total_replacements']} æ¬¡æ›¿æ¢")
    
    if dry_run_results['errors']:
        print(f"âš ï¸  å‘ç° {len(dry_run_results['errors'])} ä¸ªæ½œåœ¨é—®é¢˜")
    
    print("\\nè¦æ‰§è¡Œå®é™…è¿ç§»ï¼Œè¯·è¿è¡Œ:")
    print("python tools/migrate_to_blake3.py --execute")
    
    return 0


if __name__ == "__main__":
    import sys
    if "--execute" in sys.argv:
        print("âš ï¸  æ‰§è¡Œå®é™…è¿ç§»åŠŸèƒ½æš‚æœªå®ç°")
        print("è¯·æ‰‹åŠ¨æ£€æŸ¥è¿ç§»æŠ¥å‘Šåè°¨æ…æ‰§è¡Œ")
    else:
        exit(main())