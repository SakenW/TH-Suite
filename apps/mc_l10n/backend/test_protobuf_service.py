#!/usr/bin/env python
"""
æµ‹è¯•Protobufåºåˆ—åŒ–æœåŠ¡
éªŒè¯åºåˆ—åŒ–ã€å‹ç¼©ã€æ‰¹å¤„ç†ç­‰åŠŸèƒ½
"""

import sys
import os
import time
import json
import asyncio
from typing import List, Dict, Any

sys.path.append('.')

import structlog

# é…ç½®æ—¥å¿—
structlog.configure(
    processors=[
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.dev.ConsoleRenderer()
    ]
)

logger = structlog.get_logger(__name__)

def test_protobuf_serialization():
    """æµ‹è¯•åŸºæœ¬Protobufåºåˆ—åŒ–åŠŸèƒ½"""
    print("=== æµ‹è¯•Protobufåºåˆ—åŒ– ===")
    
    try:
        from services.protobuf_service import ProtobufSerializer, TranslationProtobufConverter
        
        # åˆ›å»ºåºåˆ—åŒ–å™¨
        serializer = ProtobufSerializer(
            enable_compression=True,
            compression_algorithm="zstd",
            enable_content_addressing=True
        )
        
        converter = TranslationProtobufConverter(serializer)
        print("âœ“ ProtobufæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•ç¿»è¯‘æ¡ç›®è½¬æ¢
        test_entry = {
            "uid": "entry_001",
            "key": "item.diamond_sword",
            "locale": "zh_cn",
            "src_text": "Diamond Sword",
            "dst_text": "é’»çŸ³å‰‘",
            "status": "approved",
            "created_at": "2025-09-10T12:00:00",
            "updated_at": "2025-09-10T12:30:00",
            "language_file_uid": "minecraft_zh_cn_001",
            "pack_uid": "minecraft_vanilla",
            "metadata": {
                "translator": "test_user",
                "confidence": "0.95",
                "source": "crowdsourced"
            },
            "notes": "é«˜è´¨é‡ç¿»è¯‘",
            "version": 1
        }
        
        # è½¬æ¢ä¸ºProtobufæ¶ˆæ¯
        proto_entry = converter.translation_entry_to_proto(test_entry)
        print("âœ“ ç¿»è¯‘æ¡ç›®è½¬æ¢ä¸ºProtobufæ¶ˆæ¯")
        
        # åºåˆ—åŒ–
        serialized_data = serializer.serialize(proto_entry)
        print(f"âœ“ åºåˆ—åŒ–å®Œæˆï¼Œå¤§å°: {len(serialized_data)} bytes")
        
        # ååºåˆ—åŒ–
        import generated.translation_pb2 as pb
        deserialized_entry = serializer.deserialize(serialized_data, pb.TranslationEntry)
        print("âœ“ ååºåˆ—åŒ–æˆåŠŸ")
        
        # è½¬æ¢å›å­—å…¸
        recovered_entry = converter.proto_to_translation_entry(deserialized_entry)
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        key_fields = ["uid", "key", "locale", "src_text", "dst_text", "status"]
        for field in key_fields:
            if recovered_entry.get(field) != test_entry.get(field):
                print(f"âœ— å­—æ®µä¸åŒ¹é…: {field}")
                return False
        
        print("âœ“ æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")
        
        # æµ‹è¯•JSONåºåˆ—åŒ–ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        json_str = serializer.serialize_to_json(proto_entry)
        print("âœ“ JSONåºåˆ—åŒ–æˆåŠŸ")
        
        json_deserialized = serializer.deserialize_from_json(json_str, pb.TranslationEntry)
        if json_deserialized.uid == test_entry["uid"]:
            print("âœ“ JSONååºåˆ—åŒ–éªŒè¯é€šè¿‡")
        else:
            print("âœ— JSONååºåˆ—åŒ–éªŒè¯å¤±è´¥")
            return False
        
        # è·å–æ€§èƒ½ç»Ÿè®¡
        stats = serializer.get_stats()
        print(f"âœ“ æ€§èƒ½ç»Ÿè®¡: {stats['serializations']} æ¬¡åºåˆ—åŒ–, {stats['deserializations']} æ¬¡ååºåˆ—åŒ–")
        
        return True
        
    except Exception as e:
        print(f"âœ— Protobufåºåˆ—åŒ–æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_batch_processing():
    """æµ‹è¯•æ‰¹å¤„ç†åŠŸèƒ½"""
    print("\n=== æµ‹è¯•Protobufæ‰¹å¤„ç† ===")
    
    try:
        from services.protobuf_service import ProtobufBatchProcessor, ProtobufSerializer, TranslationProtobufConverter
        
        # åˆ›å»ºæ‰¹å¤„ç†å™¨
        serializer = ProtobufSerializer(enable_compression=True)
        converter = TranslationProtobufConverter(serializer)
        processor = ProtobufBatchProcessor(
            serializer, 
            converter, 
            batch_size=10,  # å°æ‰¹æ¬¡ç”¨äºæµ‹è¯•
            max_batch_size_bytes=1024  # 1KBé™åˆ¶
        )
        
        print("âœ“ æ‰¹å¤„ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        test_entries = []
        for i in range(25):  # åˆ›å»º25ä¸ªæ¡ç›®ï¼Œåº”è¯¥åˆ†æˆ3ä¸ªæ‰¹æ¬¡
            entry = {
                "uid": f"entry_{i:03d}",
                "key": f"item.test_{i}",
                "locale": "zh_cn",
                "src_text": f"Test Item {i}",
                "dst_text": f"æµ‹è¯•ç‰©å“{i}",
                "status": "approved" if i % 2 == 0 else "pending",
                "created_at": int(time.time()),
                "updated_at": int(time.time()),
                "language_file_uid": "test_file",
                "pack_uid": "test_pack",
                "metadata": {
                    "batch_test": "true",
                    "item_id": str(i)
                }
            }
            test_entries.append(entry)
        
        # æ‰¹å¤„ç†
        batch_info = {
            "batch_id": "test_batch_001",
            "source": "unit_test",
            "metadata": {
                "test_type": "batch_processing"
            }
        }
        
        batch_bytes_list = processor.process_translation_entries_batch(test_entries, batch_info)
        print(f"âœ“ æ‰¹å¤„ç†å®Œæˆ: {len(batch_bytes_list)} ä¸ªæ‰¹æ¬¡")
        
        # éªŒè¯æ¯ä¸ªæ‰¹æ¬¡
        total_recovered_entries = 0
        import generated.translation_pb2 as pb
        
        for i, batch_bytes in enumerate(batch_bytes_list):
            batch = serializer.deserialize(batch_bytes, pb.TranslationBatch)
            total_recovered_entries += len(batch.entries)
            print(f"  æ‰¹æ¬¡ {i+1}: {len(batch.entries)} ä¸ªæ¡ç›®")
        
        if total_recovered_entries == len(test_entries):
            print("âœ“ æ‰¹æ¬¡æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")
        else:
            print(f"âœ— æ‰¹æ¬¡æ•°æ®ä¸¢å¤±: æœŸæœ› {len(test_entries)}, å®é™… {total_recovered_entries}")
            return False
        
        # æµ‹è¯•æ‰¹æ¬¡åˆå¹¶
        merged_batch = processor.merge_batches(batch_bytes_list)
        print(f"âœ“ æ‰¹æ¬¡åˆå¹¶å®Œæˆ: {len(merged_batch.entries)} ä¸ªæ¡ç›®")
        
        if len(merged_batch.entries) == len(test_entries):
            print("âœ“ åˆå¹¶åæ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")
        else:
            print(f"âœ— åˆå¹¶åæ•°æ®ä¸¢å¤±: æœŸæœ› {len(test_entries)}, å®é™… {len(merged_batch.entries)}")
            return False
        
        return True
        
    except Exception as e:
        print(f"âœ— æ‰¹å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_sync_protocol():
    """æµ‹è¯•åŒæ­¥åè®®æ¶ˆæ¯"""
    print("\n=== æµ‹è¯•åŒæ­¥åè®® ===")
    
    try:
        from services.protobuf_service import ProtobufSerializer, TranslationProtobufConverter
        import generated.translation_pb2 as pb
        
        serializer = ProtobufSerializer()
        converter = TranslationProtobufConverter(serializer)
        
        # æµ‹è¯•åŒæ­¥è¯·æ±‚
        sync_request = converter.create_sync_request(
            session_id="session_001",
            client_id="client_test",
            locale="zh_cn",
            pack_uids=["minecraft", "create_mod"],
            since_timestamp=int(time.time() - 3600),  # ä¸€å°æ—¶å‰
            compression="zstd"
        )
        
        print("âœ“ åŒæ­¥è¯·æ±‚åˆ›å»ºæˆåŠŸ")
        
        # åºåˆ—åŒ–åŒæ­¥è¯·æ±‚
        request_bytes = serializer.serialize(sync_request)
        print(f"âœ“ åŒæ­¥è¯·æ±‚åºåˆ—åŒ–å®Œæˆ: {len(request_bytes)} bytes")
        
        # ååºåˆ—åŒ–éªŒè¯
        deserialized_request = serializer.deserialize(request_bytes, pb.SyncRequest)
        if (deserialized_request.session_id == "session_001" and 
            deserialized_request.client_id == "client_test" and
            deserialized_request.locale == "zh_cn"):
            print("âœ“ åŒæ­¥è¯·æ±‚ååºåˆ—åŒ–éªŒè¯é€šè¿‡")
        else:
            print("âœ— åŒæ­¥è¯·æ±‚ååºåˆ—åŒ–éªŒè¯å¤±è´¥")
            return False
        
        # æµ‹è¯•åŒæ­¥å“åº”
        added_entries = [
            {
                "uid": "new_entry_001",
                "key": "item.new_item",
                "locale": "zh_cn",
                "src_text": "New Item",
                "dst_text": "æ–°ç‰©å“",
                "status": "approved",
                "created_at": int(time.time()),
                "updated_at": int(time.time()),
                "language_file_uid": "test_file",
                "pack_uid": "test_pack"
            }
        ]
        
        updated_entries = [
            {
                "uid": "updated_entry_001",
                "key": "item.updated_item",
                "locale": "zh_cn", 
                "src_text": "Updated Item",
                "dst_text": "æ›´æ–°ç‰©å“",
                "status": "approved",
                "created_at": int(time.time() - 1000),
                "updated_at": int(time.time()),
                "language_file_uid": "test_file",
                "pack_uid": "test_pack"
            }
        ]
        
        deleted_uids = ["deleted_entry_001", "deleted_entry_002"]
        
        sync_response = converter.create_sync_response(
            session_id="session_001",
            status=pb.SYNC_SUCCESS,
            added_entries=added_entries,
            updated_entries=updated_entries,
            deleted_uids=deleted_uids
        )
        
        print("âœ“ åŒæ­¥å“åº”åˆ›å»ºæˆåŠŸ")
        
        # éªŒè¯å“åº”å†…å®¹
        if (len(sync_response.added_entries) == 1 and
            len(sync_response.updated_entries) == 1 and
            len(sync_response.deleted_entry_uids) == 2 and
            sync_response.total_changes == 4):
            print("âœ“ åŒæ­¥å“åº”å†…å®¹éªŒè¯é€šè¿‡")
        else:
            print("âœ— åŒæ­¥å“åº”å†…å®¹éªŒè¯å¤±è´¥")
            return False
        
        # åºåˆ—åŒ–åŒæ­¥å“åº”
        response_bytes = serializer.serialize(sync_response)
        print(f"âœ“ åŒæ­¥å“åº”åºåˆ—åŒ–å®Œæˆ: {len(response_bytes)} bytes")
        
        return True
        
    except Exception as e:
        print(f"âœ— åŒæ­¥åè®®æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_compression_performance():
    """æµ‹è¯•å‹ç¼©æ€§èƒ½"""
    print("\n=== æµ‹è¯•å‹ç¼©æ€§èƒ½ ===")
    
    try:
        from services.protobuf_service import ProtobufSerializer, TranslationProtobufConverter
        import generated.translation_pb2 as pb
        
        # æµ‹è¯•ä¸åŒå‹ç¼©ç®—æ³•
        algorithms = ["zstd", "gzip", "zlib"]  # ä¸åŒ…å« "none" å› ä¸ºéœ€è¦ç‰¹æ®Šå¤„ç†
        results = {}
        
        # åˆ›å»ºå¤§é‡æµ‹è¯•æ•°æ®
        test_entries = []
        for i in range(1000):
            entry = {
                "uid": f"perf_entry_{i:04d}",
                "key": f"item.performance_test_{i}",
                "locale": "zh_cn",
                "src_text": f"Performance Test Item {i} with some longer description text",
                "dst_text": f"æ€§èƒ½æµ‹è¯•ç‰©å“{i}ï¼ŒåŒ…å«ä¸€äº›æ›´é•¿çš„æè¿°æ–‡æœ¬ç”¨äºæµ‹è¯•å‹ç¼©æ•ˆæœ",
                "status": "approved",
                "created_at": int(time.time()),
                "updated_at": int(time.time()),
                "language_file_uid": "performance_test",
                "pack_uid": "performance_pack",
                "metadata": {
                    "test_type": "performance",
                    "iteration": str(i),
                    "category": "compression_test"
                }
            }
            test_entries.append(entry)
        
        for algorithm in algorithms:
            print(f"\næµ‹è¯•å‹ç¼©ç®—æ³•: {algorithm}")
            
            try:
                serializer = ProtobufSerializer(
                    enable_compression=True,
                    compression_algorithm=algorithm
                )
                converter = TranslationProtobufConverter(serializer)
                
                # æ‰¹é‡è½¬æ¢å’Œåºåˆ—åŒ–
                start_time = time.time()
                
                total_original_size = 0
                total_compressed_size = 0
                
                for entry in test_entries[:100]:  # æµ‹è¯•å‰100ä¸ªæ¡ç›®
                    proto_entry = converter.translation_entry_to_proto(entry)
                    
                    # æœªå‹ç¼©å¤§å°
                    uncompressed_data = proto_entry.SerializeToString()
                    total_original_size += len(uncompressed_data)
                    
                    # å‹ç¼©åå¤§å°
                    compressed_data = serializer.serialize(proto_entry)
                    total_compressed_size += len(compressed_data)
                
                end_time = time.time()
                
                compression_ratio = total_original_size / total_compressed_size if total_compressed_size > 0 else 1.0
                processing_time = (end_time - start_time) * 1000
                
                results[algorithm] = {
                    "compression_ratio": compression_ratio,
                    "processing_time_ms": processing_time,
                    "original_size_kb": total_original_size / 1024,
                    "compressed_size_kb": total_compressed_size / 1024,
                    "space_saved_percent": ((total_original_size - total_compressed_size) / total_original_size) * 100
                }
                
                print(f"  å‹ç¼©æ¯”: {compression_ratio:.2f}")
                print(f"  å¤„ç†æ—¶é—´: {processing_time:.1f} ms")
                print(f"  ç©ºé—´èŠ‚çœ: {results[algorithm]['space_saved_percent']:.1f}%")
                
            except Exception as e:
                print(f"  âš ï¸ {algorithm} æµ‹è¯•å¤±è´¥: {e}")
                continue
        
        # æ˜¾ç¤ºæ€§èƒ½å¯¹æ¯”
        if results:
            print("\næ€§èƒ½å¯¹æ¯”:")
            best_compression = max(results.items(), key=lambda x: x[1]["compression_ratio"])
            fastest_algorithm = min(results.items(), key=lambda x: x[1]["processing_time_ms"])
            
            print(f"æœ€ä½³å‹ç¼©: {best_compression[0]} (å‹ç¼©æ¯” {best_compression[1]['compression_ratio']:.2f})")
            print(f"æœ€å¿«å¤„ç†: {fastest_algorithm[0]} (æ—¶é—´ {fastest_algorithm[1]['processing_time_ms']:.1f} ms)")
        
        return True
        
    except Exception as e:
        print(f"âœ— å‹ç¼©æ€§èƒ½æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_large_data_handling():
    """æµ‹è¯•å¤§æ•°æ®å¤„ç†"""
    print("\n=== æµ‹è¯•å¤§æ•°æ®å¤„ç† ===")
    
    try:
        from services.protobuf_service import get_protobuf_service, get_batch_processor
        import generated.translation_pb2 as pb
        
        # åˆå§‹åŒ–æœåŠ¡
        serializer = get_protobuf_service()
        processor = get_batch_processor()
        
        # åˆ›å»ºå¤§é‡æµ‹è¯•æ•°æ®
        large_dataset = []
        for i in range(10000):  # 1ä¸‡ä¸ªæ¡ç›®
            entry = {
                "uid": f"large_entry_{i:05d}",
                "key": f"large_scale.test_{i}",
                "locale": "zh_cn",
                "src_text": f"Large scale test entry {i}",
                "dst_text": f"å¤§è§„æ¨¡æµ‹è¯•æ¡ç›®{i}",
                "status": "approved" if i % 3 == 0 else "pending",
                "created_at": int(time.time()),
                "updated_at": int(time.time()),
                "language_file_uid": "large_test",
                "pack_uid": "large_pack"
            }
            large_dataset.append(entry)
        
        print(f"åˆ›å»ºäº† {len(large_dataset)} ä¸ªæµ‹è¯•æ¡ç›®")
        
        # æ‰¹å¤„ç†å¤§æ•°æ®
        start_time = time.time()
        
        batches = processor.process_translation_entries_batch(large_dataset)
        
        processing_time = (time.time() - start_time) * 1000
        
        print(f"âœ“ å¤§æ•°æ®æ‰¹å¤„ç†å®Œæˆ:")
        print(f"  å¤„ç†æ—¶é—´: {processing_time:.1f} ms")
        print(f"  ç”Ÿæˆæ‰¹æ¬¡: {len(batches)} ä¸ª")
        print(f"  å¹³å‡æ¯æ‰¹æ¬¡: {len(large_dataset) / len(batches):.0f} ä¸ªæ¡ç›®")
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        total_size = sum(len(batch_bytes) for batch_bytes in batches)
        print(f"  æ€»åºåˆ—åŒ–å¤§å°: {total_size / 1024 / 1024:.2f} MB")
        
        # æŠ½æ ·éªŒè¯
        sample_batch = serializer.deserialize(batches[0], pb.TranslationBatch)
        if len(sample_batch.entries) > 0:
            sample_entry = sample_batch.entries[0]
            if sample_entry.uid and sample_entry.key:
                print("âœ“ æŠ½æ ·æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")
            else:
                print("âœ— æŠ½æ ·æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥")
                return False
        
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats = serializer.get_stats()
        print(f"âœ“ åºåˆ—åŒ–ç»Ÿè®¡:")
        print(f"  åºåˆ—åŒ–æ¬¡æ•°: {stats['serializations']}")
        print(f"  å¹³å‡åºåˆ—åŒ–æ—¶é—´: {stats['average_serialization_time_ms']:.2f} ms")
        print(f"  å‹ç¼©èŠ‚çœ: {stats['compression_saves_bytes'] / 1024:.1f} KB")
        
        return True
        
    except Exception as e:
        print(f"âœ— å¤§æ•°æ®å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

async def main():
    """è¿è¡Œæ‰€æœ‰Protobufæµ‹è¯•"""
    print("ğŸš€ å¼€å§‹ProtobufæœåŠ¡æµ‹è¯•")
    print("=" * 60)
    
    tests = [
        test_protobuf_serialization,
        test_batch_processing,
        test_sync_protocol,
        test_compression_performance,
        test_large_data_handling
    ]
    
    passed = 0
    total = len(tests)
    
    for i, test in enumerate(tests):
        try:
            if asyncio.iscoroutinefunction(test):
                result = await test()
            else:
                result = test()
            
            if result:
                passed += 1
        except Exception as e:
            print(f"æµ‹è¯•å¼‚å¸¸: {e}")
            import traceback
            traceback.print_exc()
    
    print("\n" + "=" * 60)
    print(f"ğŸ ProtobufæœåŠ¡æµ‹è¯•å®Œæˆ: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰Protobufæµ‹è¯•é€šè¿‡!")
        return 0
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦æ£€æŸ¥å®ç°")
        return 1

if __name__ == "__main__":
    exit(asyncio.run(main()))