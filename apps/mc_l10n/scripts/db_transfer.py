#!/usr/bin/env python3
"""
æ•°æ®åº“ä¼ è¾“å·¥å…·
ç”¨äºå¯¼å…¥ã€å¯¼å‡ºå’Œè¿ç§»æ•°æ®åº“æ•°æ®
"""

import sqlite3
import json
import csv
import sys
import os
from pathlib import Path
from typing import Dict, List, Any
import argparse
from datetime import datetime
import zipfile
import shutil


class DatabaseTransfer:
    """æ•°æ®åº“ä¼ è¾“å·¥å…·"""
    
    def __init__(self, db_path: str = None):
        """åˆå§‹åŒ–"""
        if db_path is None:
            db_path = Path(__file__).parent.parent / "backend" / "mc_l10n.db"
        
        self.db_path = Path(db_path)
        if not self.db_path.exists():
            raise FileNotFoundError(f"æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {db_path}")
        
        self.conn = sqlite3.connect(str(self.db_path))
        self.conn.row_factory = sqlite3.Row
        self.cursor = self.conn.cursor()
    
    def export_to_json(self, output_file: str = None):
        """å¯¼å‡ºæ•´ä¸ªæ•°æ®åº“åˆ°JSON"""
        if output_file is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"mc_l10n_export_{timestamp}.json"
        
        output_path = Path(output_file)
        
        print(f"ğŸ“¦ å¯¼å‡ºæ•°æ®åº“åˆ°JSON: {output_path}")
        
        data = {}
        
        # è·å–æ‰€æœ‰è¡¨
        self.cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
        tables = [row[0] for row in self.cursor.fetchall()]
        
        for table in tables:
            if table.startswith('sqlite_'):
                continue
            
            print(f"  å¯¼å‡ºè¡¨: {table}")
            self.cursor.execute(f"SELECT * FROM {table}")
            rows = self.cursor.fetchall()
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            data[table] = []
            for row in rows:
                data[table].append(dict(row))
        
        # å†™å…¥JSONæ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2, default=str)
        
        print(f"âœ… å¯¼å‡ºå®Œæˆ: {len(data)} ä¸ªè¡¨")
        print(f"   æ–‡ä»¶å¤§å°: {output_path.stat().st_size / 1024:.1f} KB")
        
        return str(output_path)
    
    def import_from_json(self, input_file: str, merge: bool = False):
        """ä»JSONå¯¼å…¥æ•°æ®"""
        input_path = Path(input_file)
        
        if not input_path.exists():
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")
        
        print(f"ğŸ“¥ ä»JSONå¯¼å…¥æ•°æ®: {input_path}")
        
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        if not merge:
            # æ¸…ç©ºç°æœ‰æ•°æ®
            print("âš ï¸ æ¸…ç©ºç°æœ‰æ•°æ®...")
            for table in data.keys():
                try:
                    self.cursor.execute(f"DELETE FROM {table}")
                except sqlite3.Error as e:
                    print(f"  è­¦å‘Š: æ— æ³•æ¸…ç©ºè¡¨ {table}: {e}")
        
        # å¯¼å…¥æ•°æ®
        for table, rows in data.items():
            print(f"  å¯¼å…¥è¡¨ {table}: {len(rows)} è¡Œ")
            
            if not rows:
                continue
            
            # è·å–åˆ—å
            columns = list(rows[0].keys())
            placeholders = ','.join(['?' for _ in columns])
            
            # æ’å…¥æ•°æ®
            for row in rows:
                values = [row.get(col) for col in columns]
                
                if merge:
                    # ä½¿ç”¨INSERT OR REPLACEè¿›è¡Œåˆå¹¶
                    query = f"INSERT OR REPLACE INTO {table} ({','.join(columns)}) VALUES ({placeholders})"
                else:
                    query = f"INSERT INTO {table} ({','.join(columns)}) VALUES ({placeholders})"
                
                try:
                    self.cursor.execute(query, values)
                except sqlite3.Error as e:
                    print(f"    é”™è¯¯: æ’å…¥å¤±è´¥ - {e}")
        
        self.conn.commit()
        print("âœ… å¯¼å…¥å®Œæˆ")
    
    def export_translations_csv(self, output_dir: str = None):
        """å¯¼å‡ºç¿»è¯‘åˆ°CSVæ–‡ä»¶ï¼ˆä¾¿äºç¼–è¾‘ï¼‰"""
        if output_dir is None:
            output_dir = Path.cwd() / "translations_export"
        else:
            output_dir = Path(output_dir)
        
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ“Š å¯¼å‡ºç¿»è¯‘åˆ°CSV: {output_dir}")
        
        # è·å–æ‰€æœ‰æ¨¡ç»„
        self.cursor.execute("""
            SELECT DISTINCT m.mod_id, m.display_name
            FROM mods m
            JOIN language_files lf ON m.mod_id = lf.mod_id
        """)
        
        mods = self.cursor.fetchall()
        
        for mod_id, display_name in mods:
            # åˆ›å»ºæ¨¡ç»„ç›®å½•
            mod_dir = output_dir / f"{mod_id}"
            mod_dir.mkdir(exist_ok=True)
            
            # è·å–è¯¥æ¨¡ç»„çš„æ‰€æœ‰è¯­è¨€
            self.cursor.execute("""
                SELECT DISTINCT locale_code
                FROM language_files
                WHERE mod_id = ?
            """, (mod_id,))
            
            locales = [row[0] for row in self.cursor.fetchall()]
            
            # è·å–æ‰€æœ‰ç¿»è¯‘é”®
            self.cursor.execute("""
                SELECT DISTINCT te.entry_key
                FROM translation_entries te
                JOIN language_files lf ON te.file_id = lf.file_id
                WHERE lf.mod_id = ?
                ORDER BY te.entry_key
            """, (mod_id,))
            
            keys = [row[0] for row in self.cursor.fetchall()]
            
            if not keys:
                continue
            
            # åˆ›å»ºCSVæ–‡ä»¶
            csv_file = mod_dir / f"{mod_id}_translations.csv"
            
            with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:
                writer = csv.writer(f)
                
                # å†™å…¥æ ‡é¢˜è¡Œ
                header = ['key'] + locales
                writer.writerow(header)
                
                # å†™å…¥ç¿»è¯‘æ•°æ®
                for key in keys:
                    row = [key]
                    
                    for locale in locales:
                        # è·å–è¯¥é”®åœ¨è¯¥è¯­è¨€çš„ç¿»è¯‘
                        self.cursor.execute("""
                            SELECT te.entry_value
                            FROM translation_entries te
                            JOIN language_files lf ON te.file_id = lf.file_id
                            WHERE lf.mod_id = ? AND lf.locale_code = ? AND te.entry_key = ?
                        """, (mod_id, locale, key))
                        
                        result = self.cursor.fetchone()
                        row.append(result[0] if result else '')
                    
                    writer.writerow(row)
            
            print(f"  å¯¼å‡º: {csv_file.name} ({len(keys)} é”®, {len(locales)} è¯­è¨€)")
        
        print(f"âœ… å¯¼å‡ºå®Œæˆ: {len(mods)} ä¸ªæ¨¡ç»„")
    
    def import_translations_csv(self, csv_file: str):
        """ä»CSVå¯¼å…¥ç¿»è¯‘"""
        csv_path = Path(csv_file)
        
        if not csv_path.exists():
            raise FileNotFoundError(f"CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_file}")
        
        print(f"ğŸ“ ä»CSVå¯¼å…¥ç¿»è¯‘: {csv_path}")
        
        # ä»æ–‡ä»¶åæå–æ¨¡ç»„ID
        mod_id = csv_path.stem.replace('_translations', '')
        
        with open(csv_path, 'r', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            
            # è·å–è¯­è¨€åˆ—è¡¨ï¼ˆé™¤äº†keyåˆ—ï¼‰
            locales = [col for col in reader.fieldnames if col != 'key']
            
            print(f"  æ¨¡ç»„ID: {mod_id}")
            print(f"  è¯­è¨€: {', '.join(locales)}")
            
            imported_count = 0
            
            for row in reader:
                key = row['key']
                
                for locale in locales:
                    value = row.get(locale, '')
                    
                    if not value:
                        continue
                    
                    # æŸ¥æ‰¾æˆ–åˆ›å»ºè¯­è¨€æ–‡ä»¶
                    self.cursor.execute("""
                        SELECT file_id FROM language_files
                        WHERE mod_id = ? AND locale_code = ?
                        LIMIT 1
                    """, (mod_id, locale))
                    
                    result = self.cursor.fetchone()
                    
                    if result:
                        file_id = result[0]
                        
                        # æ›´æ–°æˆ–æ’å…¥ç¿»è¯‘
                        self.cursor.execute("""
                            INSERT OR REPLACE INTO translation_entries
                            (entry_id, file_id, entry_key, entry_value)
                            VALUES (
                                (SELECT entry_id FROM translation_entries 
                                 WHERE file_id = ? AND entry_key = ?),
                                ?, ?, ?
                            )
                        """, (file_id, key, file_id, key, value))
                        
                        imported_count += 1
        
        self.conn.commit()
        print(f"âœ… å¯¼å…¥å®Œæˆ: {imported_count} ä¸ªç¿»è¯‘æ¡ç›®")
    
    def backup_database(self, backup_dir: str = None):
        """å¤‡ä»½æ•°æ®åº“"""
        if backup_dir is None:
            backup_dir = self.db_path.parent / "backups"
        else:
            backup_dir = Path(backup_dir)
        
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_file = backup_dir / f"mc_l10n_backup_{timestamp}.db"
        
        print(f"ğŸ’¾ å¤‡ä»½æ•°æ®åº“åˆ°: {backup_file}")
        
        # å¤åˆ¶æ•°æ®åº“æ–‡ä»¶
        shutil.copy2(self.db_path, backup_file)
        
        # åˆ›å»ºå‹ç¼©åŒ…
        zip_file = backup_dir / f"mc_l10n_backup_{timestamp}.zip"
        with zipfile.ZipFile(zip_file, 'w', zipfile.ZIP_DEFLATED) as zf:
            zf.write(backup_file, backup_file.name)
            
            # åŒæ—¶å¯¼å‡ºJSON
            json_file = self.export_to_json(backup_dir / f"mc_l10n_backup_{timestamp}.json")
            zf.write(json_file, Path(json_file).name)
        
        # åˆ é™¤ä¸´æ—¶æ–‡ä»¶
        backup_file.unlink()
        Path(json_file).unlink()
        
        print(f"âœ… å¤‡ä»½å®Œæˆ: {zip_file}")
        print(f"   æ–‡ä»¶å¤§å°: {zip_file.stat().st_size / 1024:.1f} KB")
        
        return str(zip_file)
    
    def restore_database(self, backup_file: str):
        """æ¢å¤æ•°æ®åº“"""
        backup_path = Path(backup_file)
        
        if not backup_path.exists():
            raise FileNotFoundError(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_file}")
        
        print(f"â™»ï¸ æ¢å¤æ•°æ®åº“ä»: {backup_path}")
        
        if backup_path.suffix == '.zip':
            # è§£å‹å¤‡ä»½
            temp_dir = backup_path.parent / "temp_restore"
            temp_dir.mkdir(exist_ok=True)
            
            with zipfile.ZipFile(backup_path, 'r') as zf:
                zf.extractall(temp_dir)
            
            # æŸ¥æ‰¾æ•°æ®åº“æ–‡ä»¶
            db_files = list(temp_dir.glob("*.db"))
            if db_files:
                # ä½¿ç”¨æ•°æ®åº“æ–‡ä»¶æ¢å¤
                print("  ä½¿ç”¨æ•°æ®åº“æ–‡ä»¶æ¢å¤...")
                self.conn.close()
                shutil.copy2(db_files[0], self.db_path)
                self.conn = sqlite3.connect(str(self.db_path))
                self.conn.row_factory = sqlite3.Row
                self.cursor = self.conn.cursor()
            else:
                # ä½¿ç”¨JSONæ–‡ä»¶æ¢å¤
                json_files = list(temp_dir.glob("*.json"))
                if json_files:
                    print("  ä½¿ç”¨JSONæ–‡ä»¶æ¢å¤...")
                    self.import_from_json(json_files[0], merge=False)
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            shutil.rmtree(temp_dir)
            
        elif backup_path.suffix == '.db':
            # ç›´æ¥ä½¿ç”¨æ•°æ®åº“æ–‡ä»¶
            self.conn.close()
            shutil.copy2(backup_path, self.db_path)
            self.conn = sqlite3.connect(str(self.db_path))
            self.conn.row_factory = sqlite3.Row
            self.cursor = self.conn.cursor()
            
        elif backup_path.suffix == '.json':
            # ä½¿ç”¨JSONæ–‡ä»¶
            self.import_from_json(backup_path, merge=False)
        
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„å¤‡ä»½æ–‡ä»¶æ ¼å¼: {backup_path.suffix}")
        
        print("âœ… æ¢å¤å®Œæˆ")
    
    def __del__(self):
        """æ¸…ç†èµ„æº"""
        if hasattr(self, 'conn'):
            self.conn.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='MC L10n æ•°æ®åº“ä¼ è¾“å·¥å…·')
    parser.add_argument('--db', type=str, help='æ•°æ®åº“æ–‡ä»¶è·¯å¾„')
    
    subparsers = parser.add_subparsers(dest='command', help='å¯ç”¨å‘½ä»¤')
    
    # å¯¼å‡ºJSON
    export_json_parser = subparsers.add_parser('export-json', help='å¯¼å‡ºæ•°æ®åº“åˆ°JSON')
    export_json_parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶å')
    
    # å¯¼å…¥JSON
    import_json_parser = subparsers.add_parser('import-json', help='ä»JSONå¯¼å…¥æ•°æ®')
    import_json_parser.add_argument('input_file', help='è¾“å…¥æ–‡ä»¶')
    import_json_parser.add_argument('--merge', action='store_true', help='åˆå¹¶è€Œä¸æ˜¯æ›¿æ¢')
    
    # å¯¼å‡ºCSV
    export_csv_parser = subparsers.add_parser('export-csv', help='å¯¼å‡ºç¿»è¯‘åˆ°CSV')
    export_csv_parser.add_argument('--output', help='è¾“å‡ºç›®å½•')
    
    # å¯¼å…¥CSV
    import_csv_parser = subparsers.add_parser('import-csv', help='ä»CSVå¯¼å…¥ç¿»è¯‘')
    import_csv_parser.add_argument('csv_file', help='CSVæ–‡ä»¶è·¯å¾„')
    
    # å¤‡ä»½
    backup_parser = subparsers.add_parser('backup', help='å¤‡ä»½æ•°æ®åº“')
    backup_parser.add_argument('--dir', help='å¤‡ä»½ç›®å½•')
    
    # æ¢å¤
    restore_parser = subparsers.add_parser('restore', help='æ¢å¤æ•°æ®åº“')
    restore_parser.add_argument('backup_file', help='å¤‡ä»½æ–‡ä»¶')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    try:
        transfer = DatabaseTransfer(args.db)
        
        if args.command == 'export-json':
            transfer.export_to_json(args.output)
        
        elif args.command == 'import-json':
            transfer.import_from_json(args.input_file, args.merge)
        
        elif args.command == 'export-csv':
            transfer.export_translations_csv(args.output)
        
        elif args.command == 'import-csv':
            transfer.import_translations_csv(args.csv_file)
        
        elif args.command == 'backup':
            transfer.backup_database(args.dir)
        
        elif args.command == 'restore':
            transfer.restore_database(args.backup_file)
    
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()